

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Graph Converter for Inference &mdash; Neural Network Libraries 1.0.18 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Python Command Line Interface" href="../command_line_interface.html" />
    <link rel="prev" title="Debugging" href="debugging.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> Neural Network Libraries
          

          
          </a>

          
            
            
              <div class="version">
                1.0.18
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../python.html">Python Package</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../installation.html">Python Package Installation</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../tutorial.html">Python API Tutorial</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="by_examples.html">NNabla by Examples</a></li>
<li class="toctree-l3"><a class="reference internal" href="python_api.html">NNabla Python API Demonstration Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="dynamic_and_static_nn.html">Static vs Dynamic Neural Networks in NNabla</a></li>
<li class="toctree-l3"><a class="reference internal" href="mixed_precision_training.html">Mixed Precision Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="multi_device_training.html">Data Parallel Distributed Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="debugging.html">Debugging</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Graph Converter for Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#batchnormalizationlinearconverter">BatchNormalizationLinearConverter</a></li>
<li class="toctree-l4"><a class="reference internal" href="#batchnormalizationfoldedconverter">BatchNormalizationFoldedConverter</a></li>
<li class="toctree-l4"><a class="reference internal" href="#fixedpointweightconverter">FixedPointWeightConverter</a></li>
<li class="toctree-l4"><a class="reference internal" href="#fixedpointactivationconverter">FixedPointActivationConverter</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../command_line_interface.html">Python Command Line Interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html">Python API Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html">Python API Reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../cpp.html">C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data_exchange_file_format.html">Data exchange file format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../format.html">Data Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../file_format_converter/file_format_converter.html">File format converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">Contributing Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../license.html">License</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Neural Network Libraries</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../python.html">Python Package</a> &raquo;</li>
        
          <li><a href="../tutorial.html">Python API Tutorial</a> &raquo;</li>
        
      <li>Graph Converter for Inference</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/python/tutorial/graph_converter_for_inference.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="graph-converter-for-inference">
<h1>Graph Converter for Inference<a class="headerlink" href="#graph-converter-for-inference" title="Permalink to this headline">Â¶</a></h1>
<p>In this tutorial, we demonstrate several graph converters mainly used
for inference. Graph converters are basically used for a trained graph,
neural network, so once you train a neural network, you can use graph
converters.</p>
<p>We show how to use the following graph converters step-by-step according
to usecases.</p>
<ol class="arabic simple">
<li>BatchNormalizationLinearConverter</li>
<li>BatchNormalizationFoldedConverter</li>
<li>FixedPointWeightConverter</li>
<li>FixedPointActivationConverter</li>
</ol>
<p><strong>Note</strong> before starting the following instruction, import python
modules needed.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">nnabla</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">nnabla.functions</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">nnabla.parametric_functions</span> <span class="k">as</span> <span class="nn">PF</span>

<span class="kn">import</span> <span class="nn">nnabla.experimental.viewers</span> <span class="k">as</span> <span class="nn">V</span>
<span class="kn">import</span> <span class="nn">nnabla.experimental.graph_converters</span> <span class="k">as</span> <span class="nn">GC</span>
</pre></div>
</div>
<p>Also, define LeNet as the motif.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># LeNet</span>
<span class="k">def</span> <span class="nf">LeNet</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;conv1&#39;</span><span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">batch_stat</span><span class="o">=</span><span class="ow">not</span> <span class="n">test</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;conv1-bn&#39;</span><span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pooling</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>

    <span class="n">h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;conv2&#39;</span><span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">batch_stat</span><span class="o">=</span><span class="ow">not</span> <span class="n">test</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;conv2-bn&#39;</span><span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pooling</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>

    <span class="n">h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;fc1&#39;</span><span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">batch_stat</span><span class="o">=</span><span class="ow">not</span> <span class="n">test</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;fc1-bn&#39;</span><span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>

    <span class="n">pred</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;fc2&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pred</span>
</pre></div>
</div>
<div class="section" id="batchnormalizationlinearconverter">
<h2>BatchNormalizationLinearConverter<a class="headerlink" href="#batchnormalizationlinearconverter" title="Permalink to this headline">Â¶</a></h2>
<p>Typical networks contain the batch normalization layers. It serves as
normalization in a network and uses the batch stats (the batch mean and
variance) to normalize inputs as</p>
<div class="math notranslate nohighlight">
\[z = \gamma \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta,\]</div>
<p>in training. <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> are the batch mean and
variance, and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are the scale and bias
parameter to be learnt.</p>
<p>At the same time, it computes the running stats (the exponential moving
average <span class="math notranslate nohighlight">\(\mu_r\)</span> and variance <span class="math notranslate nohighlight">\(\sigma_r^2\)</span> of inputs to the
batch normalization layer), which are used later for inference.</p>
<p>If nothing changes, in inference time, the batch normalization is
performed as in the above equation using the running stats.</p>
<div class="math notranslate nohighlight">
\[z = \gamma \frac{x - \mu_r}{\sqrt{\sigma_r^2 + \epsilon}} + \beta.\]</div>
<p>This is the explicit normalization, so as you can see, there are many
redundant computations (subtraction, devision, pow2, sqrt,
multiplication, addition) in inference, which should be avoided in
inference graph. We can do it by ourselves, but it is apparently
troublesome.</p>
<p>BatchNormalizationLinearConverter automatically converts this equation
of the batch normalization to the simple linear form as</p>
<div class="math notranslate nohighlight">
\[\begin{split}z = c_0 x + c_1, \\
c_0 = \frac{\gamma}{\sqrt{\sigma_r^2 + \epsilon}}, \\
c_1 = \beta - \frac{\gamma \mu_r}{\sqrt{\sigma_r^2 + \epsilon}}.\end{split}\]</div>
<p>After the conversion, we just have one multiplication and one addition
since <span class="math notranslate nohighlight">\(c_0\)</span> and <span class="math notranslate nohighlight">\(c_1\)</span> can be precomputed in inference.</p>
<p>Specifically, suppose that <span class="math notranslate nohighlight">\(x\)</span> is the output of the
2D-Convolution, so <span class="math notranslate nohighlight">\(x\)</span> is 3D-Tensor (e.g.,
<span class="math notranslate nohighlight">\(N \times H \times W\)</span>). In the batch normalization, the number of
<span class="math notranslate nohighlight">\(c\)</span>s is the map size <span class="math notranslate nohighlight">\(N\)</span>, respectively for <span class="math notranslate nohighlight">\(c_0\)</span> and
<span class="math notranslate nohighlight">\(c_1\)</span>. Thus, the multiplication (<span class="math notranslate nohighlight">\(c_0 \times x\)</span>) is
<span class="math notranslate nohighlight">\(N \times H \times W\)</span> and the addition ($ + c_1$) is same
<span class="math notranslate nohighlight">\(N \times H \times W\)</span>. We can see much reduction compared to the
native implementation.</p>
<div class="section" id="example">
<h3>Example<a class="headerlink" href="#example" title="Permalink to this headline">Â¶</a></h3>
<p>First, create LeNet.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="o">.</span><span class="n">from_numpy_array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">LeNet</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Now look at LeNet visually.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">viewer</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">SimpleGraph</span><span class="p">()</span>
<span class="n">viewer</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>Convert it to the one with the batch normalization linearly folded.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">converter</span> <span class="o">=</span> <span class="n">GC</span><span class="o">.</span><span class="n">BatchNormalizationLinearConverter</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;bn-linear-lenet&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">converter</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span>
</pre></div>
</div>
<p>Also, show the converted graph.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">viewer</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">SimpleGraph</span><span class="p">()</span>
<span class="n">viewer</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="batchnormalizationfoldedconverter">
<h2>BatchNormalizationFoldedConverter<a class="headerlink" href="#batchnormalizationfoldedconverter" title="Permalink to this headline">Â¶</a></h2>
<p>As you can see in the previous converter,
BatchNormalizationLinearConverter is the linear folding of the batch
normalization layer in inference. However, if the preceding layer of the
batch normalization is the convolution, affine or another layer
performing inner-product, that the linear folding is further folded into
the weights of the preceding layers.</p>
<p>Suppose the sequence of a convolution and a batch normalization in
inference, it can be written as,</p>
<div class="math notranslate nohighlight">
\[z = c_0 \times (w \ast x + b) + c_1,\]</div>
<p>where <span class="math notranslate nohighlight">\(\ast\)</span> is the convolutional operator, <span class="math notranslate nohighlight">\(w\)</span> is the
convolutional weights, and <span class="math notranslate nohighlight">\(b\)</span> is the bias of the convolution
layer. Since <span class="math notranslate nohighlight">\(\ast\)</span> has linearity, we can further fold <span class="math notranslate nohighlight">\(c_0\)</span>
into the weights <span class="math notranslate nohighlight">\(w\)</span> and bias <span class="math notranslate nohighlight">\(b\)</span>, such that we have the
simpler form.</p>
<div class="math notranslate nohighlight">
\[\begin{split}z = w' \ast x + b', \\
w' = c_0 w, \\
b' = c_0 b + c_1.\end{split}\]</div>
<p>BatchNormalizationFoldedConverter automatically finds a sequence of the
convolution and the batch normalization in a given graph, then folds all
parameters related to the batch normalization into the preceding
convolution layer. Now, we do not need the multiplication and addition
seen in the previous case, BatchNormalizationLinearConverter.</p>
<div class="section" id="id1">
<h3>Example<a class="headerlink" href="#id1" title="Permalink to this headline">Â¶</a></h3>
<p>First, create LeNet.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="o">.</span><span class="n">from_numpy_array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">LeNet</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Now look at LeNet visually.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">viewer</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">SimpleGraph</span><span class="p">()</span>
<span class="n">viewer</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>Convert it to the one with the batch normalization linearly folded.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">converter</span> <span class="o">=</span> <span class="n">GC</span><span class="o">.</span><span class="n">BatchNormalizationFoldedConverter</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;bn-folded-lenet&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">converter</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span>
</pre></div>
</div>
<p>Also, show the converted graph.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">viewer</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">SimpleGraph</span><span class="p">()</span>
<span class="n">viewer</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="fixedpointweightconverter">
<h2>FixedPointWeightConverter<a class="headerlink" href="#fixedpointweightconverter" title="Permalink to this headline">Â¶</a></h2>
<p>Once training finishes, where to deploy? Your destination of deployment
of a trained model might be on Cloud or an embedded device. In either
case, the typical data type, FloatingPoint32 (FP32) might be redundant
for inference, so you may want to use SIMD operation with e.g., 4-bit or
8-bit of your target device. Training is usually performed using FP32,
while interfence might be performed FixedPoint. Hence, you have to
change corresponding layers, e.g., the convolution and affine.</p>
<p>FixedPointWeightConverter automatically converts the affine,
convolution, and deconvolution of a given graph to that of fixed point
version.</p>
<div class="section" id="id2">
<h3>Example<a class="headerlink" href="#id2" title="Permalink to this headline">Â¶</a></h3>
<p>First, create LeNet.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="o">.</span><span class="n">from_numpy_array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">LeNet</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Now look at LeNet visually.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">viewer</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">SimpleGraph</span><span class="p">()</span>
<span class="n">viewer</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>Convert it to the one with the batch normalization linearly folded.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">converter</span> <span class="o">=</span> <span class="n">GC</span><span class="o">.</span><span class="n">FixedPointWeightConverter</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;fixed-point-weight-lenet&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">converter</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span>
</pre></div>
</div>
<p>Also, show the converted graph.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">viewer</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">SimpleGraph</span><span class="p">()</span>
<span class="n">viewer</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="fixedpointactivationconverter">
<h2>FixedPointActivationConverter<a class="headerlink" href="#fixedpointactivationconverter" title="Permalink to this headline">Â¶</a></h2>
<p>FixedPointWeightConverter converts layers of weights, but
FixedPointActivationConverter automatically converts activation layers,
e.g., ReLU. The typial neural network architecture contains the sequence
of the block <code class="docutils literal notranslate"><span class="pre">ReLU</span> <span class="pre">-&gt;</span> <span class="pre">Convolution</span> <span class="pre">-&gt;</span> <span class="pre">BatchNormalization</span></code>; therefore,
when you convert both <code class="docutils literal notranslate"><span class="pre">ReLU</span></code> and <code class="docutils literal notranslate"><span class="pre">Convolution</span></code> to the fixed-point
ones with proper hyper-paremters (step-size and bitwidth), you can
utilize your SIMD operation of your target device because both of the
weights and inputs of the convolution are fixed-point.</p>
<div class="section" id="id3">
<h3>Example<a class="headerlink" href="#id3" title="Permalink to this headline">Â¶</a></h3>
<p>First, create LeNet.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="o">.</span><span class="n">from_numpy_array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">LeNet</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Now look at LeNet visually.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">viewer</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">SimpleGraph</span><span class="p">()</span>
<span class="n">viewer</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>Convert it to the one with the batch normalization linearly folded.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">converter</span> <span class="o">=</span> <span class="n">GC</span><span class="o">.</span><span class="n">FixedPointActivationConverter</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;fixed-point-activation-lenet&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">converter</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span>
</pre></div>
</div>
<p>Also, show the converted graph.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">viewer</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">SimpleGraph</span><span class="p">()</span>
<span class="n">viewer</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>Tipically, FixedPointWeightConverter and FixedPointActivationConverter
are used togather. For such purposes, you can use
<code class="docutils literal notranslate"><span class="pre">GC.SequentialConverter</span></code>.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">converter_w</span> <span class="o">=</span> <span class="n">GC</span><span class="o">.</span><span class="n">FixedPointWeightConverter</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;fixed-point-lenet&quot;</span><span class="p">)</span>
<span class="n">converter_a</span> <span class="o">=</span> <span class="n">GC</span><span class="o">.</span><span class="n">FixedPointActivationConverter</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;fixed-point-lenet&quot;</span><span class="p">)</span>
<span class="n">converter</span> <span class="o">=</span> <span class="n">GC</span><span class="o">.</span><span class="n">SequentialConverter</span><span class="p">([</span><span class="n">converter_w</span><span class="p">,</span> <span class="n">converter_a</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">converter</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span>
</pre></div>
</div>
<p>Needless to say, <code class="docutils literal notranslate"><span class="pre">GC.SequentialConverter</span></code> is not limited to using this
case. One you creat your own <code class="docutils literal notranslate"><span class="pre">Conveterter</span></code>s, then you can add these
converters to <code class="docutils literal notranslate"><span class="pre">GC.SequentialConverter</span></code> if these are used togather.</p>
<p>Look at the converted graph visually.</p>
<div class="code python highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">viewer</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">SimpleGraph</span><span class="p">()</span>
<span class="n">viewer</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../command_line_interface.html" class="btn btn-neutral float-right" title="Python Command Line Interface" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="debugging.html" class="btn btn-neutral float-left" title="Debugging" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Sony Corporation

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>