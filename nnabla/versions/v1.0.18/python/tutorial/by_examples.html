

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>NNabla by Examples &mdash; Neural Network Libraries 1.0.18 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="NNabla Python API Demonstration Tutorial" href="python_api.html" />
    <link rel="prev" title="Python API Tutorial" href="../tutorial.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> Neural Network Libraries
          

          
          </a>

          
            
            
              <div class="version">
                1.0.18
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../python.html">Python Package</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../installation.html">Python Package Installation</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../tutorial.html">Python API Tutorial</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">NNabla by Examples</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#logistic-regression">Logistic Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multi-layer-perceptron-mlp">Multi-Layer Perceptron (MLP)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#convolutional-neural-network-with-cuda-acceleration">Convolutional Neural Network with CUDA acceleration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#recurrent-neural-network-elman-rnn">Recurrent Neural Network (Elman RNN)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#siamese-network">Siamese Network</a></li>
<li class="toctree-l4"><a class="reference internal" href="#appendix">Appendix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="python_api.html">NNabla Python API Demonstration Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="dynamic_and_static_nn.html">Static vs Dynamic Neural Networks in NNabla</a></li>
<li class="toctree-l3"><a class="reference internal" href="mixed_precision_training.html">Mixed Precision Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="multi_device_training.html">Data Parallel Distributed Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="debugging.html">Debugging</a></li>
<li class="toctree-l3"><a class="reference internal" href="graph_converter_for_inference.html">Graph Converter for Inference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../command_line_interface.html">Python Command Line Interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html">Python API Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html">Python API Reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../cpp.html">C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data_exchange_file_format.html">Data exchange file format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../format.html">Data Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../file_format_converter/file_format_converter.html">File format converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">Contributing Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../license.html">License</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Neural Network Libraries</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../python.html">Python Package</a> &raquo;</li>
        
          <li><a href="../tutorial.html">Python API Tutorial</a> &raquo;</li>
        
      <li>NNabla by Examples</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/python/tutorial/by_examples.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="nnabla-by-examples">
<h1>NNabla by Examples<a class="headerlink" href="#nnabla-by-examples" title="Permalink to this headline">Â¶</a></h1>
<p>This tutorial demonstrates how you can write a script to train a neural
network by using a simple hand digits classification task.</p>
<p>Note: This tutorial notebook requires
<a class="reference external" href="http://scikit-learn.org">scikit-learn</a> and
<a class="reference external" href="https://matplotlib.org/">matplotlib</a> installed in your Python
environment.</p>
<p>First let us prepare some dependencies.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nnabla</span> <span class="kn">as</span> <span class="nn">nn</span>

<span class="kn">import</span> <span class="nn">nnabla.functions</span> <span class="kn">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">nnabla.parametric_functions</span> <span class="kn">as</span> <span class="nn">PF</span>
<span class="kn">import</span> <span class="nn">nnabla.solvers</span> <span class="kn">as</span> <span class="nn">S</span>
<span class="kn">from</span> <span class="nn">nnabla.monitor</span> <span class="kn">import</span> <span class="n">tile_images</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">tiny_digits</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">imshow_opt</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2017</span><span class="o">-</span><span class="mi">06</span><span class="o">-</span><span class="mi">26</span> <span class="mi">23</span><span class="p">:</span><span class="mi">09</span><span class="p">:</span><span class="mi">49</span><span class="p">,</span><span class="mi">971</span> <span class="p">[</span><span class="n">nnabla</span><span class="p">][</span><span class="n">INFO</span><span class="p">]:</span> <span class="n">Initializing</span> <span class="n">CPU</span> <span class="n">extension</span><span class="o">...</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">tiny_digits</span></code> module is located under this folder. It provides
some utilities for loading a handwritten-digit classification dataset
(MNIST) available in scikit-learn.</p>
<div class="section" id="logistic-regression">
<h2>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">Â¶</a></h2>
<p>We will first start by defining a computation graph for logistic
regression. (For details on logistic regression, see Appendix A.)</p>
<p>The training will be done by gradient descent, where gradients are
calculated using the error backpropagation algorithm (backprop).</p>
<div class="section" id="preparing-a-toy-dataset">
<h3>Preparing a Toy Dataset<a class="headerlink" href="#preparing-a-toy-dataset" title="Permalink to this headline">Â¶</a></h3>
<p>This section just prepares a dataset to be used for demonstration of
NNabla usage.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">digits</span> <span class="o">=</span> <span class="n">tiny_digits</span><span class="o">.</span><span class="n">load_digits</span><span class="p">(</span><span class="n">n_class</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">tiny_digits</span><span class="o">.</span><span class="n">plot_stats</span><span class="p">(</span><span class="n">digits</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Num</span> <span class="n">images</span><span class="p">:</span> <span class="mi">1797</span>
<span class="n">Image</span> <span class="n">shape</span><span class="p">:</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">Labels</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span><span class="p">]</span>
</pre></div>
</div>
<img alt="../../_images/by_examples_8_1.png" src="../../_images/by_examples_8_1.png" />
<p>The next block creates a dataset loader which is a generator providing
images and labels as minibatches. Note that this dataset is just an
example purpose and not a part of NNabla.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">tiny_digits</span><span class="o">.</span><span class="n">data_iterator_tiny_digits</span><span class="p">(</span><span class="n">digits</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2017</span><span class="o">-</span><span class="mi">06</span><span class="o">-</span><span class="mi">26</span> <span class="mi">23</span><span class="p">:</span><span class="mi">09</span><span class="p">:</span><span class="mi">50</span><span class="p">,</span><span class="mi">545</span> <span class="p">[</span><span class="n">nnabla</span><span class="p">][</span><span class="n">INFO</span><span class="p">]:</span> <span class="n">DataSource</span> <span class="k">with</span> <span class="n">shuffle</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="mi">2017</span><span class="o">-</span><span class="mi">06</span><span class="o">-</span><span class="mi">26</span> <span class="mi">23</span><span class="p">:</span><span class="mi">09</span><span class="p">:</span><span class="mi">50</span><span class="p">,</span><span class="mi">546</span> <span class="p">[</span><span class="n">nnabla</span><span class="p">][</span><span class="n">INFO</span><span class="p">]:</span> <span class="n">Using</span> <span class="n">DataSourceWithMemoryCache</span>
<span class="mi">2017</span><span class="o">-</span><span class="mi">06</span><span class="o">-</span><span class="mi">26</span> <span class="mi">23</span><span class="p">:</span><span class="mi">09</span><span class="p">:</span><span class="mi">50</span><span class="p">,</span><span class="mi">546</span> <span class="p">[</span><span class="n">nnabla</span><span class="p">][</span><span class="n">INFO</span><span class="p">]:</span> <span class="n">DataSource</span> <span class="k">with</span> <span class="n">shuffle</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="mi">2017</span><span class="o">-</span><span class="mi">06</span><span class="o">-</span><span class="mi">26</span> <span class="mi">23</span><span class="p">:</span><span class="mi">09</span><span class="p">:</span><span class="mi">50</span><span class="p">,</span><span class="mi">547</span> <span class="p">[</span><span class="n">nnabla</span><span class="p">][</span><span class="n">INFO</span><span class="p">]:</span> <span class="n">On</span><span class="o">-</span><span class="n">memory</span>
<span class="mi">2017</span><span class="o">-</span><span class="mi">06</span><span class="o">-</span><span class="mi">26</span> <span class="mi">23</span><span class="p">:</span><span class="mi">09</span><span class="p">:</span><span class="mi">50</span><span class="p">,</span><span class="mi">547</span> <span class="p">[</span><span class="n">nnabla</span><span class="p">][</span><span class="n">INFO</span><span class="p">]:</span> <span class="n">Using</span> <span class="n">DataIterator</span>
</pre></div>
</div>
<p>A minibatch is as follows. <code class="docutils literal notranslate"><span class="pre">img</span></code> and <code class="docutils literal notranslate"><span class="pre">label</span></code> are in
<code class="docutils literal notranslate"><span class="pre">numpy.ndarray</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">img</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">tile_images</span><span class="p">(</span><span class="n">img</span><span class="p">),</span> <span class="o">**</span><span class="n">imshow_opt</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;labels: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">label</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Label shape: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">label</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">labels</span><span class="p">:</span> <span class="p">[[</span> <span class="mf">2.</span>  <span class="mf">8.</span>  <span class="mf">2.</span>  <span class="mf">6.</span>  <span class="mf">6.</span>  <span class="mf">7.</span>  <span class="mf">1.</span>  <span class="mf">9.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">8.</span>  <span class="mf">5.</span>  <span class="mf">2.</span>  <span class="mf">8.</span>  <span class="mf">6.</span>  <span class="mf">6.</span>  <span class="mf">6.</span>  <span class="mf">6.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">1.</span>  <span class="mf">0.</span>  <span class="mf">5.</span>  <span class="mf">8.</span>  <span class="mf">8.</span>  <span class="mf">7.</span>  <span class="mf">8.</span>  <span class="mf">4.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">7.</span>  <span class="mf">5.</span>  <span class="mf">4.</span>  <span class="mf">9.</span>  <span class="mf">2.</span>  <span class="mf">9.</span>  <span class="mf">4.</span>  <span class="mf">7.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">6.</span>  <span class="mf">8.</span>  <span class="mf">9.</span>  <span class="mf">4.</span>  <span class="mf">3.</span>  <span class="mf">1.</span>  <span class="mf">0.</span>  <span class="mf">1.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">8.</span>  <span class="mf">6.</span>  <span class="mf">7.</span>  <span class="mf">7.</span>  <span class="mf">1.</span>  <span class="mf">0.</span>  <span class="mf">7.</span>  <span class="mf">6.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">2.</span>  <span class="mf">1.</span>  <span class="mf">9.</span>  <span class="mf">6.</span>  <span class="mf">7.</span>  <span class="mf">9.</span>  <span class="mf">0.</span>  <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">5.</span>  <span class="mf">1.</span>  <span class="mf">6.</span>  <span class="mf">3.</span>  <span class="mf">0.</span>  <span class="mf">2.</span>  <span class="mf">3.</span>  <span class="mf">4.</span><span class="p">]]</span>
<span class="n">Label</span> <span class="n">shape</span><span class="p">:</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<img alt="../../_images/by_examples_12_1.png" src="../../_images/by_examples_12_1.png" />
</div>
<div class="section" id="preparing-the-computation-graph">
<h3>Preparing the Computation Graph<a class="headerlink" href="#preparing-the-computation-graph" title="Permalink to this headline">Â¶</a></h3>
<p>NNabla provides two different ways for backprop-based gradient descent
optimization. One is with a static graph, and another is with a dynamic
graph. We are going to show a static version first.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Forward pass</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Define an image variable</span>
<span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;affine1&quot;</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># Output is 10 class</span>
</pre></div>
</div>
<p>This code block shows one of the most important features in graph
building in NNabla, the <strong>parameter scope</strong>. The first line defines an
input variable <code class="docutils literal notranslate"><span class="pre">x</span></code>. The second line creates a <strong>parameter scope</strong>. The
third line then applies <code class="docutils literal notranslate"><span class="pre">PF.affine</span></code> - an affine transform - to <code class="docutils literal notranslate"><span class="pre">x</span></code>,
and creates a variable <code class="docutils literal notranslate"><span class="pre">y</span></code> holding that result. Here, the <code class="docutils literal notranslate"><span class="pre">PF</span></code>
(parametric_function) module provides functions that contain learnable
parameters, such as affine transforms (which contains weights),
convolution (which contains kernels) and batch normalization (which
contains transformation factors and coefficients). We will call these
functions as <strong>parametric functions</strong>. The parameters are created and
initialized randomly at function call, and registered by a name
âaffine1â using <code class="docutils literal notranslate"><span class="pre">parameter_scope</span></code> context.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Building a loss graph</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">label</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Define an target variable</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">softmax_cross_entropy</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">))</span>  <span class="c1"># Softmax Xentropy fits multi-class classification problems</span>
</pre></div>
</div>
<p>The remaining lines shown above define a target variable and attach
functions for loss at the end of the graph. Note that the static graph
build doesnât execute any computation, but the shapes of output
variables are inferred. Therefore, we can inspect the shapes of each
variable at this time:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s2">&quot;Printing shapes of variables&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># empty tuple means scalar</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Printing</span> <span class="n">shapes</span> <span class="n">of</span> <span class="n">variables</span>
<span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="executing-a-static-graph">
<h3>Executing a static graph<a class="headerlink" href="#executing-a-static-graph" title="Permalink to this headline">Â¶</a></h3>
<p>You can execute the computation of the graph by calling the
<code class="docutils literal notranslate"><span class="pre">forward()</span></code> method in a sink variable. Inputs can be set via <code class="docutils literal notranslate"><span class="pre">.d</span></code>
accessor. It will borrow CPU array references as <code class="docutils literal notranslate"><span class="pre">numpy.ndarray</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set data</span>
<span class="n">x</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">img</span>
<span class="n">t</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">label</span>
<span class="c1"># Execute a forward pass</span>
<span class="n">loss</span><span class="o">.</span><span class="n">forward</span><span class="p">()</span>
<span class="c1"># Showing results</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Prediction score of 0-th image: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Loss: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">d</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Prediction</span> <span class="n">score</span> <span class="n">of</span> <span class="mi">0</span><span class="o">-</span><span class="n">th</span> <span class="n">image</span><span class="p">:</span> <span class="p">[</span>  <span class="mf">9.75851917</span>   <span class="mf">6.49118519</span>  <span class="mf">16.47323608</span>  <span class="o">-</span><span class="mf">1.36296904</span>  <span class="o">-</span><span class="mf">0.78583491</span>
   <span class="mf">4.08872032</span>   <span class="mf">7.84134388</span>   <span class="mf">2.42956853</span>   <span class="mf">3.31485462</span>   <span class="mf">3.61868763</span><span class="p">]</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">10.6016616821</span>
</pre></div>
</div>
<p>The output doesnât make sense since the network is just randomly
initialized.</p>
</div>
<div class="section" id="backward-propagation-through-the-graph">
<h3>Backward propagation through the graph<a class="headerlink" href="#backward-propagation-through-the-graph" title="Permalink to this headline">Â¶</a></h3>
<p>The parameters registered by <code class="docutils literal notranslate"><span class="pre">parameter_scope</span></code> management function can
be queried by <code class="docutils literal notranslate"><span class="pre">get_parameters()</span></code> as a dict format.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">OrderedDict</span><span class="p">([(</span><span class="s1">&#39;affine1/affine/W&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7fa0ba361d50</span><span class="o">&gt;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;affine1/affine/b&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">10</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7fa0ba361ce8</span><span class="o">&gt;</span><span class="p">)])</span>
</pre></div>
</div>
<p>Before executing backpropagation, we should initialize gradient buffers
of all parameter to zeros.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">nn</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
    <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>
</pre></div>
</div>
<p>Then, you can execute backprop by calling <code class="docutils literal notranslate"><span class="pre">backward()</span></code> method at the
sink variable.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute backward</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="c1"># Showing gradients.</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">nn</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">param</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="n">flat</span><span class="p">[:</span><span class="mi">20</span><span class="p">])</span>  <span class="c1"># Showing first 20.</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">affine1</span><span class="o">/</span><span class="n">affine</span><span class="o">/</span><span class="n">W</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="p">[</span>  <span class="mf">0.00000000e+00</span>   <span class="mf">0.00000000e+00</span>   <span class="mf">0.00000000e+00</span>   <span class="mf">0.00000000e+00</span>
   <span class="mf">0.00000000e+00</span>   <span class="mf">0.00000000e+00</span>   <span class="mf">0.00000000e+00</span>   <span class="mf">0.00000000e+00</span>
   <span class="mf">0.00000000e+00</span>   <span class="mf">0.00000000e+00</span>   <span class="mf">4.98418584e-02</span>   <span class="mf">8.72317329e-03</span>
  <span class="o">-</span><span class="mf">4.06671129e-02</span>  <span class="o">-</span><span class="mf">4.68742661e-02</span>   <span class="mf">2.52632981e-09</span>   <span class="mf">7.86017510e-04</span>
   <span class="mf">9.06870365e-02</span>  <span class="o">-</span><span class="mf">1.56249944e-02</span>  <span class="o">-</span><span class="mf">1.56217301e-02</span>  <span class="o">-</span><span class="mf">3.12499963e-02</span><span class="p">]</span>
<span class="n">affine1</span><span class="o">/</span><span class="n">affine</span><span class="o">/</span><span class="n">b</span> <span class="p">(</span><span class="mi">10</span><span class="p">,)</span> <span class="p">[</span> <span class="mf">0.42710391</span> <span class="o">-</span><span class="mf">0.01852455</span>  <span class="mf">0.07369987</span> <span class="o">-</span><span class="mf">0.04687012</span> <span class="o">-</span><span class="mf">0.07798236</span> <span class="o">-</span><span class="mf">0.03664626</span>
  <span class="mf">0.01651323</span> <span class="o">-</span><span class="mf">0.1249291</span>  <span class="o">-</span><span class="mf">0.11862005</span> <span class="o">-</span><span class="mf">0.09374455</span><span class="p">]</span>
</pre></div>
</div>
<p>Gradient is stored in grad field of <code class="docutils literal notranslate"><span class="pre">Variable</span></code>. <code class="docutils literal notranslate"><span class="pre">.g</span></code> accessor can be
used to access grad data in <code class="docutils literal notranslate"><span class="pre">numpy.ndarray</span></code> format.</p>
</div>
<div class="section" id="optimizing-parameters-training">
<h3>Optimizing parameters (=Training)<a class="headerlink" href="#optimizing-parameters-training" title="Permalink to this headline">Â¶</a></h3>
<p>To optimize parameters, we provide solver module (aliased as S here).
The solver module contains a bunch of optimizer implementations such as
SGD, SGD with momentum, Adam etc. The below block creates SGD solver and
sets parameters of logistic regression to it.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a solver (gradient-based optimizer)</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">solver</span> <span class="o">=</span> <span class="n">S</span><span class="o">.</span><span class="n">Sgd</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">solver</span><span class="o">.</span><span class="n">set_parameters</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">())</span>  <span class="c1"># Set parameter variables to be updated.</span>
</pre></div>
</div>
<p>In the next block, we demonstrate a single step of optimization loop.
<code class="docutils literal notranslate"><span class="pre">solver.zero_grad()</span></code> line does equivalent to calling <code class="docutils literal notranslate"><span class="pre">.grad.zero()</span></code>
for all parameters as we shown above. After backward computation, we
apply weight decay, then applying gradient descent implemented in Sgd
solver class as follows</p>
<div class="math notranslate nohighlight">
\[\theta \leftarrow \theta - \eta \nabla_{\theta} L(\theta, X_{\mathrm minibatch})\]</div>
<p>where <span class="math notranslate nohighlight">\(\eta\)</span> denotes learning rate.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># One step of training</span>
<span class="n">x</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
<span class="n">loss</span><span class="o">.</span><span class="n">forward</span><span class="p">()</span>
<span class="n">solver</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>  <span class="c1"># Initialize gradients of all parameters to zero.</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">solver</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">)</span>  <span class="c1"># Applying weight decay as an regularization</span>
<span class="n">solver</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">12.9438686371</span>
</pre></div>
</div>
<p>Next block iterates optimization steps, and shows the loss decreases.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">x</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">forward</span><span class="p">()</span>
    <span class="n">solver</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>  <span class="c1"># Initialize gradients of all parameters to zero.</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">solver</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">)</span>  <span class="c1"># Applying weight decay as an regularization</span>
    <span class="n">solver</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Print for each 10 iterations</span>
        <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">0</span> <span class="mf">12.6905069351</span>
<span class="mi">100</span> <span class="mf">3.17041015625</span>
<span class="mi">200</span> <span class="mf">1.60036706924</span>
<span class="mi">300</span> <span class="mf">0.673069953918</span>
<span class="mi">400</span> <span class="mf">0.951370298862</span>
<span class="mi">500</span> <span class="mf">0.724424362183</span>
<span class="mi">600</span> <span class="mf">0.361597299576</span>
<span class="mi">700</span> <span class="mf">0.588107347488</span>
<span class="mi">800</span> <span class="mf">0.28792989254</span>
<span class="mi">900</span> <span class="mf">0.415006935596</span>
</pre></div>
</div>
</div>
<div class="section" id="show-prediction">
<h3>Show prediction<a class="headerlink" href="#show-prediction" title="Permalink to this headline">Â¶</a></h3>
<p>The following code displays training results.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>  <span class="c1"># Here we predict images from training set although it&#39;s useless.</span>
<span class="n">y</span><span class="o">.</span><span class="n">forward</span><span class="p">()</span>  <span class="c1"># You can execute a sub graph.</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">tile_images</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">d</span><span class="p">),</span> <span class="o">**</span><span class="n">imshow_opt</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;prediction:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">d</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>  <span class="c1"># Taking a class index based on prediction score.</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">prediction</span><span class="p">:</span>
<span class="p">[[</span><span class="mi">5</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">9</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">3</span> <span class="mi">3</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">2</span> <span class="mi">4</span> <span class="mi">1</span> <span class="mi">7</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">5</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">7</span> <span class="mi">7</span> <span class="mi">9</span> <span class="mi">7</span> <span class="mi">9</span> <span class="mi">0</span> <span class="mi">7</span> <span class="mi">3</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">5</span> <span class="mi">3</span> <span class="mi">7</span> <span class="mi">6</span> <span class="mi">6</span> <span class="mi">8</span> <span class="mi">0</span> <span class="mi">9</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">3</span> <span class="mi">5</span> <span class="mi">5</span> <span class="mi">5</span> <span class="mi">4</span> <span class="mi">9</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">8</span> <span class="mi">5</span> <span class="mi">1</span> <span class="mi">8</span> <span class="mi">8</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">7</span> <span class="mi">5</span> <span class="mi">0</span> <span class="mi">7</span> <span class="mi">6</span> <span class="mi">9</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">0</span> <span class="mi">6</span> <span class="mi">2</span> <span class="mi">6</span> <span class="mi">4</span> <span class="mi">4</span> <span class="mi">2</span> <span class="mi">6</span><span class="p">]]</span>
</pre></div>
</div>
<img alt="../../_images/by_examples_36_1.png" src="../../_images/by_examples_36_1.png" />
</div>
<div class="section" id="dynamic-graph-construction-support">
<h3>Dynamic graph construction support<a class="headerlink" href="#dynamic-graph-construction-support" title="Permalink to this headline">Â¶</a></h3>
<p>This is another way of running computation graph in NNabla. This example
doesnât show how useful dynamic graph is, but shows a bit of flavor.</p>
<p>The next block just define computation graph building as functions for
later use.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">logreg_forward</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;affine1&quot;</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">logreg_loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">softmax_cross_entropy</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">))</span>  <span class="c1"># Softmax Xentropy fits multi-class classification problems</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>To run a computation graph dynamically during creation, you use
<code class="docutils literal notranslate"><span class="pre">nnabla.auto_forward()</span></code> context as you see in the below block. By
this, computation is fired immediately at functions are called. (You can
also use <code class="docutils literal notranslate"><span class="pre">nnabla.set_auto_forward(auto)</span></code> to set the auto-forward state
globally.)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">label</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">x</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
<span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">auto_forward</span><span class="p">():</span>  <span class="c1"># Graph are executed</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">logreg_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">logreg_loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Loss: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">d</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">tile_images</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">d</span><span class="p">),</span> <span class="o">**</span><span class="n">imshow_opt</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;prediction:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">d</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Loss</span><span class="p">:</span> <span class="mf">0.43071603775</span>
<span class="n">prediction</span><span class="p">:</span>
<span class="p">[[</span><span class="mi">9</span> <span class="mi">3</span> <span class="mi">5</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">9</span> <span class="mi">9</span> <span class="mi">2</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">5</span> <span class="mi">6</span> <span class="mi">6</span> <span class="mi">2</span> <span class="mi">7</span> <span class="mi">5</span> <span class="mi">1</span> <span class="mi">1</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">3</span> <span class="mi">7</span> <span class="mi">7</span> <span class="mi">6</span> <span class="mi">0</span> <span class="mi">8</span> <span class="mi">3</span> <span class="mi">8</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">0</span> <span class="mi">6</span> <span class="mi">4</span> <span class="mi">6</span> <span class="mi">0</span> <span class="mi">6</span> <span class="mi">9</span> <span class="mi">9</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">6</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">5</span> <span class="mi">8</span> <span class="mi">3</span> <span class="mi">2</span> <span class="mi">4</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">1</span> <span class="mi">4</span> <span class="mi">4</span> <span class="mi">0</span> <span class="mi">5</span> <span class="mi">7</span> <span class="mi">1</span> <span class="mi">7</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span> <span class="mi">5</span> <span class="mi">8</span> <span class="mi">3</span> <span class="mi">7</span> <span class="mi">8</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">5</span> <span class="mi">7</span> <span class="mi">5</span> <span class="mi">3</span> <span class="mi">3</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">7</span><span class="p">]]</span>
</pre></div>
</div>
<img alt="../../_images/by_examples_41_1.png" src="../../_images/by_examples_41_1.png" />
<p>Backward computation can be done on a dynamically constructed graph.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">solver</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="multi-layer-perceptron-mlp">
<h2>Multi-Layer Perceptron (MLP)<a class="headerlink" href="#multi-layer-perceptron-mlp" title="Permalink to this headline">Â¶</a></h2>
<p>In this section, you see an example of MLP graph building and training.</p>
<p>Before starting, we clear all parameters registered in the logistic
regression example.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nn</span><span class="o">.</span><span class="n">clear_parameters</span><span class="p">()</span>  <span class="c1"># Clear all parameters</span>
</pre></div>
</div>
<p>Here is the function that builds a MLP with an arbitrary depth and width
for 10 class classification.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">hidden</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">16</span><span class="p">]):</span>
    <span class="n">hs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;mlp&quot;</span><span class="p">):</span>  <span class="c1"># Parameter scope can be nested</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">for</span> <span class="n">hid</span><span class="p">,</span> <span class="n">hsize</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">hidden</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;affine{}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">hid</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)):</span>
                <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">PF</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">hsize</span><span class="p">))</span>
                <span class="n">hs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;classifier&quot;</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">hs</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Construct a MLP graph</span>
<span class="n">y</span><span class="p">,</span> <span class="n">hs</span> <span class="o">=</span> <span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s2">&quot;Printing shapes&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;x: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">hs</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;h{}:&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;y: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Printing</span> <span class="n">shapes</span>
<span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">h1</span><span class="p">:</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">h2</span><span class="p">:</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="n">h3</span><span class="p">:</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">y</span><span class="p">:</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Training</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">logreg_loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>  <span class="c1"># Reuse logreg loss function.</span>

<span class="c1"># Copied from the above logreg example.</span>
<span class="k">def</span> <span class="nf">training</span><span class="p">(</span><span class="n">steps</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="n">solver</span> <span class="o">=</span> <span class="n">S</span><span class="o">.</span><span class="n">Sgd</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">solver</span><span class="o">.</span><span class="n">set_parameters</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">())</span>  <span class="c1"># Set parameter variables to be updated.</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
        <span class="n">x</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">forward</span><span class="p">()</span>
        <span class="n">solver</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>  <span class="c1"># Initialize gradients of all parameters to zero.</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">solver</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">)</span>  <span class="c1"># Applying weight decay as an regularization</span>
        <span class="n">solver</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Print for each 10 iterations</span>
            <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">d</span><span class="p">)</span>


<span class="c1"># Training</span>
<span class="n">training</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">0</span> <span class="mf">2.42193937302</span>
<span class="mi">100</span> <span class="mf">1.83251476288</span>
<span class="mi">200</span> <span class="mf">1.49943637848</span>
<span class="mi">300</span> <span class="mf">1.30751883984</span>
<span class="mi">400</span> <span class="mf">1.00974023342</span>
<span class="mi">500</span> <span class="mf">0.904026031494</span>
<span class="mi">600</span> <span class="mf">0.873289525509</span>
<span class="mi">700</span> <span class="mf">0.725554704666</span>
<span class="mi">800</span> <span class="mf">0.614291608334</span>
<span class="mi">900</span> <span class="mf">0.555113613605</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Showing responses for each layer</span>
<span class="n">num_plot</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">hs</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">gid</span> <span class="o">=</span> <span class="mi">1</span>

<span class="k">def</span> <span class="nf">scale01</span><span class="p">(</span><span class="n">h</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">h</span> <span class="o">-</span> <span class="n">h</span><span class="o">.</span><span class="n">min</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">h</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">h</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>

<span class="k">def</span> <span class="nf">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">title</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">gid</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">num_plot</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">gid</span><span class="p">)</span>
    <span class="n">gid</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="o">**</span><span class="n">imshow_opt</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">imshow</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">hid</span><span class="p">,</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">hs</span><span class="p">):</span>
    <span class="n">imshow</span><span class="p">(</span><span class="n">scale01</span><span class="p">(</span><span class="n">h</span><span class="o">.</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="s1">&#39;h{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">hid</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">imshow</span><span class="p">(</span><span class="n">scale01</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="s1">&#39;y&#39;</span><span class="p">)</span>
</pre></div>
</div>
<img alt="../../_images/by_examples_52_0.png" src="../../_images/by_examples_52_0.png" />
</div>
<div class="section" id="convolutional-neural-network-with-cuda-acceleration">
<h2>Convolutional Neural Network with CUDA acceleration<a class="headerlink" href="#convolutional-neural-network-with-cuda-acceleration" title="Permalink to this headline">Â¶</a></h2>
<p>Here we demonstrates a CNN with CUDA GPU acceleration.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nn</span><span class="o">.</span><span class="n">clear_parameters</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cnn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;cnn&quot;</span><span class="p">):</span>  <span class="c1"># Parameter scope can be nested</span>
        <span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;conv1&quot;</span><span class="p">):</span>
            <span class="n">c1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">PF</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span>
                <span class="n">PF</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">pad</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))))</span>
        <span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;conv2&quot;</span><span class="p">):</span>
            <span class="n">c2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">PF</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span>
                <span class="n">PF</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">c1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">pad</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))))</span>
            <span class="n">c2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">average_pooling</span><span class="p">(</span><span class="n">c2</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;fc3&quot;</span><span class="p">):</span>
            <span class="n">fc3</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">PF</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">c2</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
        <span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;classifier&quot;</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">fc3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="p">[</span><span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="p">,</span> <span class="n">fc3</span><span class="p">]</span>
</pre></div>
</div>
<p>To enable CUDA extension in NNabla, you have to install nnabla-ext-cuda
package first. See <a class="reference external" href="http://nnabla.readthedocs.io/en/latest/python/installation.html">the install
guide</a>.
After installing the CUDA extension, you can easily switch to run on
CUDA by specifying a context before building a graph. We strongly
recommend using a CUDNN context that is fast. Although the context class
can be instantiated by <code class="docutils literal notranslate"><span class="pre">nn.Context()</span></code>, specifying a context descriptor
might be a bit complicated for users. There for we recommend create a
context by using a helper function <code class="docutils literal notranslate"><span class="pre">get_extension_context()</span></code> found in the
<code class="docutils literal notranslate"><span class="pre">nnabla.ext_utils</span></code> module. NNabla officially supports <code class="docutils literal notranslate"><span class="pre">cpu</span></code>
and <code class="docutils literal notranslate"><span class="pre">cudnn</span></code> as a context specifier passed to the first argument
(extension name). NOTE: By setting the cudnn context as a global default
context, Functions and solves created are instantiated with CUDNN
(preferred) mode. You can also specify a context using
<code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">nn.context_scope()</span></code>. See <a class="reference external" href="http://nnabla.readthedocs.io/en/latest/python/api/common.html#context">API
reference</a>
for details.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run on CUDA</span>
<span class="kn">from</span> <span class="nn">nnabla.ext_utils</span> <span class="kn">import</span> <span class="n">get_extension_context</span>
<span class="n">cuda_device_id</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">ctx</span> <span class="o">=</span> <span class="n">get_extension_context</span><span class="p">(</span><span class="s1">&#39;cudnn&#39;</span><span class="p">,</span> <span class="n">device_id</span><span class="o">=</span><span class="n">cuda_device_id</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Context: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ctx</span><span class="p">))</span>
<span class="n">nn</span><span class="o">.</span><span class="n">set_default_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>  <span class="c1"># Set CUDA as a default context.</span>
<span class="n">y</span><span class="p">,</span> <span class="n">hs</span> <span class="o">=</span> <span class="n">cnn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">logreg_loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2017</span><span class="o">-</span><span class="mi">06</span><span class="o">-</span><span class="mi">26</span> <span class="mi">23</span><span class="p">:</span><span class="mi">09</span><span class="p">:</span><span class="mi">54</span><span class="p">,</span><span class="mi">555</span> <span class="p">[</span><span class="n">nnabla</span><span class="p">][</span><span class="n">INFO</span><span class="p">]:</span> <span class="n">Initializing</span> <span class="n">CUDA</span> <span class="n">extension</span><span class="o">...</span>
<span class="mi">2017</span><span class="o">-</span><span class="mi">06</span><span class="o">-</span><span class="mi">26</span> <span class="mi">23</span><span class="p">:</span><span class="mi">09</span><span class="p">:</span><span class="mi">54</span><span class="p">,</span><span class="mi">731</span> <span class="p">[</span><span class="n">nnabla</span><span class="p">][</span><span class="n">INFO</span><span class="p">]:</span> <span class="n">Initializing</span> <span class="n">cuDNN</span> <span class="n">extension</span><span class="o">...</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Context</span><span class="p">:</span> <span class="n">Context</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;cpu|cuda&#39;</span><span class="p">,</span> <span class="n">array_class</span><span class="o">=</span><span class="s1">&#39;CudaCachedArray&#39;</span><span class="p">,</span> <span class="n">device_id</span><span class="o">=</span><span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="n">compute_backend</span><span class="o">=</span><span class="s1">&#39;default|cudnn&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">training</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">0</span> <span class="mf">2.34862923622</span>
<span class="mi">100</span> <span class="mf">1.00527024269</span>
<span class="mi">200</span> <span class="mf">0.416576713324</span>
<span class="mi">300</span> <span class="mf">0.240603536367</span>
<span class="mi">400</span> <span class="mf">0.254562884569</span>
<span class="mi">500</span> <span class="mf">0.206138283014</span>
<span class="mi">600</span> <span class="mf">0.220851421356</span>
<span class="mi">700</span> <span class="mf">0.161689639091</span>
<span class="mi">800</span> <span class="mf">0.230873346329</span>
<span class="mi">900</span> <span class="mf">0.121101222932</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Showing responses for each layer</span>
<span class="n">num_plot</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">hs</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">gid</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">imshow</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">imshow</span><span class="p">(</span><span class="n">tile_images</span><span class="p">(</span><span class="n">hs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="bp">None</span><span class="p">]),</span> <span class="s1">&#39;conv1&#39;</span><span class="p">)</span>
<span class="n">imshow</span><span class="p">(</span><span class="n">tile_images</span><span class="p">(</span><span class="n">hs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="bp">None</span><span class="p">]),</span> <span class="s1">&#39;conv2&#39;</span><span class="p">)</span>
<span class="n">imshow</span><span class="p">(</span><span class="n">hs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="s1">&#39;fc3&#39;</span><span class="p">)</span>
<span class="n">imshow</span><span class="p">(</span><span class="n">scale01</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="s1">&#39;y&#39;</span><span class="p">)</span>
</pre></div>
</div>
<img alt="../../_images/by_examples_59_0.png" src="../../_images/by_examples_59_0.png" />
<p><code class="docutils literal notranslate"><span class="pre">nn.save_parameters</span></code> writes parameters registered in
<code class="docutils literal notranslate"><span class="pre">parameter_scope</span></code> system in HDF5 format. We use it a later example.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">path_cnn_params</span> <span class="o">=</span> <span class="s2">&quot;tmp.params.cnn.h5&quot;</span>
<span class="n">nn</span><span class="o">.</span><span class="n">save_parameters</span><span class="p">(</span><span class="n">path_cnn_params</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2017</span><span class="o">-</span><span class="mi">06</span><span class="o">-</span><span class="mi">26</span> <span class="mi">23</span><span class="p">:</span><span class="mi">09</span><span class="p">:</span><span class="mi">56</span><span class="p">,</span><span class="mi">132</span> <span class="p">[</span><span class="n">nnabla</span><span class="p">][</span><span class="n">INFO</span><span class="p">]:</span> <span class="n">Parameter</span> <span class="n">save</span> <span class="p">(</span><span class="n">hdf5</span><span class="p">):</span> <span class="n">tmp</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">cnn</span><span class="o">.</span><span class="n">h5</span>
</pre></div>
</div>
</div>
<div class="section" id="recurrent-neural-network-elman-rnn">
<h2>Recurrent Neural Network (Elman RNN)<a class="headerlink" href="#recurrent-neural-network-elman-rnn" title="Permalink to this headline">Â¶</a></h2>
<p>This is an example of recurrent neural network training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nn</span><span class="o">.</span><span class="n">clear_parameters</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">rnn</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">h0</span><span class="p">,</span> <span class="n">hidden</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
    <span class="n">hs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;rnn&quot;</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">h0</span>
        <span class="c1"># Time step loop</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">:</span>
            <span class="c1"># Note: Parameter scopes are reused over time</span>
            <span class="c1"># which means parameters are shared over time.</span>
            <span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;x2h&quot;</span><span class="p">):</span>
                <span class="n">x2h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;h2h&quot;</span><span class="p">):</span>
                <span class="n">h2h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x2h</span> <span class="o">+</span> <span class="n">h2h</span><span class="p">)</span>
            <span class="n">hs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;classifier&quot;</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">hs</span>
</pre></div>
</div>
<p>It is not meaningful, but just a demonstration purpose. We split an
image into 2 by 2 grids, and feed them sequentially into RNN.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">split_grid4</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="mi">4</span><span class="p">,</span> <span class="p">:</span><span class="mi">4</span><span class="p">]</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">:]</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">4</span><span class="p">:,</span> <span class="p">:</span><span class="mi">4</span><span class="p">]</span>
    <span class="n">x3</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">4</span><span class="p">:,</span> <span class="mi">4</span><span class="p">:]</span>
    <span class="k">return</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">hidden</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">seq_img</span> <span class="o">=</span> <span class="n">split_grid4</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="n">seq_x</span> <span class="o">=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">subimg</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">subimg</span> <span class="ow">in</span> <span class="n">seq_img</span><span class="p">]</span>
<span class="n">h0</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">hidden</span><span class="p">))</span>  <span class="c1"># Initial hidden state.</span>
<span class="n">y</span><span class="p">,</span> <span class="n">hs</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">seq_x</span><span class="p">,</span> <span class="n">h0</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">logreg_loss</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Copied from the above logreg example.</span>
<span class="k">def</span> <span class="nf">training_rnn</span><span class="p">(</span><span class="n">steps</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="n">solver</span> <span class="o">=</span> <span class="n">S</span><span class="o">.</span><span class="n">Sgd</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">solver</span><span class="o">.</span><span class="n">set_parameters</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">())</span>  <span class="c1"># Set parameter variables to be updated.</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
        <span class="n">minibatch</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
        <span class="n">img</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">minibatch</span>
        <span class="n">seq_img</span> <span class="o">=</span> <span class="n">split_grid4</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
        <span class="n">h0</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Initialize as 0</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">subimg</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">seq_x</span><span class="p">,</span> <span class="n">seq_img</span><span class="p">):</span>
            <span class="n">x</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">subimg</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">forward</span><span class="p">()</span>
        <span class="n">solver</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>  <span class="c1"># Initialize gradients of all parameters to zero.</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">solver</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">)</span>  <span class="c1"># Applying weight decay as an regularization</span>
        <span class="n">solver</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Print for each 10 iterations</span>
            <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">d</span><span class="p">)</span>

<span class="n">training_rnn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">0</span> <span class="mf">2.62527275085</span>
<span class="mi">100</span> <span class="mf">0.780260562897</span>
<span class="mi">200</span> <span class="mf">0.486522495747</span>
<span class="mi">300</span> <span class="mf">0.289345681667</span>
<span class="mi">400</span> <span class="mf">0.249717146158</span>
<span class="mi">500</span> <span class="mf">0.538961410522</span>
<span class="mi">600</span> <span class="mf">0.276877015829</span>
<span class="mi">700</span> <span class="mf">0.159639537334</span>
<span class="mi">800</span> <span class="mf">0.249660402536</span>
<span class="mi">900</span> <span class="mf">0.0925596579909</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Showing responses for each layer</span>
<span class="n">num_plot</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">hs</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">gid</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">imshow</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">hid</span><span class="p">,</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">hs</span><span class="p">):</span>
    <span class="n">imshow</span><span class="p">(</span><span class="n">scale01</span><span class="p">(</span><span class="n">h</span><span class="o">.</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="s1">&#39;h{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">hid</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">imshow</span><span class="p">(</span><span class="n">scale01</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="s1">&#39;y&#39;</span><span class="p">)</span>
</pre></div>
</div>
<img alt="../../_images/by_examples_69_0.png" src="../../_images/by_examples_69_0.png" />
</div>
<div class="section" id="siamese-network">
<h2>Siamese Network<a class="headerlink" href="#siamese-network" title="Permalink to this headline">Â¶</a></h2>
<p>This example show how to embed an image in a categorical dataset into 2D
space using deep learning. This also demonstrates how to reuse a
pretrained network.</p>
<p>First, we load parameters learned in the CNN example.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nn</span><span class="o">.</span><span class="n">clear_parameters</span><span class="p">()</span>
<span class="c1"># Loading CNN pretrained parameters.</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">load_parameters</span><span class="p">(</span><span class="n">path_cnn_params</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2017</span><span class="o">-</span><span class="mi">06</span><span class="o">-</span><span class="mi">26</span> <span class="mi">23</span><span class="p">:</span><span class="mi">09</span><span class="p">:</span><span class="mi">57</span><span class="p">,</span><span class="mi">838</span> <span class="p">[</span><span class="n">nnabla</span><span class="p">][</span><span class="n">INFO</span><span class="p">]:</span> <span class="n">Parameter</span> <span class="n">load</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">built</span><span class="o">-</span><span class="ow">in</span> <span class="n">function</span> <span class="nb">format</span><span class="o">&gt;</span><span class="p">):</span> <span class="n">tmp</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">cnn</span><span class="o">.</span><span class="n">h5</span>
</pre></div>
</div>
<p>We define embedding function. Note that the network structure and
parameter hierarchy is identical to the previous CNN example. That
enables you to reuse the saved parameters and finetune from it.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cnn_embed</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="c1"># Note: Identical configuration with the CNN example above.</span>
    <span class="c1"># Parameters pretrained in the above CNN example are used.</span>
    <span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;cnn&quot;</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;conv1&quot;</span><span class="p">):</span>
            <span class="n">c1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">PF</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span><span class="n">PF</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">pad</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> <span class="n">batch_stat</span><span class="o">=</span><span class="ow">not</span> <span class="n">test</span><span class="p">))</span>
        <span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;conv2&quot;</span><span class="p">):</span>
            <span class="n">c2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">PF</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span><span class="n">PF</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">c1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">pad</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">batch_stat</span><span class="o">=</span><span class="ow">not</span> <span class="n">test</span><span class="p">))</span>
            <span class="n">c2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">average_pooling</span><span class="p">(</span><span class="n">c2</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;fc3&quot;</span><span class="p">):</span>
            <span class="n">fc3</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">c2</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
    <span class="c1"># Additional affine for map into 2D.</span>
    <span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;embed2d&quot;</span><span class="p">):</span>
        <span class="n">embed</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">c2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">embed</span><span class="p">,</span> <span class="p">[</span><span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="p">,</span> <span class="n">fc3</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">siamese_loss</span><span class="p">(</span><span class="n">e0</span><span class="p">,</span> <span class="n">e1</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">):</span>
    <span class="n">dist</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">squared_error</span><span class="p">(</span><span class="n">e0</span><span class="p">,</span> <span class="n">e1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Squared distance</span>
    <span class="c1"># Contrastive loss</span>
    <span class="n">sim_cost</span> <span class="o">=</span> <span class="n">t</span> <span class="o">*</span> <span class="n">dist</span>
    <span class="n">dissim_cost</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">*</span> \
        <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">maximum_scalar</span><span class="p">(</span><span class="n">margin</span> <span class="o">-</span> <span class="p">(</span><span class="n">dist</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sim_cost</span> <span class="o">+</span> <span class="n">dissim_cost</span><span class="p">)</span>
</pre></div>
</div>
<p>We build two stream CNNs and compare them with the contrastive loss
function defined above. Note that both CNNs have the same parameter
hierarchy, which means both parameters are shared.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x0</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],))</span>  <span class="c1"># Same class or not</span>
<span class="n">e0</span><span class="p">,</span> <span class="n">hs0</span> <span class="o">=</span> <span class="n">cnn_embed</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
<span class="n">e1</span><span class="p">,</span> <span class="n">hs1</span> <span class="o">=</span> <span class="n">cnn_embed</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>  <span class="c1"># NOTE: parameters are shared</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">siamese_loss</span><span class="p">(</span><span class="n">e0</span><span class="p">,</span> <span class="n">e1</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_siamese</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
        <span class="n">minibatchs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
            <span class="n">minibatch</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
            <span class="n">minibatchs</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">minibatch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span> <span class="n">minibatch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()))</span>
        <span class="n">x0</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="n">label0</span> <span class="o">=</span> <span class="n">minibatchs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">x1</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="n">label1</span> <span class="o">=</span> <span class="n">minibatchs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">t</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="p">(</span><span class="n">label0</span> <span class="o">==</span> <span class="n">label1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span><span class="o">.</span><span class="n">flat</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">forward</span><span class="p">()</span>
        <span class="n">solver</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>  <span class="c1"># Initialize gradients of all parameters to zero.</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">solver</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">)</span>  <span class="c1"># Applying weight decay as an regularization</span>
        <span class="n">solver</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Print for each 10 iterations</span>
            <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">d</span><span class="p">)</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">solver</span> <span class="o">=</span> <span class="n">S</span><span class="o">.</span><span class="n">Sgd</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;embed2d&quot;</span><span class="p">):</span>
    <span class="c1"># Only 2d embedding affine will be updated.</span>
    <span class="n">solver</span><span class="o">.</span><span class="n">set_parameters</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">())</span>
<span class="n">training_siamese</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>
<span class="c1"># Decay learning rate</span>
<span class="n">solver</span><span class="o">.</span><span class="n">set_learning_rate</span><span class="p">(</span><span class="n">solver</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">training_siamese</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">0</span> <span class="mf">0.150528043509</span>
<span class="mi">100</span> <span class="mf">0.186870157719</span>
<span class="mi">200</span> <span class="mf">0.149316266179</span>
<span class="mi">300</span> <span class="mf">0.207163512707</span>
<span class="mi">400</span> <span class="mf">0.171384960413</span>
<span class="mi">500</span> <span class="mf">0.190256178379</span>
<span class="mi">600</span> <span class="mf">0.138507723808</span>
<span class="mi">700</span> <span class="mf">0.0918073058128</span>
<span class="mi">800</span> <span class="mf">0.159692272544</span>
<span class="mi">900</span> <span class="mf">0.0833697617054</span>
<span class="mi">1000</span> <span class="mf">0.0839115008712</span>
<span class="mi">1100</span> <span class="mf">0.104669973254</span>
<span class="mi">1200</span> <span class="mf">0.0776312947273</span>
<span class="mi">1300</span> <span class="mf">0.114788673818</span>
<span class="mi">1400</span> <span class="mf">0.120309025049</span>
<span class="mi">1500</span> <span class="mf">0.107732802629</span>
<span class="mi">1600</span> <span class="mf">0.070114441216</span>
<span class="mi">1700</span> <span class="mf">0.101728007197</span>
<span class="mi">1800</span> <span class="mf">0.114350572228</span>
<span class="mi">1900</span> <span class="mf">0.118794307113</span>
<span class="mi">0</span> <span class="mf">0.0669310241938</span>
<span class="mi">100</span> <span class="mf">0.0553173273802</span>
<span class="mi">200</span> <span class="mf">0.0829797014594</span>
<span class="mi">300</span> <span class="mf">0.0951051414013</span>
<span class="mi">400</span> <span class="mf">0.128303915262</span>
<span class="mi">500</span> <span class="mf">0.102963000536</span>
<span class="mi">600</span> <span class="mf">0.0910559669137</span>
<span class="mi">700</span> <span class="mf">0.0898950695992</span>
<span class="mi">800</span> <span class="mf">0.119949311018</span>
<span class="mi">900</span> <span class="mf">0.0603067912161</span>
<span class="mi">1000</span> <span class="mf">0.105748720467</span>
<span class="mi">1100</span> <span class="mf">0.108760476112</span>
<span class="mi">1200</span> <span class="mf">0.0820947736502</span>
<span class="mi">1300</span> <span class="mf">0.0971114039421</span>
<span class="mi">1400</span> <span class="mf">0.0836166366935</span>
<span class="mi">1500</span> <span class="mf">0.0899554267526</span>
<span class="mi">1600</span> <span class="mf">0.109069615602</span>
<span class="mi">1700</span> <span class="mf">0.0921652168036</span>
<span class="mi">1800</span> <span class="mf">0.0759357959032</span>
<span class="mi">1900</span> <span class="mf">0.100669950247</span>
</pre></div>
</div>
<p>We visualize embedded training images as following. You see the images
from the same class embedded near each other.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">all_image</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">[:</span><span class="mi">512</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>
<span class="n">all_label</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">[:</span><span class="mi">512</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_all</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">all_image</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">x_all</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">all_image</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">auto_forward</span><span class="p">():</span>
    <span class="n">embed</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">cnn_embed</span><span class="p">(</span><span class="n">x_all</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Set1</span><span class="p">(</span><span class="n">i</span> <span class="o">/</span> <span class="mf">10.</span><span class="p">)</span>  <span class="c1"># Maybe it doesn&#39;t work in an older version of Matplotlib where color map lies in [0, 256)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">embed</span><span class="o">.</span><span class="n">d</span><span class="p">[</span><span class="n">all_label</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">embed</span><span class="o">.</span><span class="n">d</span><span class="p">[</span>
             <span class="n">all_label</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
</pre></div>
</div>
<img alt="../../_images/by_examples_81_0.png" src="../../_images/by_examples_81_0.png" />
</div>
<div class="section" id="appendix">
<h2>Appendix<a class="headerlink" href="#appendix" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="a-logistic-regression">
<h3>A. Logistic Regression<a class="headerlink" href="#a-logistic-regression" title="Permalink to this headline">Â¶</a></h3>
<p>Here we demonstrate how to train the simplest neural network, logistic
regression (single layer perceptron). Logistic regression is a linear
classifier
<span class="math notranslate nohighlight">\(f : {\cal R}^{D\times 1} \rightarrow {\cal R}^{K\times 1}\)</span></p>
<div class="math notranslate nohighlight">
\[\mathbf f(\mathbf x, \mathbf \Theta) = \mathbf W \mathbf x + \mathbf b\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf x \in {\cal R}^{D \times 1}\)</span> is an input image
flattened to a vector, <span class="math notranslate nohighlight">\(t \in \{0, 1, \cdots, K\}\)</span> is a target
label, <span class="math notranslate nohighlight">\(\mathbf W \in {\cal R}^{K \times D}\)</span> is a weight matrix,
<span class="math notranslate nohighlight">\(\mathbf b \in {\cal R}^{K \times 1}\)</span> is a bias vector and
<span class="math notranslate nohighlight">\(\mathbf \Theta \equiv \left\{\mathbf W, \mathbf b\right\}\)</span>. Loss
function is defined as</p>
<div class="math notranslate nohighlight">
\[\mathbf L(\mathbf \Theta, \mathbf X) = \frac{1}{N} \sum_{\mathbf x, t \subset \mathbf X}
    -log \left(\left[\sigma\left(f(\mathbf x, \mathbf \Theta)\right)\right]_{t}\right)\]</div>
<p>where
<span class="math notranslate nohighlight">\(\mathbf X \equiv \left\{\mathbf x_1, t_1, \cdots, \mathbf x_N, t_N\right\}\)</span>
denotes a dataset the network trained on, <span class="math notranslate nohighlight">\(\sigma(\mathbf z)\)</span> is
softmax operation defined as
<span class="math notranslate nohighlight">\(\frac{\exp(-\mathbf z)}{\sum_{z \subset \mathbf z} \exp(-z)}\)</span>,
and <span class="math notranslate nohighlight">\(\left[\mathbf z\right]_i\)</span> denotes i-th element of
<span class="math notranslate nohighlight">\(\mathbf z\)</span>.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="python_api.html" class="btn btn-neutral float-right" title="NNabla Python API Demonstration Tutorial" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../tutorial.html" class="btn btn-neutral float-left" title="Python API Tutorial" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Sony Corporation

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>