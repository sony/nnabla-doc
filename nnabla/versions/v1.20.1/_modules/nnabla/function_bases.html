

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>nnabla.function_bases &mdash; Neural Network Libraries 1.20.1 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> Neural Network Libraries
          

          
          </a>

          
            
            
              <div class="version">
                1.20.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../python.html">Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cpp.html">C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data_exchange_file_format.html">Data exchange file format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../format.html">Data Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python/file_format_converter/file_format_converter.html">File format converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../support_status.html">Support Status</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">Contributing Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../license.html">License</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Neural Network Libraries</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>nnabla.function_bases</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for nnabla.function_bases</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) 2017 Sony Corporation. All Rights Reserved.</span>
<span class="c1"># </span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1"># </span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1"># </span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>

<span class="c1">#</span>
<span class="c1"># *WARNING*</span>
<span class="c1"># THIS FILE IS AUTO-GENERATED BY CODE GENERATOR.</span>
<span class="c1"># PLEASE DO NOT EDIT THIS FILE BY HAND!</span>
<span class="c1"># If you want to modify this file, edit following files.</span>
<span class="c1"># - python/src/nnabla/function_bases.py.tmpl</span>
<span class="c1"># - build-tools/code_generator/generate.py</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>

<span class="kn">from</span> <span class="nn">.context</span> <span class="kn">import</span> <span class="n">get_current_context</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">function</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">.auto_forward</span> <span class="kn">import</span> <span class="n">get_auto_forward</span>

<span class="c1"># Templates for function_api source building.</span>
<span class="n">FUNCTION_API_HEADER</span> <span class="o">=</span> <span class="s2">&quot;def </span><span class="si">{name}{signature}</span><span class="s2">:&quot;</span>
<span class="n">FUNCTION_API_BODY</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;ctx = get_current_context()</span>
<span class="s1">return _func_(ctx, </span><span class="si">{shortsignature}</span><span class="s1">)&#39;&#39;&#39;</span>


<span class="k">def</span> <span class="nf">function_api</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Decorator for making function called with current context.</span>
<span class="sd">    Some tricky things are done here so that signature and docstring are available.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">nnabla.utils.py23_compatible</span> <span class="kn">import</span> <span class="n">getargspec</span>
    <span class="kn">import</span> <span class="nn">inspect</span>

    <span class="n">name</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="vm">__name__</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="vm">__doc__</span>
    <span class="k">if</span> <span class="n">doc</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">doc</span> <span class="o">=</span> <span class="s2">&quot;No docstring.&quot;</span>

    <span class="c1"># Parsing argspecs</span>
    <span class="n">spec</span> <span class="o">=</span> <span class="n">getargspec</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
    <span class="n">defaults</span> <span class="o">=</span> <span class="n">spec</span><span class="o">.</span><span class="n">defaults</span>
    <span class="k">if</span> <span class="n">spec</span><span class="o">.</span><span class="n">defaults</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">defaults</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">defaults</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">args</span><span class="p">):</span>
        <span class="n">defaults</span> <span class="o">=</span> <span class="n">defaults</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="c1"># Creating signature</span>
    <span class="c1"># e.g. (x, weights, biases=None, n_outputs=None)</span>
    <span class="n">signature</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">formatargspec</span><span class="p">(</span>
        <span class="n">spec</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">spec</span><span class="o">.</span><span class="n">varargs</span><span class="p">,</span> <span class="n">spec</span><span class="o">.</span><span class="n">keywords</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>
    <span class="c1"># Creating signature without params and defaults</span>
    <span class="c1"># e.g. x, weights, biases, n_outputs</span>
    <span class="n">shortsignature</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">formatargspec</span><span class="p">(</span>
        <span class="n">spec</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">spec</span><span class="o">.</span><span class="n">varargs</span><span class="p">,</span> <span class="n">spec</span><span class="o">.</span><span class="n">keywords</span><span class="p">,</span> <span class="kc">None</span><span class="p">)[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Create code by string</span>
    <span class="n">src</span> <span class="o">=</span> <span class="p">(</span><span class="n">FUNCTION_API_HEADER</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;    &#39;</span> <span class="o">+</span>
                                                      <span class="n">x</span><span class="p">,</span> <span class="n">FUNCTION_API_BODY</span><span class="o">.</span><span class="n">splitlines</span><span class="p">())))</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="nb">locals</span><span class="p">())</span>

    <span class="c1"># Evaluate source code from string</span>
    <span class="n">code</span> <span class="o">=</span> <span class="nb">compile</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="s2">&quot;&lt;</span><span class="si">{name}</span><span class="s2">&gt;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="nb">locals</span><span class="p">()),</span> <span class="s1">&#39;single&#39;</span><span class="p">)</span>
    <span class="n">execdict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">_func_</span><span class="o">=</span><span class="n">func</span><span class="p">,</span> <span class="n">get_current_context</span><span class="o">=</span><span class="n">get_current_context</span><span class="p">)</span>
    <span class="n">exec</span><span class="p">(</span><span class="n">code</span><span class="p">,</span> <span class="n">execdict</span><span class="p">)</span>

    <span class="c1"># Get created function.</span>
    <span class="n">newfunc</span> <span class="o">=</span> <span class="n">execdict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
    <span class="c1"># DOC newfunc.__doc__ = FUNCTION_API_DOC.format(**locals())</span>
    <span class="n">doc</span> <span class="o">+=</span> <span class="s1">&#39;&#39;&#39;</span>

<span class="s1">    Note:</span>
<span class="s1">        All nnabla functions in :obj:`nnabla.functions` are decorated with the :obj:`nnabla.function_bases.function_api` decorator,</span>
<span class="s1">        which queries the current context and passes it into the first argument of the</span>
<span class="s1">        original function. The original function always takes a context as the first argument.</span>

<span class="s1">    &#39;&#39;&#39;</span>
    <span class="n">newfunc</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">doc</span>
    <span class="n">newfunc</span><span class="o">.</span><span class="n">__source__</span> <span class="o">=</span> <span class="n">src</span>
    <span class="n">newfunc</span><span class="o">.</span><span class="n">__function_api_base__</span> <span class="o">=</span> <span class="n">func</span>
    <span class="n">newfunc</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">=</span> <span class="vm">__name__</span>
    <span class="k">return</span> <span class="n">newfunc</span>





<div class="viewcode-block" id="affine"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.affine">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">affine</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Affine layer, also called as the fully connected layer. It calculates:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        {\mathbf y} = {\mathbf A} {\mathbf x} + {\mathbf b}.</span>
<span class="sd">    </span>
<span class="sd">    where :math:`{\mathbf x}` is the input and :math:`{\mathbf y}` is the output.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input N-D array with shape (:math:`M_0 \times ... \times M_{B-1} \times D_B \times ... \times D_N`). Dimensions before and after base_axis are flattened as if it is a matrix.</span>
<span class="sd">        weight(~nnabla.Variable): Weight matrix with shape (:math:`(D_B \times ... \times D_N) \times L_{0} \times \ldots \times L_{I}`)</span>
<span class="sd">            [parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Bias vector (:math:`L_{0} \times \ldots \times L_{I}`)</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        base_axis(int): Base axis of Affine operation. Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">            [default= `1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: :math:`(B + 1)`-D array. (:math:`M_0 \times ... \times M_{B-1} \times L_{0} \times \ldots \times L_{I}`)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Affine</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="rnn"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.rnn">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">rnn</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">weight_l0</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    RNN function implements Elman RNN with nonlineraity to input sequence.</span>
<span class="sd">    RNN function is defined as following:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        {\mathbf h_t} = {\mathbf \tanh}( {\mathbf w_{ih}} *{\mathbf x_t} + {\mathbf b_{ih}} + {\mathbf w_{hh}}* {\mathbf h_{(t-1)}} + {\mathbf b_{hh}}).</span>
<span class="sd">    </span>
<span class="sd">    We use the following notations to describe the inputs and outputs below.</span>
<span class="sd">    :math:`T`: sequcne length, :math:`B`: batch size, :math:`I`: input size, :math:`L`: number of layers, :math:`D`: number of directions, can be either 1 or 2, :math:`H`: hidden size.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">        * `Jeffrey Elman, Finding Structure in Time. &lt;https://crl.ucsd.edu/~elman/Papers/fsit.pdf&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input N-D array with shape :math:`(T, B, I)`.</span>
<span class="sd">        h(~nnabla.Variable): Input N-D array with shape :math:`(L, D, B, H)`.</span>
<span class="sd">        weight_l0(~nnabla.Variable): Input N-D array with shape :math:`(D, H, I + H)`.</span>
<span class="sd">            [parameter]</span>
<span class="sd">        weight(~nnabla.Variable): Input N-D array with shape :math:`(L-1, D, H, D * H + H)`.</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Input N-D array with shape :math:`(L, D, H)`.</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        num_layers(int): Number of layers in the network. If set to 1, only the weights for the first layer will be invoked. Default is 1.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        nonlinearity(string): Type of nonlinearity applied to input sequcne. Must be either tanh or relu. Default is tanh.</span>
<span class="sd">            [default= `&#39;tanh&#39;` ]</span>
<span class="sd">        dropout(float): Dropout ratio applied to parameters. Default is 0.0.</span>
<span class="sd">            [default= `0.0` ]</span>
<span class="sd">        bidirectional(bool): If True, bidirectional computation will be performed in each layer. Default is False.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        training(bool): Backpropagation will be performed only when it is true. Default is True.</span>
<span class="sd">            [default= `True` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output :math:`y` with shape :math:`(T, B, D * H)`</span>
<span class="sd">        ~nnabla.Variable: Output :math:`h_n` with shape :math:`(L, D, B, H)`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">weight_l0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">weight</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">bidirectional</span><span class="p">,</span> <span class="n">training</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="lstm"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.lstm">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">lstm</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">weight_l0</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    N-Step LSTM layer.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        {\mathbf f_t} &amp;=&amp; {\mathbf \sigma}( {\mathbf W_f} *{\mathbf x_t} + {\mathbf U_f}* {\mathbf h_{(t-1)}} + {\mathbf b_f})\\</span>
<span class="sd">        {\mathbf i_t} &amp;=&amp; {\mathbf \sigma}( {\mathbf W_i} *{\mathbf x_t} + {\mathbf U_i}* {\mathbf h_{(t-1)}} + {\mathbf b_i})\\</span>
<span class="sd">        {\mathbf o_t} &amp;=&amp; {\mathbf \sigma}( {\mathbf W_o} *{\mathbf x_t} + {\mathbf U_o}* {\mathbf h_{(t-1)}} + {\mathbf b_o})\\</span>
<span class="sd">        {\mathbf c_t} &amp;=&amp; {\mathbf f_t}\odot {\mathbf c_{(t-1)}} + {\mathbf i_t}\odot {\mathbf \tanh}({\mathbf W_c}*{\mathbf x_t} + {\mathbf U_c} *{\mathbf h_{(t-1)}} + {\mathbf b_c})\\</span>
<span class="sd">        {\mathbf h_t} &amp;=&amp; {\mathbf o_t} \odot {\mathbf \tanh}({\mathbf c_t}).</span>
<span class="sd">    </span>
<span class="sd">    We use the following notations to describe the inputs and outputs below.</span>
<span class="sd">    :math:`T`: sequcne length, :math:`B`: batch size, :math:`I`: input size, :math:`L`: number of layers, :math:`D`: number of directions, can be either 1 or 2, :math:`H`: hidden size.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">        * `S. Hochreiter and J. Schmidhuber, Long Short-Term Memory. &lt;https://www.bioinf.jku.at/publications/older/2604.pdf&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input N-D array with shape :math:`(T, B, I)`.</span>
<span class="sd">        h(~nnabla.Variable): Input N-D array with shape :math:`(L, D, B, H)`.</span>
<span class="sd">        c(~nnabla.Variable): Input N-D array with shape :math:`(L, D, B, H)`.</span>
<span class="sd">        weight_l0(~nnabla.Variable): weight parameters for the first layer. Shape is :math:`(D, 4, H, I + H)`.</span>
<span class="sd">            [parameter]</span>
<span class="sd">        weight(~nnabla.Variable): weight parameters for the second layer and above. Shape is :math:`(L-1, D, 4, H, D * H + H)`.</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Bias vector (:math:`L`). Shape is :math:`(L, D, 4, H)`.</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        num_layers(int): Number of layers in the network. If set to 1, only the weights for the first layer will be invoked. Default is 1.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        dropout(float): Dropout ratio applied to parameters. Default is 0.0.</span>
<span class="sd">            [default= `0.0` ]</span>
<span class="sd">        bidirectional(bool): If True, bidirecitonal computation will be performed in each layer. Default is False.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        training(bool): Backpropagation will be performed only when it is True. Default is True.</span>
<span class="sd">            [default= `True` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output :math:`y` with shape :math:`(T, B, D * H)`. Its memory layout can be reshaped as :math:`(T, B, D, H)`.</span>
<span class="sd">        ~nnabla.Variable: Output :math:`h_n` with shape :math:`(L, D, B, H)`</span>
<span class="sd">        ~nnabla.Variable: Output :math:`c_n` with shape :math:`(L, D, B, H)`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">weight_l0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">weight</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">bidirectional</span><span class="p">,</span> <span class="n">training</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="gru"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.gru">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">gru</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">weight_l0</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    N-Step GRU layer.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        {\mathbf r_t} &amp;=&amp; {\mathbf \sigma}( {\mathbf W_r} *{\mathbf x_t} + {\mathbf U_r}* {\mathbf h_{(t-1)}} + {\mathbf b_r})\\</span>
<span class="sd">        {\mathbf z_t} &amp;=&amp; {\mathbf \sigma}( {\mathbf W_z} *{\mathbf x_t} + {\mathbf U_z}* {\mathbf h_{(t-1)}} + {\mathbf b_z})\\</span>
<span class="sd">      {\mathbf n_t} &amp;=&amp; {\mathbf \tanh}( {\mathbf W_n}{\mathbf x_t}+ {\mathbf b_{in}}+ {\mathbf r_n}\odot( {\mathbf U_n}{\mathbf h_{t-1}}+ {\mathbf b_{hn}})) \\</span>
<span class="sd">      {\mathbf h_t} &amp;=&amp; (1- {\mathbf z_t})\odot {\mathbf n_t} + {\mathbf z_t}\odot {\mathbf h_{t-1}}.</span>
<span class="sd">    </span>
<span class="sd">    We use the following notations to describe the inputs and outputs below.</span>
<span class="sd">    :math:`T`: sequcne length, :math:`B`: batch size, :math:`I`: input size, :math:`L`: number of layers, :math:`D`: number of directions, can be either 1 or 2, :math:`H`: hidden size.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">        * `K. cho et al., Learning Phrases Representations using RNN Encoder-Decoder for Statistical Machine Translation. &lt;https://www.aclweb.org/anthology/D14-1179&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input N-D array with shape :math:`(T, B, I)`.</span>
<span class="sd">        h(~nnabla.Variable): Input N-D array with shape :math:`(L, D, B, H)`.</span>
<span class="sd">        weight_l0(~nnabla.Variable): weight parameters for the first layer. Shape is :math:`(D, 3, H, I + H)`.</span>
<span class="sd">            [parameter]</span>
<span class="sd">        weight(~nnabla.Variable): weight parameters for the second layer and above. Shape is :math:`(L-1, D, 3, H, D * H + H)`.</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Bias vector (:math:`L`). Shape is :math:`(L, D, 4, H)`.</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        num_layers(int): Number of layers in the network. If set to 1, only the weights for the first layer will be invoked. Default is 1.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        dropout(float): Dropout ratio applied to parameters. Default is 0.0.</span>
<span class="sd">            [default= `0.0` ]</span>
<span class="sd">        bidirectional(bool): If True, bidirecitonal computation will be performed in each layer. Default is False.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        training(bool): Backpropagation will be performed only when it is True. Default is True.</span>
<span class="sd">            [default= `True` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output :math:`y` with shape :math:`(T, B, D * H)`. Its memory layout can be reshaped as :math:`(T, B, D, H)`.</span>
<span class="sd">        ~nnabla.Variable: Output :math:`h_n` with shape :math:`(L, D, B, H)`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">weight_l0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">weight</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">bidirectional</span><span class="p">,</span> <span class="n">training</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="convolution"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.convolution">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">convolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">channel_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    N-D Convolution with bias.</span>
<span class="sd">    </span>
<span class="sd">    See references for dilated convolution (a.k.a. atrous convolution).</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">        * `Chen et al., DeepLab: Semantic Image Segmentation with Deep Convolutional</span>
<span class="sd">          Nets, Atrous Convolution, and Fully Connected CRFs.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1606.00915&gt;`_</span>
<span class="sd">    </span>
<span class="sd">        * `Yu et al., Multi-Scale Context Aggregation by Dilated Convolutions.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1511.07122&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">    </span>
<span class="sd">      Convolution is a computationally intensive operation that</span>
<span class="sd">      should preferrably be run with the `cudnn` backend. NNabla</span>
<span class="sd">      then uses CuDNN library functions to determine and cache the</span>
<span class="sd">      fastest algorithm for the given set of convolution parameters,</span>
<span class="sd">      which results in additional memory consumption which may pose</span>
<span class="sd">      a problem for GPUs with insufficient memory size. In that</span>
<span class="sd">      case, the `NNABLA_CUDNN_WORKSPACE_LIMIT` environment variable</span>
<span class="sd">      can be used to restrict the choice of algorithms to those that</span>
<span class="sd">      fit the given workspace memory limit, expressed in bytes. In</span>
<span class="sd">      some cases it may also be desired to restrict the automatic</span>
<span class="sd">      search to algorithms that produce deterministic (reproducable)</span>
<span class="sd">      results. This can be requested by setting the the environment</span>
<span class="sd">      variable `NNABLA_CUDNN_DETERMINISTIC` to a non-zero value.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): :math:`(B + 1 + N)`-D array (:math:`M_1 \times ... \times M_B \times C \times L_1 \times ... \times L_N`).</span>
<span class="sd">        weight(~nnabla.Variable): :math:`(2 + N)`-D array (:math:`C&#39; \times C \times K_1 \times ... \times K_N`).</span>
<span class="sd">            [parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Bias vector (:math:`C&#39;`).</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        base_axis(int): base axis :math:`B`.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">            [default= `(0,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        stride(:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">            [default= `(1,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        dilation(:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">            [default= `(1,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        group(int): Number of groups of channels. This makes the connection across channels sparser, by grouping connections along the mapping direction.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        channel_last(bool): If True, the last dimension is considered as channel dimension, a.k.a NHWC order.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: </span>
<span class="sd">            :math:`(B + 1 + N)`-D array (:math:`M_1 \times ... \times M_B \times C&#39; \times L&#39;_1 \times ... \times L&#39;_N`).</span>
<span class="sd">            </span>
<span class="sd">            A spatial size of the output is calculated as</span>
<span class="sd">            </span>
<span class="sd">            .. math::</span>
<span class="sd">            </span>
<span class="sd">              L&#39;_i = \frac{L_i + 2 p_i - d_i (k_i - 1) - 1}{s_i} + 1,</span>
<span class="sd">            </span>
<span class="sd">            where :math:`L_i` is the spatial size, :math:`p_i` is the padding, :math:`d_i` is the dilation, :math:`k_i` is the kernel size, and :math:`s_i` is the stride for :math:`i`-th spatial dimension. The same calculation can also be applied to the other spatial dimensions.</span>
<span class="sd">            </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">pad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">dilation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Convolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">channel_last</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">fused_convolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">channel_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">batch_stat</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">nonlinearity_args</span><span class="o">=</span><span class="nb">list</span><span class="p">(),</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fused operation of Convolution, Batch Normalization, Add2 and Activation.</span>
<span class="sd">    </span>
<span class="sd">    This is an equivalent operation to the following,</span>
<span class="sd">    but may be more computationally efficient depending on the backend implementation</span>
<span class="sd">    (currently we don&#39;t provide an efficient implementation on any backend).</span>
<span class="sd">    </span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">      h = F.convolution(x, weight, bias, *conv_opts)</span>
<span class="sd">      h = F.batch_normalization(h, beta, gamma, mean, variance, *bn_opts)</span>
<span class="sd">      y = F.relu(h + z)</span>
<span class="sd">    </span>
<span class="sd">    You can optionally disable either of batch normalization, residual addition and activation.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array of input.</span>
<span class="sd">        weight(~nnabla.Variable): `weight` in :meth:`~nnabla.functions.convolution`.</span>
<span class="sd">            </span>
<span class="sd">            [parameter]</span>
<span class="sd">        bias(~nnabla.Variable): `bias` in :meth:`~nnabla.functions.convolution`.</span>
<span class="sd">            </span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        beta(~nnabla.Variable): `beta` in :meth:`~nnabla.functions.batch_normalization`.</span>
<span class="sd">            </span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        gamma(~nnabla.Variable): `gamma` in :meth:`~nnabla.functions.batch_normalization`.</span>
<span class="sd">            </span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        mean(~nnabla.Variable): `mean` in :meth:`~nnabla.functions.batch_normalization`.</span>
<span class="sd">            </span>
<span class="sd">            [optional]</span>
<span class="sd">        variance(~nnabla.Variable): `variance` in :meth:`~nnabla.functions.batch_normalization`.</span>
<span class="sd">            </span>
<span class="sd">            [optional]</span>
<span class="sd">        z(~nnabla.Variable): N-D array of a residual input. By specifying None, the activation function will follow immediately after BN operation.</span>
<span class="sd">            [optional]</span>
<span class="sd">        base_axis(int): `base_axis` in :meth:`~nnabla.functions.convolution`. Note that the batch normalization `axes` is determined by this and `channel_last` option.</span>
<span class="sd">            </span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): `pad` in :meth:`~nnabla.functions.convolution`.</span>
<span class="sd">            </span>
<span class="sd">            [default= `(0,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        stride(:obj:`tuple` of :obj:`int`): `stride` in :meth:`~nnabla.functions.convolution`.</span>
<span class="sd">            </span>
<span class="sd">            [default= `(1,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        dilation(:obj:`tuple` of :obj:`int`): `dilation` in :meth:`~nnabla.functions.convolution`.</span>
<span class="sd">            </span>
<span class="sd">            [default= `(1,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        group(int): `group` in :meth:`~nnabla.functions.convolution`.</span>
<span class="sd">            </span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        channel_last(bool): `channel_last` in :meth:`~nnabla.functions.convolution`.group</span>
<span class="sd">            </span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        decay_rate(float): `decay_rate` in :meth:`~nnabla.functions.batch_normalization`.</span>
<span class="sd">            </span>
<span class="sd">            [default= `0.9` ]</span>
<span class="sd">        eps(float): `eps` in :meth:`~nnabla.functions.batch_normalization`.</span>
<span class="sd">            </span>
<span class="sd">            [default= `1e-05` ]</span>
<span class="sd">        batch_stat(bool): `batch_stat` in :meth:`~nnabla.functions.batch_normalization`.</span>
<span class="sd">            </span>
<span class="sd">            [default= `True` ]</span>
<span class="sd">        nonlinearity(string): Activation type as string. The following is a list of available activation types</span>
<span class="sd">            and optional parameters specified as a vector of float by `nonlinearity_args`.</span>
<span class="sd">            </span>
<span class="sd">            =============== ===============================</span>
<span class="sd">            Activation type Arguments (`nonlinearity_args`)</span>
<span class="sd">            =============== ===============================</span>
<span class="sd">            identity        No argument</span>
<span class="sd">            relu            No argument</span>
<span class="sd">            sigmoid         No argument</span>
<span class="sd">            tanh            No argument</span>
<span class="sd">            leaky_relu      [alpha] (see LeakyReLU doc)</span>
<span class="sd">            elu             [alpha] (see ELU doc)</span>
<span class="sd">            relu6           No argument</span>
<span class="sd">            =============== ===============================</span>
<span class="sd">            </span>
<span class="sd">            [default= `&#39;relu&#39;` ]</span>
<span class="sd">        nonlinearity_args(repeated float): Optional arguments of nonlinearity as a vector of float.</span>
<span class="sd">            See the description of the `nonlinearity` argument.</span>
<span class="sd">            </span>
<span class="sd">            [default= `list()` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">pad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">dilation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">beta</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">beta</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">gamma</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">gamma</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">mean</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">mean</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">variance</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">variance</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">z</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">z</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">FusedConvolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">channel_last</span><span class="p">,</span> <span class="n">decay_rate</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">batch_stat</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="p">,</span> <span class="n">nonlinearity_args</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<div class="viewcode-block" id="depthwise_convolution"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.depthwise_convolution">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">depthwise_convolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">multiplier</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    N-D Depthwise Convolution with bias.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">        * `F. Chollet. Xception: Deep Learning with Depthwise Separable Convolutions.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1610.02357&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): :math:`(B + 1 + N)`-D array (:math:`M_1 \times ... \times M_B \times C \times L_1 \times ... \times L_N`).</span>
<span class="sd">        weight(~nnabla.Variable): :math:`(1 + N)`-D array (:math:`C \times K_1 \times ... \times K_N`).</span>
<span class="sd">            [parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Bias vector (:math:`C&#39;`).</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        base_axis(int): base axis :math:`B`.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">            [default= `(0,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        stride(:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">            [default= `(1,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        dilation(:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">            [default= `(1,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        multiplier(int): Number of output feature maps per input feature map.</span>
<span class="sd">            [default= `1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: </span>
<span class="sd">            :math:`(B + 1 + N)`-D array (:math:`M_1 \times ... \times M_B \times C&#39; \times L&#39;_1 \times ... \times L&#39;_N`).</span>
<span class="sd">            </span>
<span class="sd">            The output map size :math:`C&#39;` is :math:`C` multiplied by :math:`m`</span>
<span class="sd">            </span>
<span class="sd">            .. math::</span>
<span class="sd">            </span>
<span class="sd">              C&#39; =  m \times C,</span>
<span class="sd">            </span>
<span class="sd">            where :math:`m` is the multiplier.</span>
<span class="sd">            </span>
<span class="sd">            A spatial size of the output is calculated as</span>
<span class="sd">            </span>
<span class="sd">            .. math::</span>
<span class="sd">            </span>
<span class="sd">              L&#39;_i = \frac{L_i + 2 p_i - d_i (k_i - 1) - 1}{s_i} + 1,</span>
<span class="sd">            </span>
<span class="sd">            where :math:`L_i` is the spatial size, :math:`p_i` is the padding, :math:`d_i` is the dilation, :math:`k_i` is the kernel size, and :math:`s_i` is the stride for :math:`i`-th spatial dimension. The same calculation can also be applied to the other spatial dimensions.</span>
<span class="sd">            </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">pad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">dilation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">DepthwiseConvolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">multiplier</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="deconvolution"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.deconvolution">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">deconvolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">channel_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">output_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    N-D deconvolution, also known as transposed convolution, with bias operates backward convolution (derivative of the output w.r.t. the input) plus channel-wise learned bias.</span>
<span class="sd">    </span>
<span class="sd">    The weights are specified in the same manner as :meth:`~nnabla.functions.convolution` , as if it was an ordinary convolution function.</span>
<span class="sd">    The forward operation of :meth:`~nnabla.functions.deconvolution` will then be operationally equivalent to the backward pass of :meth:`~nnabla.functions.convolution` .</span>
<span class="sd">    Therefore, the number of input channels (can be seen as output channels of forward convolution) is specified in the first dimension, and the number of the output channels divided by the number of groups is specified in the second dimension.</span>
<span class="sd">    </span>
<span class="sd">    For `stride &gt; 1`, a parameter-wise identical deconvolution on the output</span>
<span class="sd">    of a convolution may not produce the same output shape as the input to</span>
<span class="sd">    the convolution if, due to striding, the convolution did not fully cover</span>
<span class="sd">    the input spatial dimension. The `output_padding` parameter can then be</span>
<span class="sd">    used to appropriately increase the calculated output shape. Note that</span>
<span class="sd">    this is used to find the output shape for the deconvolution operation,</span>
<span class="sd">    but not to add zero-padding to the output.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): :math:`(B + 1 + N)`-D array (:math:`M_1 \times ... \times M_B \times C \times L_1 \times ... \times L_N`).</span>
<span class="sd">        weight(~nnabla.Variable): :math:`(2 + N)`-D array (:math:`C \times C&#39; \times K_1 \times ... \times K_N`).</span>
<span class="sd">            [parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Bias vector (:math:`C&#39;`).</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        base_axis(int): base axis :math:`B`.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">            [default= `(0,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        stride(:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">            [default= `(1,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        dilation(:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">            [default= `(1,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        group(int): Number of groups of channels. This makes the connection across channels sparser, by grouping connections along the mapping direction.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        channel_last(bool): If True, the last dimension is considered as channel dimension, a.k.a NHWC order.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        output_padding(:obj:`tuple` of :obj:`int`): Additional size added to the output shape.</span>
<span class="sd">            [default= `(0,) * (len(x.shape) - (base_axis+1))` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: </span>
<span class="sd">            :math:`(B + 1 + N)`-D array (:math:`M_1 \times ... \times M_B \times C&#39; \times L&#39;_1 \times ... \times L&#39;_N`).</span>
<span class="sd">            </span>
<span class="sd">            A spatial size of the output is calculated as</span>
<span class="sd">            </span>
<span class="sd">            .. math::</span>
<span class="sd">            </span>
<span class="sd">              L&#39;_i =s_i (L_i - 1) - 2 p_i + d_i (k_i - 1) + 1,</span>
<span class="sd">            </span>
<span class="sd">            where :math:`s_i` is the stride, :math:`L_i` is the spatial size, :math:`p_i` is the padding, :math:`d_i` is the dilation, and :math:`k_i` is the kernel size for :math:`i`-th spatial dimension. The same calculation can also be applied to the other spatial dimensions.</span>
<span class="sd">            </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">pad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">dilation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">output_padding</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">output_padding</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Deconvolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">channel_last</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="depthwise_deconvolution"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.depthwise_deconvolution">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">depthwise_deconvolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">divisor</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Depthwise deconvolution computes the transposed depthwise convolution with bias for one-dimensional and two-dimensional input data.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): :math:`(B + 1 + N)`-D array (:math:`M_1 \times ... \times M_B \times C \times L_1 \times ... \times L_N`).</span>
<span class="sd">        weight(~nnabla.Variable): :math:`(1 + N)`-D array (:math:`C \times K_1 \times ... \times K_N`).</span>
<span class="sd">            [parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Bias vector (:math:`C&#39;`).</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        base_axis(int): base axis :math:`B`.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">            [default= `(0,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        stride(:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">            [default= `(1,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        dilation(:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">            [default= `(1,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        divisor(int): Number of input feature maps per output feature map.</span>
<span class="sd">            [default= `1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: </span>
<span class="sd">            :math:`(B + 1 + N)`-D array (:math:`M_1 \times ... \times M_B \times C&#39; \times L&#39;_1 \times ... \times L&#39;_N`).</span>
<span class="sd">            </span>
<span class="sd">            The output map size :math:`C&#39;` is :math:`C` multiplied by :math:`m`</span>
<span class="sd">            </span>
<span class="sd">            .. math::</span>
<span class="sd">            </span>
<span class="sd">              C&#39; =  \frac{C}{d},</span>
<span class="sd">            </span>
<span class="sd">            where :math:`d` is the divisor.</span>
<span class="sd">            </span>
<span class="sd">            A spatial size of the output is calculated as</span>
<span class="sd">            </span>
<span class="sd">            .. math::</span>
<span class="sd">              L&#39;_i =s_i (L_i - 1) - 2 p_i + d_i (k_i - 1) + 1,</span>
<span class="sd">            </span>
<span class="sd">            where :math:`s_i` is the stride, :math:`L_i` is the spatial size, :math:`p_i` is the padding, :math:`d_i` is the dilation, and :math:`k_i` is the kernel size for :math:`i`-th spatial dimension. The same calculation can also be applied to the other spatial dimensions.</span>
<span class="sd">            </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">pad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">dilation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">DepthwiseDeconvolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">divisor</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="deformable_convolution"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.deformable_convolution">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">deformable_convolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">deformable_group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">channel_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    2-D Deformable Convolution with bias.</span>
<span class="sd">    Another convolution with fixed output channels must be passed externally to calculate the offsets and mask.</span>
<span class="sd">    Mask should be normalized to :math:`[0,1]` interval.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">      \begin{eqnarray}</span>
<span class="sd">        y(p) = \sum_{k=1}^{K} w_k \cdot x(p + p_k + \Delta p_k) \cdot \Delta m_k,</span>
<span class="sd">      \end{eqnarray}</span>
<span class="sd">    </span>
<span class="sd">    where :math:`x` and :math:`y` are input and output, :math:`w_k` is the weight, :math:`p` is the pixel location of interest, :math:`p_k` is the fixed displacement e.g., :math:`p_k \in \{(-1, -1), (-1, 0), \ldots (1, 1)\}` for the 2D 3x3 receptive field, :math:`\Delta p_k` is the learnable displacement, and :math:`\Delta m_k` is the learnable scale normalized in :math:`[0, 1]` by a function like the sigmoid. Note that :math:`\Delta p_k` and :math:`\Delta m_k` are sample-dependent, location-dependent, and feature-independent.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">        * `Dai et al., Deformable Convolutional Networks.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1703.06211&gt;`_</span>
<span class="sd">    </span>
<span class="sd">        * `Zhu et al., Deformable ConvNets v2: More Deformable, Better Results.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1811.11168&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): :math:`(B + 1 + N)`-D array (:math:`M_1 \times ... \times M_B \times C \times L_1 \times ... \times L_N`).</span>
<span class="sd">        weight(~nnabla.Variable): :math:`(2 + N)`-D array (:math:`C&#39; \times C \times K_1 \times ... \times K_N`).</span>
<span class="sd">            [parameter]</span>
<span class="sd">        offset(~nnabla.Variable): Offsets for deformable convolutions. Shape is fixed to :math:`(N, deformable_group \times 2 \times Kh \times Kw, H, W)`. Offsets must be calculated externally through a separate convolution layer.</span>
<span class="sd">        mask(~nnabla.Variable): Normalized mask for deformable convolutions v2. Shape is fixed to :math:`(N, deformable_group \times 2 \times Kh \times Kw, H, W)`. Masks must be calculated externally together with the offsets through a separate convolution layer.</span>
<span class="sd">            [optional]</span>
<span class="sd">        bias(~nnabla.Variable): Bias vector (:math:`C&#39;`).</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        base_axis(int): base axis :math:`B`.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">            [default= `(0,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        stride(:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">            [default= `(1,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        dilation(:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">            [default= `(1,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        group(int): Number of groups of channels. This makes the connection across channels sparser, by grouping connections along the mapping direction.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        deformable_group(int): Number of deformable groups of channels.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        channel_last(bool): If True, the last dimension is considered as channel dimension, a.k.a NHWC order.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: </span>
<span class="sd">            :math:`(B + 1 + N)`-D array (:math:`M_1 \times ... \times M_B \times C&#39; \times L&#39;_1 \times ... \times L&#39;_N`).</span>
<span class="sd">            </span>
<span class="sd">            A spatial size of the output is calculated as</span>
<span class="sd">            </span>
<span class="sd">            .. math::</span>
<span class="sd">            </span>
<span class="sd">              L&#39;_i = \frac{L_i + 2 p_i - d_i (k_i - 1) - 1}{s_i} + 1,</span>
<span class="sd">            </span>
<span class="sd">            where :math:`L_i` is the spatial size, :math:`p_i` is the padding, :math:`d_i` is the dilation, :math:`k_i` is the kernel size, and :math:`s_i` is the stride for :math:`i`-th spatial dimension. The same calculation can also be applied to the other spatial dimensions.</span>
<span class="sd">            </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">pad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">dilation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">offset</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">mask</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">DeformableConvolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">deformable_group</span><span class="p">,</span> <span class="n">channel_last</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="adaptive_separable_convolution"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.adaptive_separable_convolution">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">adaptive_separable_convolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">vertical_kernel</span><span class="p">,</span> <span class="n">horizontal_kernel</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    2-D Adaptive Separable Convolution for NCHW (the channel-first tensor).</span>
<span class="sd">    Sample and pixel dependent vertical and horizontal kernels are dynamically generated ones,</span>
<span class="sd">    which are used for approximating a feature-independent 2-D kernel in this function.</span>
<span class="sd">    Thus, the kernel used in this function is dependent on samples and pixels but independent on features.</span>
<span class="sd">    </span>
<span class="sd">    If the padding is needed, use the pad function to the input :math:`x` before this function.</span>
<span class="sd">    </span>
<span class="sd">    Adaptive separable convolution is formulated as</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">      \tilde{I}(c, h, w) = \sum_{j, i} K_v(j, h, w) \times K_h(i, h, w) \times I(c, h + j, w + i),</span>
<span class="sd">    </span>
<span class="sd">    where :math:`I(c, h, w)` and :math:`\tilde{I}(c, h, w)` are the input and output images</span>
<span class="sd">    at :math:`c`-th channel, :math:`h`-th height, :math:`w`-th width.</span>
<span class="sd">    :math:`K_V(:, h, w)` and :math:`K_h(:, h, w)` are vertical and horizontal 1-D kernels</span>
<span class="sd">    at :math:`h`-th height and :math:`w`-th width.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">        * `Simon Niklaus, Long Mai, Feng Liu,</span>
<span class="sd">          Video Frame Interpolation via Adaptive Separable Convolution,</span>
<span class="sd">          &lt;https://arxiv.org/abs/1708.01692&gt;`_</span>
<span class="sd">    </span>
<span class="sd">        * `Mart Kartasev, Carlo Rapisarda, Dominik Fay,</span>
<span class="sd">          Implementing Adaptive Separable Convolution for Video Frame Interpolation,</span>
<span class="sd">          &lt;https://arxiv.org/abs/1809.07759&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): :math:`4-D` array (:math:`B \times C \times H \times W`)</span>
<span class="sd">        vertical_kernel(~nnabla.Variable): :math:`4-D` array (:math:`B \times K_v \times H \times W`)</span>
<span class="sd">        horizontal_kernel(~nnabla.Variable): :math:`4-D` array (:math:`B \times K_h \times H \times W`)</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: :math:`4-D` array (:math:`B \times C \times H - K_v + 1 \times W - K_h + 1`)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">AdaptiveSeparableConvolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">vertical_kernel</span><span class="p">,</span> <span class="n">horizontal_kernel</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="max_pooling"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.max_pooling">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">max_pooling</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_border</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">channel_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Max pooling. It pools the maximum values inside the scanning kernel:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_{i_1, i_2} = \max_{k_1, k_2 \in K} (x_{i_1 + k_1, i_2 + k_2})</span>
<span class="sd">    </span>
<span class="sd">    where :math:`x_{i_1 + k_1, i_2 + k_2}` is the input and :math:`y_{i_1, i_2}` is the output.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable.</span>
<span class="sd">        kernel(:obj:`tuple` of :obj:`int`): Kernel sizes for each spatial axis.</span>
<span class="sd">        stride(:obj:`tuple` of :obj:`int`): Subsampling factors for each spatial axis.</span>
<span class="sd">            [default= `kernel` ]</span>
<span class="sd">        ignore_border(bool): If false, kernels covering borders are also considered for the output.</span>
<span class="sd">            [default= `True` ]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): Border padding values for each spatial axis. Padding will be added both sides of the dimension.</span>
<span class="sd">            [default= `(0,) * len(kernel)` ]</span>
<span class="sd">        channel_last(bool): If True, the last dimension is considered as channel dimension, a.k.a NHWC order.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Maximum values variable</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">kernel</span>
    <span class="k">if</span> <span class="n">pad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">MaxPooling</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">ignore_border</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">channel_last</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="average_pooling"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.average_pooling">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">average_pooling</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_border</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">channel_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">including_pad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Average pooling. It pools the averaged values inside the scanning kernel:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_{i_1, i_2} = \frac{1}{K_1 K_2} \sum_{k1} \sum_{k2} x_{i_1 + k_1, i_2 + k_2}</span>
<span class="sd">    </span>
<span class="sd">    where :math:`x_{i_1 + k_1, i_2 + k_2}` is the input and :math:`y_{i_1, i_2}` is the output.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable.</span>
<span class="sd">        kernel(:obj:`tuple` of :obj:`int`): Kernel sizes for each spatial axis.</span>
<span class="sd">        stride(:obj:`tuple` of :obj:`int`): Subsampling factors for each spatial axis.</span>
<span class="sd">            [default= `kernel` ]</span>
<span class="sd">        ignore_border(bool): If false, kernels covering borders are also considered for the output.</span>
<span class="sd">            [default= `True` ]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): Border padding values for each spatial axis. Padding will be added both sides of the dimension.</span>
<span class="sd">            [default= `(0,) * len(kernel)` ]</span>
<span class="sd">        channel_last(bool): If True, the last dimension is considered as channel dimension, a.k.a NHWC order.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        including_pad(bool): If true, border padding values are considered for the output.</span>
<span class="sd">            [default= `True` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Average values variable</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">kernel</span>
    <span class="k">if</span> <span class="n">pad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">AveragePooling</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">ignore_border</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">channel_last</span><span class="p">,</span> <span class="n">including_pad</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="global_average_pooling"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.global_average_pooling">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">global_average_pooling</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;.. WARNING::</span>
<span class="sd">      This function is experimental support, so please do not actively use it.</span>
<span class="sd">    </span>
<span class="sd">    Global average pooling. It pools an averaged value from the whole image</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Average values variable</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">GlobalAveragePooling</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="sum_pooling"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.sum_pooling">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">sum_pooling</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_border</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">channel_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sum pooling. It pools the summed values inside the scanning kernel:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_{i_1, i_2} = \sum_{k1} \sum_{k2} x_{i_1 + k_1, i_2 + k_2}</span>
<span class="sd">    </span>
<span class="sd">    where :math:`x_{i_1 + k_1, i_2 + k_2}` is the input and :math:`y_{i_1, i_2}` is the output.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable.</span>
<span class="sd">        kernel(:obj:`tuple` of :obj:`int`): Kernel sizes for each spatial axis.</span>
<span class="sd">        stride(:obj:`tuple` of :obj:`int`): Subsampling factors for each spatial axis.</span>
<span class="sd">            [default= `kernel` ]</span>
<span class="sd">        ignore_border(bool): If false, kernels covering borders are also considered for the output.</span>
<span class="sd">            [default= `True` ]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): Border padding values for each spatial axis. Padding will be added both sides of the dimension.</span>
<span class="sd">            [default= `(0,) * len(kernel)` ]</span>
<span class="sd">        channel_last(bool): If True, the last dimension is considered as channel dimension, a.k.a NHWC order.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Summed values variable</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">kernel</span>
    <span class="k">if</span> <span class="n">pad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">SumPooling</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">ignore_border</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">channel_last</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="unpooling"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.unpooling">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">unpooling</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">channel_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Inverse operation of pooling. It spreads the input values:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_{k_1 i_1 + j_1, k_2 i_2 + j_2} = x_{i_1, i_2}</span>
<span class="sd">    </span>
<span class="sd">    where :math:`_{i_1, i_2}` is the input and :math:`y_{k_1 i_1 + j_1, k_2 i_2 + j_2}` is the output.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable.</span>
<span class="sd">        kernel(:obj:`tuple` of :obj:`int`): Kernel sizes for each spatial axis.</span>
<span class="sd">        channel_last(bool): If True, the last dimension is considered as channel dimension, a.k.a NHWC order.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Spread values variable</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Unpooling</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">channel_last</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="embed"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.embed">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">embed</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Embed slices of a matrix/tensor with indexing array/tensor.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Indices with shape :math:`(I_0, ..., I_N)`</span>
<span class="sd">        w(~nnabla.Variable): Weights with shape :math:`(W_0, ..., W_M)`</span>
<span class="sd">            [parameter]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output with shape :math:`(I_0, ..., I_N, W_1, ..., W_M)`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Embed</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="sigmoid"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.sigmoid">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise sigmoid function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        f(x) = \frac{1}{1 + \exp(-x)},</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="swish"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.swish">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">swish</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise swish function, by Ramachandran et al. (2017).</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        y_i = \frac{x_i}{1 + \exp(-x_i)},</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">        * `Prajit Ramachandran, Barret Zoph, and Quoc V. Le, Swish: a Self-Gated Activation Function, arXiv:1710.05941 [cs.NE]</span>
<span class="sd">          &lt;https://arxiv.org/abs/1710.05941&gt;`_</span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Swish</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="tanh"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.tanh">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise hyperbolic tangent (tanh) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \tanh (x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="relu"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.relu">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise Rectified Linear Unit (ReLU) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \max (0, x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        inplace(bool): The output array is shared with the input array if True.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="leaky_relu"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.leaky_relu">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">leaky_relu</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise Leaky Rectified Linear Unit (ReLU) function.</span>
<span class="sd">    </span>
<span class="sd">    It is defined as:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \alpha * \min(0, x_i) + \max (0, x_i)</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        alpha(float): The slope value multiplied to negative numbers. :math:`\alpha` in the definition.</span>
<span class="sd">            [default= `0.1` ]</span>
<span class="sd">        inplace(bool): The output array is shared with the input array if True.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="softmax"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.softmax">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax normalization. Calculates</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)}</span>
<span class="sd">    </span>
<span class="sd">    along the dimension specified by `axis`, where :math:`x_i` is the input and :math:`y_i` is the output.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array. Typically indicates a score.</span>
<span class="sd">        axis(int): Axis normalization is taken.</span>
<span class="sd">            [default= `len(x.shape) - 1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="log_softmax"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.log_softmax">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fused operation of Softmax normalization followed by log, which is defined as</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \log \frac{\exp(x_i)}{\sum_j \exp(x_j)},</span>
<span class="sd">    </span>
<span class="sd">    where :math:`y_i` is the input and :math:`x_i` is the output at i-th channel.</span>
<span class="sd">    An advantage of this fusion is reducing the numerical instability due to the log application.</span>
<span class="sd">    </span>
<span class="sd">    The original definition can be rewritten as</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = x_i - \max_j(x_j) - \log\left(\sum_j \exp(x_j - \max_k(x_k))\right).</span>
<span class="sd">    </span>
<span class="sd">    It is more stable as a log is always applied to a value :math:`\ge e`, while a log can be evaluated for 0 in the non-fused operation.</span>
<span class="sd">    </span>
<span class="sd">    Also, backward gradient computation is more stable than the original one as it doesn&#39;t perform division by x due to a gradient of log. The definition is as following.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        dx_i = dy_i - y_i * \sum_j dy_j</span>
<span class="sd">    </span>
<span class="sd">    where :math:`dx_i` and :math:`dy_i` denote gradients of loss</span>
<span class="sd">    wrt :math:`x_i` and :math:`y_i` respectively.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array. Typically indicates a score.</span>
<span class="sd">        axis(int): Axis normalization is taken.</span>
<span class="sd">            [default= `len(x.shape) - 1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="elu"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.elu">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">elu</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise Exponential Linear Unit (ELU) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i= \left\{</span>
<span class="sd">        \begin{array}{ll}</span>
<span class="sd">        x_i &amp; (x &gt; 0)\\</span>
<span class="sd">        \alpha (\exp(x_i) - 1) &amp; (x \leq 0)</span>
<span class="sd">        \end{array} \right..</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">        * `Clevart et al., Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs).</span>
<span class="sd">          &lt;http://arxiv.org/abs/1511.07289&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        alpha(float): Coefficient for negative outputs. :math:`\alpha` in definition</span>
<span class="sd">            [default= `1.0` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ELU</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="selu"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.selu">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">selu</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.05070098735548</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.673263242354377</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise Scaled Exponential Linear Unit (SELU) function by Klambauer et al. (2017).</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i= \lambda \left\{</span>
<span class="sd">        \begin{array}{ll}</span>
<span class="sd">        x_i &amp; (x &gt; 0)\\</span>
<span class="sd">        \alpha (\exp(x_i) - 1) &amp; (x \leq 0)</span>
<span class="sd">        \end{array} \right..</span>
<span class="sd">    </span>
<span class="sd">    The coefficients :math:`\lambda` and :math:`\alpha` default to the following values :math:`\lambda_{01}` and :math:`\alpha_{01}`, respectively, provided by Klambauer et al. (2017):</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{lll}</span>
<span class="sd">          \lambda_{01} &amp;=&amp;  \left(  1 - \operatorname{erfc}\left( \frac{1}{\sqrt{2}} \right) \sqrt{e}  \right)</span>
<span class="sd">                      \sqrt{2 \pi} \\</span>
<span class="sd">                     &amp;&amp; \left(</span>
<span class="sd">                          2 \operatorname{erfc} \left( \sqrt{2} \right) e^2</span>
<span class="sd">                          + \pi \operatorname{erfc}\left( \frac{1}{\sqrt{2}} \right)^2 e</span>
<span class="sd">                          \right. \\</span>
<span class="sd">                     &amp;&amp; \left.</span>
<span class="sd">                          - 2(2 + \pi) \operatorname{erfc} \left( \frac{1}{\sqrt{2}} \right) \sqrt{e}</span>
<span class="sd">                          + \pi + 2</span>
<span class="sd">                     \right)^{-1/2}  \\</span>
<span class="sd">                  &amp;\approx&amp; 1.0507 \\</span>
<span class="sd">          \alpha_{01} &amp;=&amp;  - \frac</span>
<span class="sd">                        {\sqrt {\frac {2}{\pi}}}</span>
<span class="sd">                        {\operatorname{erfc} \left( \frac{1}{\sqrt{2}} \right) \exp \left(\frac {1} {2} \right) - 1} \\</span>
<span class="sd">                  &amp;\approx&amp; 1.67326</span>
<span class="sd">        \end{array}</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">        * `Klambauer, G., Unterthiner, T., Mayr, A., &amp; Hochreiter, S. (2017).</span>
<span class="sd">          Self-Normalizing Neural Networks. In Advances in Neural Information</span>
<span class="sd">          Processing Systems (NIPS). &lt;https://arxiv.org/abs/1706.02515&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        scale(float): The coefficient :math:`\lambda` in the definition.</span>
<span class="sd">            [default= `1.05070098735548` ]</span>
<span class="sd">        alpha(float): The coefficient :math:`\alpha` in the definition.</span>
<span class="sd">            [default= `1.673263242354377` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">SELU</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="crelu"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.crelu">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">crelu</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise Concatenated Rectified Linear Unit (CReLU) function.</span>
<span class="sd">    This function calculates the ReLU of :math:`x` and :math:`-x` , then concatenates the results together at a specified axis,</span>
<span class="sd">    and returns the resulting array.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">        * `Wenling Shang, Kihyuk Sohn, Diogo Almeida, Honglak Lee.</span>
<span class="sd">          Understanding and Improving Convolutional Neural Networks</span>
<span class="sd">          via Concatenated Rectified Linear Units.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1603.05201&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        axis(int): The ReLU activations of positive inputs and negative inputs are concatenated at axis.</span>
<span class="sd">            [default= `1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array where axis dimension is doubled by concatenating.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">CReLU</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="celu"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.celu">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">celu</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise Concatenated Exponential Linear Unit (CELU) function.</span>
<span class="sd">    Concatenates ELU outputs of positive and negative inputs together at specified axis.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        alpha(float): Coefficient for negative outputs. :math:`\alpha` in definition.</span>
<span class="sd">            [default= `1.0` ]</span>
<span class="sd">        axis(int): The ELU activations of positive inputs and negative inputs are concatenated at axis.</span>
<span class="sd">            [default= `1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array where axis dimension is doubled by concatenating.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">CELU</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">axis</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="prelu"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.prelu">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">prelu</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise Parametrized Rectified Linear Unit function. Calculates:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \max(0, x_i) + w_i \min(0, x_i)</span>
<span class="sd">    </span>
<span class="sd">    where negative slope :math:`w` is learned and can vary across channels (an</span>
<span class="sd">    axis specified with `base_axis`).</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): (N-D array) Input</span>
<span class="sd">        x1(~nnabla.Variable): (N-D array) Weights</span>
<span class="sd">        base_axis(int): Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">            [default= `1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">PReLU</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="gelu"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.gelu">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">gelu</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gaussian Error Unit (GELU) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        GELU(x) = xP(X \leq  x) = x \Phi (x)</span>
<span class="sd">    </span>
<span class="sd">    which is approximated by</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        GELU(x) = 0.5x (1 + \tanh ( \sqrt(2/\pi)(x + 0.044715x^3) ))</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">        * `Dan Hendrycks and Kevin Gimpel.</span>
<span class="sd">          Gaussian Error Linera Units (GELUs).</span>
<span class="sd">          &lt;https://arxiv.org/abs/1606.08415&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">GELU</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="mish"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.mish">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">mish</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Mish activation function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        Mish(x) = x \tanh(\log(1+\exp(x_i)))</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">        * `Diganta Misra.</span>
<span class="sd">          Mish A Self Regularized Non-Monotonic Neural Activation Function.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1908.08681&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Mish</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="relu6"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.relu6">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">relu6</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise ReLU6 function.</span>
<span class="sd">    Capping ReLU activation to 6 is often observed to learn sparse features earlier.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        ReLU6(x) = \min(\max(0,x,),6)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="hard_sigmoid"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.hard_sigmoid">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">hard_sigmoid</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Segment-wise linear approximation of sigmoid.</span>
<span class="sd">    Preferable when speed of computation is more important than precision.</span>
<span class="sd">    Returns :math:`0` if :math:`x &lt; -2.5`.</span>
<span class="sd">    Returns :math:`1` if :math:`x&gt; 2.5`.</span>
<span class="sd">    Returns :math:`0.2x + 0.5` if :math:`-2.5 &lt;= x &lt;= 2.5`.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">HardSigmoid</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="hard_tanh"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.hard_tanh">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">hard_tanh</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise HardTanh function.</span>
<span class="sd">    Computationally cheaper than Tanh function.</span>
<span class="sd">    Returns :math:`1` if :math:`x &gt; 1`.</span>
<span class="sd">    Returns :math:`-1` if :math:`x &lt; -1`.</span>
<span class="sd">    Returns :math:`x` otherwise.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">HardTanh</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="log_sigmoid"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.log_sigmoid">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">log_sigmoid</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise LogSigmoid function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        LogSigmoid(x) = \log(1/(1+\exp(-x_i)))</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LogSigmoid</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="softplus"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.softplus">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">softplus</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise SoftPlus function.</span>
<span class="sd">    Unlike Sigmoid and Tanh that have upper and lower bound, SoftPlus is only lower-bounded by 0.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        SoftPlus(x) = \frac{1}{\beta} * \log(1+\exp(\beta * x_i))</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        beta(float): the `beta` value for SoftPlus formulation</span>
<span class="sd">            [default= `1.0` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">SoftPlus</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">beta</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="softsign"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.softsign">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">softsign</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise SoftSign.</span>
<span class="sd">    Can be used in place of Tanh function.</span>
<span class="sd">    While Tanh converges exponentially, SoftSign converges polynomially.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        SoftSign(x) = x/(1+|x|)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">SoftSign</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="tanh_shrink"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.tanh_shrink">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">tanh_shrink</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wies TanhShrink function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        TanhShrink(x) = x - \tanh(x)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">TanhShrink</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="sinc"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.sinc">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">sinc</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise Sinc function.</span>
<span class="sd">    Unlike other popular activation functions, it has rises and falls.</span>
<span class="sd">    returns :math:`1` if :math:`x = 0`.</span>
<span class="sd">    returns :math:`\sin(x)/x` otherwise.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Sinc</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">fused_batch_normalization</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">batch_stat</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Batch normalization fused with add2 (adding a residual input) and activation.</span>
<span class="sd">    </span>
<span class="sd">    This is an equivalent operation to the following,</span>
<span class="sd">    but is more computationally efficient:</span>
<span class="sd">    </span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">      h = F.batch_normalization(x, beta, gamma, mean, variance, *opts)</span>
<span class="sd">      y = F.relu(h + z)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array of input.</span>
<span class="sd">        beta(~nnabla.Variable): N-D array of beta which is learned.</span>
<span class="sd">        gamma(~nnabla.Variable): N-D array of gamma which is learned.</span>
<span class="sd">        mean(~nnabla.Variable): N-D array of running mean (modified during forward execution).</span>
<span class="sd">        variance(~nnabla.Variable): N-D array of running variance (modified during forward execution).</span>
<span class="sd">        z(~nnabla.Variable): N-D array of a residual input. By specifying None, the activation function will follow immediately after BN operation.</span>
<span class="sd">            [optional]</span>
<span class="sd">        axes(repeated int64): Axes mean and variance are taken.</span>
<span class="sd">            [default= `(1,)` ]</span>
<span class="sd">        decay_rate(float): Decay rate of running mean and variance.</span>
<span class="sd">            [default= `0.9` ]</span>
<span class="sd">        eps(float): Tiny value to avoid zero division by std.</span>
<span class="sd">            [default= `1e-05` ]</span>
<span class="sd">        batch_stat(bool): Use mini-batch statistics rather than running ones.</span>
<span class="sd">            [default= `True` ]</span>
<span class="sd">        nonlinearity(string): Activation chosen from (&#39;relu&#39;).</span>
<span class="sd">            [default= `&#39;relu&#39;` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">z</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">z</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">FusedBatchNormalization</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">decay_rate</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">batch_stat</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">batch_normalization</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">batch_stat</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">no_scale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">no_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Batch normalization.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \begin{eqnarray}</span>
<span class="sd">          \mu &amp;=&amp; \frac{1}{M} \sum x_i \\</span>
<span class="sd">          \sigma^2 &amp;=&amp; \frac{1}{M} \left(\sum x_i - \mu\right)^2 \\</span>
<span class="sd">          \hat{x}_i &amp;=&amp; \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} \\</span>
<span class="sd">          y_i &amp;=&amp; \hat{x}_i \gamma + \beta.</span>
<span class="sd">        \end{eqnarray}</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    At testing time, the mean and variance values used are those that were computed during training by moving average.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">        * `Ioffe and Szegedy, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1502.03167&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array of input.</span>
<span class="sd">        beta(~nnabla.Variable): N-D array of beta which is learned.</span>
<span class="sd">            [optional]</span>
<span class="sd">        gamma(~nnabla.Variable): N-D array of gamma which is learned.</span>
<span class="sd">            [optional]</span>
<span class="sd">        mean(~nnabla.Variable): N-D array of running mean (modified during forward execution).</span>
<span class="sd">            [optional]</span>
<span class="sd">        variance(~nnabla.Variable): N-D array of running variance (modified during forward execution).</span>
<span class="sd">            [optional]</span>
<span class="sd">        axes(repeated int64): Axes mean and variance are taken.</span>
<span class="sd">            [default= `(1,)` ]</span>
<span class="sd">        decay_rate(float): Decay rate of running mean and variance.</span>
<span class="sd">            [default= `0.9` ]</span>
<span class="sd">        eps(float): Tiny value to avoid zero division by std.</span>
<span class="sd">            [default= `1e-05` ]</span>
<span class="sd">        batch_stat(bool): Use mini-batch statistics rather than running ones.</span>
<span class="sd">            [default= `True` ]</span>
<span class="sd">        no_scale(bool): If `True`, the scale term is omitted.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        no_bias(bool): If `True`, the bias term is omitted.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">beta</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">beta</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">gamma</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">gamma</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">mean</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">mean</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">variance</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">variance</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">decay_rate</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">batch_stat</span><span class="p">,</span> <span class="n">no_scale</span><span class="p">,</span> <span class="n">no_bias</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">group_normalization</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_groups</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">channel_axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">no_scale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">no_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies Group Normalization over an input tensor, which is defined as:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">      \begin{eqnarray}</span>
<span class="sd">        \mu^g &amp;=&amp; \frac{1}{H} \sum_{i=1}^{H} x_i^g \\</span>
<span class="sd">        \sigma^g &amp;=&amp; \sqrt{\frac{1}{H} \sum_{i=1}^{H} \left(x_i^g - \mu^g\right)^2 + \epsilon} \\</span>
<span class="sd">        y &amp;=&amp; \frac{x - \mu^g}{\sigma^g} \gamma + \beta</span>
<span class="sd">      \end{eqnarray}</span>
<span class="sd">    </span>
<span class="sd">    where :math:`x` and :math:`y` are input and output variable,</span>
<span class="sd">    :math:`\mu^g` and :math:`\sigma^g` are the mean and std of each group which contains `num_channels / num_groups` channels,</span>
<span class="sd">    and :math:`\gamma` and :math:`\beta` are adaptive gains and biases.</span>
<span class="sd">    </span>
<span class="sd">    The input channels, specified by :attr:`channel_axis`, are separated into :attr:`num_groups` groups,</span>
<span class="sd">    and the mean and std are calculated over the each group.</span>
<span class="sd">    For example, if the input shape is [B, C, H, W] (= channel_axis=1, batch_axis=0),</span>
<span class="sd">    an input variable is once reshaped to [B, num_groups, C / num_groups, H, W]</span>
<span class="sd">    and standardize by its mean and std whose shapes are [B, num_groups, 1, 1, 1].</span>
<span class="sd">    Finally, an output variable is reshaped again to the original input shape (= [B, C, H, W] in the case above).</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">        * `Yuxin Wu, Kaiming He, Group Normalization.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1803.08494&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array of input.</span>
<span class="sd">        beta(~nnabla.Variable): N-D array of beta which is learned.</span>
<span class="sd">            [optional]</span>
<span class="sd">        gamma(~nnabla.Variable): N-D array of gamma which is learned.</span>
<span class="sd">            [optional]</span>
<span class="sd">        num_groups(int): A number of groups. The channel dim of &#39;x&#39; must be integer multiple of `num_groups`.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        channel_axis(int): Channel axis.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        batch_axis(repeated int64): Axes mean and variance are taken.</span>
<span class="sd">            [default= `(0,)` ]</span>
<span class="sd">        eps(float): Tiny value to avoid zero division by std.</span>
<span class="sd">            [default= `1e-05` ]</span>
<span class="sd">        no_scale(bool): If `True`, the scale term is omitted.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        no_bias(bool): If `True`, the bias term is omitted.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">num_groups</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">num_groups</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">channel_axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">channel_axis</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">beta</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">beta</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">gamma</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">gamma</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">GroupNormalization</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="n">channel_axis</span><span class="p">,</span> <span class="n">batch_axis</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">no_scale</span><span class="p">,</span> <span class="n">no_bias</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">instance_normalization</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">channel_axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">no_scale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">no_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies Instance Normalization over an input tensor, which is defined as</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">      \begin{eqnarray}</span>
<span class="sd">        \mu^i &amp;=&amp; \frac{1}{H} \sum_{i=1}^{H} x_i^i \\</span>
<span class="sd">        \sigma^i &amp;=&amp; \sqrt{\frac{1}{H} \sum_{i=1}^{H} \left(x_i^i - \mu^i\right)^2 + \epsilon} \\</span>
<span class="sd">        y &amp;=&amp; \frac{x - \mu^i}{\sigma^i} \gamma + \beta</span>
<span class="sd">      \end{eqnarray}</span>
<span class="sd">    </span>
<span class="sd">    where :math:`x` and :math:`y` are input and output variable,</span>
<span class="sd">    :math:`\mu^i` and :math:`\sigma^i` are the mean and std of each instance which is separately calculated for each batch and channel,</span>
<span class="sd">    and :math:`\gamma` and :math:`\beta` are adaptive gains and biases.</span>
<span class="sd">    </span>
<span class="sd">    If the input shape is [B, C, H, W] (= channel_axis=1, batch_axis=0), the shape of calculated mean and std are [B, C, 1, 1]</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">        * `Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky, Instance Normalization: The Missing Ingredient for Fast Stylization.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1607.08022&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array of input.</span>
<span class="sd">        beta(~nnabla.Variable): N-D array of beta which is learned.</span>
<span class="sd">            [optional]</span>
<span class="sd">        gamma(~nnabla.Variable): N-D array of gamma which is learned.</span>
<span class="sd">            [optional]</span>
<span class="sd">        channel_axis(int): Channel axis.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        batch_axis(repeated int64): Axes mean and variance are taken.</span>
<span class="sd">            [default= `(0,)` ]</span>
<span class="sd">        eps(float): Tiny value to avoid zero division by std.</span>
<span class="sd">            [default= `1e-05` ]</span>
<span class="sd">        no_scale(bool): If `True`, the scale term is omitted.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        no_bias(bool): If `True`, the bias term is omitted.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">channel_axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">channel_axis</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">beta</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">beta</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">gamma</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">gamma</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">InstanceNormalization</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">channel_axis</span><span class="p">,</span> <span class="n">batch_axis</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">no_scale</span><span class="p">,</span> <span class="n">no_bias</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">layer_normalization</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">no_scale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">no_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies Layer Normalization over an input tensor, which is defined as</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">      \begin{eqnarray}</span>
<span class="sd">        \mu^l &amp;=&amp; \frac{1}{H} \sum_{i=1}^{H} x_i^l \\</span>
<span class="sd">        \sigma^l &amp;=&amp; \sqrt{\frac{1}{H} \sum_{i=1}^{H} \left(x_i^l - \mu^l\right)^2 + \epsilon} \\</span>
<span class="sd">        y &amp;=&amp; \frac{x - \mu^l}{\sigma^l} \gamma + \beta</span>
<span class="sd">      \end{eqnarray}</span>
<span class="sd">    </span>
<span class="sd">    where :math:`x` and :math:`y` are input and output variable,</span>
<span class="sd">    :math:`\mu^l` and :math:`\sigma^l` are the mean and std of each layer which is separately calculated for each batch,</span>
<span class="sd">    and :math:`\beta` and :math:`\gamma` are adaptive biases and gains.</span>
<span class="sd">    </span>
<span class="sd">    If the input shape is [B, C, H, W] (= batch_axis=0), the shape of calculated mean and std are [B, 1, 1, 1]</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">        * `Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton, Layer Normalization.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1607.06450&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array of input.</span>
<span class="sd">        beta(~nnabla.Variable): N-D array of beta which is learned.</span>
<span class="sd">            [optional]</span>
<span class="sd">        gamma(~nnabla.Variable): N-D array of gamma which is learned.</span>
<span class="sd">            [optional]</span>
<span class="sd">        batch_axis(repeated int64): Axes mean and variance are taken.</span>
<span class="sd">            [default= `(0,)` ]</span>
<span class="sd">        eps(float): Tiny value to avoid zero division by std.</span>
<span class="sd">            [default= `1e-05` ]</span>
<span class="sd">        no_scale(bool): If `True`, the scale term is omitted.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        no_bias(bool): If `True`, the bias term is omitted.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">beta</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">beta</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">gamma</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">gamma</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">batch_axis</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">no_scale</span><span class="p">,</span> <span class="n">no_bias</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">norm_normalization</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Norm normalization.</span>
<span class="sd">        </span>
<span class="sd">    .. math::</span>
<span class="sd">        y = \frac{x_i}{\|x\|_p}</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        p(float): Order of the norm.</span>
<span class="sd">            [default= `2` ]</span>
<span class="sd">        axes(repeated int64): Axes to be reduced. If empty list is given, all dimensions are reduced.</span>
<span class="sd">            [default= `range(x.ndim)` ]</span>
<span class="sd">        eps(float): Epsilon for the normalization. This `eps` is added before taking the p-th root in the norm computation.</span>
<span class="sd">            [default= `1e-12` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">p</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">p</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="k">if</span> <span class="n">axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">NormNormalization</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">eps</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">sync_batch_normalization</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">comm</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">batch_stat</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Synchronized Batch Normalization:</span>
<span class="sd">    </span>
<span class="sd">    For some tasks (e.g., semantic segmentation), batch size will be too small and BatchNormalization layer might not work well.</span>
<span class="sd">    SyncBatchNorlization layer solves these problems by synchronizing batch stats (mean and var) between multiple processes.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \begin{eqnarray}</span>
<span class="sd">          \mu &amp;=&amp; \frac{1}{M} \sum x_i \\</span>
<span class="sd">          \sigma^2 &amp;=&amp; \frac{1}{M} \left(\sum x_i - \mu\right)^2 \\</span>
<span class="sd">          \hat{x}_i &amp;=&amp; \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} \\</span>
<span class="sd">          y_i &amp;=&amp; \hat{x}_i \gamma + \beta.</span>
<span class="sd">        \end{eqnarray}</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">        * Implementing Synchronized Multi-GPU Batch Normalization https://hangzhang.org/PyTorch-Encoding/notes/syncbn.html</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array of input.</span>
<span class="sd">        beta(~nnabla.Variable): N-D array of beta which is learned.</span>
<span class="sd">        gamma(~nnabla.Variable): N-D array of gamma which is learned.</span>
<span class="sd">        mean(~nnabla.Variable): N-D array of running mean (modified during forward execution).</span>
<span class="sd">        variance(~nnabla.Variable): N-D array of running variance (modified during forward execution).</span>
<span class="sd">        comm(Communicator): The communicator</span>
<span class="sd">        group(string): The name of the communicator group</span>
<span class="sd">            [default= `world` ]</span>
<span class="sd">        axes(repeated int64): Axes mean and variance are taken.</span>
<span class="sd">            [default= `(1,)` ]</span>
<span class="sd">        decay_rate(float): Decay rate of running mean and variance.</span>
<span class="sd">            [default= `0.9` ]</span>
<span class="sd">        eps(float): Tiny value to avoid zero division by std.</span>
<span class="sd">            [default= `1e-05` ]</span>
<span class="sd">        batch_stat(bool): Use mini-batch statistics rather than running ones.</span>
<span class="sd">            [default= `True` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">group</span> <span class="o">=</span> <span class="n">world</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">SyncBatchNormalization</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">comm</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">decay_rate</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">batch_stat</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">tensor_normalization</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">no_scale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">no_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    General tensor normalization.</span>
<span class="sd">    Input variable `x` is normalized by mean and std calculated by `x` itself.</span>
<span class="sd">    Mean and variance are calculated along `axes`.</span>
<span class="sd">    For example, if the input shape is (B, C, H, W) and axes is [0, 1],</span>
<span class="sd">    the shape of calculated mean and std are (B, C, 1 ,1).</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array of input.</span>
<span class="sd">        beta(~nnabla.Variable): N-D array of beta which is learned.</span>
<span class="sd">            [optional]</span>
<span class="sd">        gamma(~nnabla.Variable): N-D array of gamma which is learned.</span>
<span class="sd">            [optional]</span>
<span class="sd">        axes(repeated int64): Axes mean and variance are taken.</span>
<span class="sd">            [default= `(1,)` ]</span>
<span class="sd">        eps(float): Tiny value to avoid zero division by std.</span>
<span class="sd">            [default= `1e-05` ]</span>
<span class="sd">        no_scale(bool): If `True`, the scale term is omitted.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        no_bias(bool): If `True`, the bias term is omitted.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">beta</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">beta</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">gamma</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">gamma</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">TensorNormalization</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">no_scale</span><span class="p">,</span> <span class="n">no_bias</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<div class="viewcode-block" id="weight_normalization"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.weight_normalization">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">weight_normalization</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Weight normalization.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">      \mathbf{w}_{WN} = g \dfrac{\mathbf{w}}{\|\mathbf{w}\|}</span>
<span class="sd">    </span>
<span class="sd">    where :math:`\mathbf{w}` is the input weights to be normalized.</span>
<span class="sd">    and :math:`g` is learnable multiplication factors each of which is applied to each data at `dim`.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">        * `Tim Salimans, Diederik P. Kingma, Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks. &lt;https://arxiv.org/abs/1602.07868&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        w(~nnabla.Variable): N-D array of learnable weights.</span>
<span class="sd">        g(~nnabla.Variable): 1-D array of learnable scales.</span>
<span class="sd">        dim(int): Output dimension. For the other dimensions, the norms are computed.</span>
<span class="sd">            [default= `0` ]</span>
<span class="sd">        eps(float): Epsilon for the normalization. This `eps` is added before taking the sqrt in the norm computation.</span>
<span class="sd">            [default= `1e-12` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">WeightNormalization</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">eps</span><span class="p">)(</span><span class="n">w</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">weight_standardization</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">channel_axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies Weight Standardization over an input weight, which is defined as</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">      \begin{eqnarray}</span>
<span class="sd">        \mu_{W_i} &amp;=&amp; \frac{1}{I} \sum_{j=1}^{I} W_{ij} \\</span>
<span class="sd">        \sigma_{W_i} &amp;=&amp; \sqrt{\frac{1}{I} \sum_{i=1}^{I} \left(W_{ij} - \mu_{W_{i}}\right)^2 + \epsilon} \\</span>
<span class="sd">        \hat{W_{ij}} &amp;=&amp; \frac{W_{ij} - \mu_{W_i}}{\sigma_{W_i}} \\</span>
<span class="sd">        y &amp;=&amp; \hat{W} \ast x</span>
<span class="sd">      \end{eqnarray}</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">      * `Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, Alan Yuille, Weight Standardization</span>
<span class="sd">        &lt;https://arxiv.org/pdf/1903.10520v1.pdf&gt;`_</span>
<span class="sd">        </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        w(~nnabla.Variable): N-D array of learnable weights.</span>
<span class="sd">        channel_axis(int): An axis for output channel. Default value is 0 which assumes the weights of convolution.</span>
<span class="sd">            [default= `0` ]</span>
<span class="sd">        eps(float): Tiny value to avoid zero division by std.</span>
<span class="sd">            [default= `1e-05` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">channel_axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">channel_axis</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">WeightStandardization</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">channel_axis</span><span class="p">,</span> <span class="n">eps</span><span class="p">)(</span><span class="n">w</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<div class="viewcode-block" id="spectral_norm"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.spectral_norm">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">spectral_norm</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">itr</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Spectral Normalization.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        W_{sn} = \\frac{W}{\\sigma(W)}.</span>
<span class="sd">    </span>
<span class="sd">    where :math:`W` is the input matrix, and the :math:`\\sigma(W)` is the spectral norm of :math:`W`. The spectral norm is approximately computed by the power iteration.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">        Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida, </span>
<span class="sd">        &quot;Spectral Normalization for Generative Adversarial Networks&quot;, </span>
<span class="sd">        International Conference on Learning Representations. 2018.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        w(~nnabla.Variable): N-D array of learnable weights. This is normally network parameter.</span>
<span class="sd">        u(~nnabla.Variable): 1-D array of singular vector. When `test == False`, the data region of `u` will be updated during forward calculation.</span>
<span class="sd">        dim(int): Output dimension. Default is 0. If the dimension is not 0, then the specified dimension becomes the most-left dimension by transposing.</span>
<span class="sd">            [default= `0` ]</span>
<span class="sd">        itr(int): Number of power iterations. Default is 1.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        eps(float): Epsilon for the normalization. This `eps` is added before taking the sqrt in the norm computation.</span>
<span class="sd">            [default= `1e-12` ]</span>
<span class="sd">        test(bool): When in `True`, `u` will not be updated. Default is `False`.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Spectrally normalized :math:`W_{sn}` with the same shape as :math:`W`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">SpectralNorm</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">itr</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">test</span><span class="p">)(</span><span class="n">w</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">mean_subtraction</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">rmean</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">update_running_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    It subtracts the mean of the elements of the input array,</span>
<span class="sd">    and normalizes it to :math:`0`. Preprocessing arrays with this function has the effect of improving accuracy</span>
<span class="sd">    in various tasks such as image classification.</span>
<span class="sd">    </span>
<span class="sd">    At training time, this function is defined as</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \begin{eqnarray}</span>
<span class="sd">          \mu &amp;=&amp; \frac{1}{M} \sum x_i \\</span>
<span class="sd">          y_i &amp;=&amp; x_i - \mu</span>
<span class="sd">        \end{eqnarray}</span>
<span class="sd">    </span>
<span class="sd">    At testing time, the mean values used are those that were computed during training by moving average.</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        The backward performs an approximated differentiation that takes into account only the latest mini-batch.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array of input.</span>
<span class="sd">        rmean(~nnabla.Variable): N-D array of running mean (modified during forward execution).</span>
<span class="sd">        t(~nnabla.Variable): Scalar of num of iteration of running mean (modified during forward execution).</span>
<span class="sd">        base_axis(int): Base axis of Mean Subtraction operation. Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        update_running_mean(bool): Update running mean during forward execution.</span>
<span class="sd">            [default= `True` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">MeanSubtraction</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">update_running_mean</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">rmean</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<div class="viewcode-block" id="clip_grad_by_value"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.clip_grad_by_value">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">clip_grad_by_value</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="nb">min</span><span class="p">,</span> <span class="nb">max</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;In forward pass, the function behaves as the identity.</span>
<span class="sd">    </span>
<span class="sd">    In backward pass,</span>
<span class="sd">    </span>
<span class="sd">        .. math::</span>
<span class="sd">            g_x = \begin{cases}</span>
<span class="sd">                max &amp; (g_y &gt; max) \\</span>
<span class="sd">                g_y &amp; (otherwise) \\</span>
<span class="sd">                min &amp; (g_y &lt; min)</span>
<span class="sd">               \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    A typical case for use is to prevent the gradient explosion through a whole computational graph.</span>
<span class="sd">    For example, if you want to clip gradient values for each feature map,</span>
<span class="sd">    </span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">      x = nn.Variable([16, 3, 32, 32])</span>
<span class="sd">      min = F.broadcast(nn.Variable.from_numpy_array(np.asarray([-1.0]).reshape((1, 1, 1, 1))), (16, 3, 32, 32))</span>
<span class="sd">      max = F.broadcast(nn.Variable.from_numpy_array(np.asarray([1.0]).reshape((1, 1, 1, 1))), (16, 3, 32, 32))</span>
<span class="sd">      c = F.clip_grad_by_value(x, min=min, max=max)</span>
<span class="sd">      h = PF.convolution(c, 64, (3, 3), pad=(1, 1))</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array of input.</span>
<span class="sd">        min(~nnabla.Variable): N-D array of minimum input value by which the gradients of the `y` are clipped. Note that the shape of `min` must be the same as `x`&#39;s and the backward to `min` is not performed.</span>
<span class="sd">        max(~nnabla.Variable): N-D array of maximum input value by which the gradients of the `y` are clipped. Note that the shape of `max` must be the same as `x`&#39;s and the backward to `max` is not performed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ClipGradByValue</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="nb">min</span><span class="p">,</span> <span class="nb">max</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="clip_grad_by_norm"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.clip_grad_by_norm">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">clip_grad_by_norm</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">clip_norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    In the forward pass, the function behaves like the identity.</span>
<span class="sd">    </span>
<span class="sd">    In the backward pass,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        g_x = N \times \frac{g_y}{\|g_y\|_2}.</span>
<span class="sd">    </span>
<span class="sd">    where :math:`g_x` is the gradient w.r.t the input, :math:`g_y` is the gradient w.r.t. the output,</span>
<span class="sd">    and :math:`N` is `clip_norm` where the norm of :math:`g_y` becomes. this is the case that `axes` is not set.</span>
<span class="sd">    When `axes` is set, the norm is computed over `axes`.</span>
<span class="sd">    </span>
<span class="sd">    A typical case for use is to prevent the gradient explosion through a whole computational graph.</span>
<span class="sd">    For example, if you want to normalize gradient values over feature axis,</span>
<span class="sd">    </span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">      x = nn.Variable([16, 3, 32, 32])</span>
<span class="sd">      c = F.clip_grad_by_norm(x, axes=(1, ))</span>
<span class="sd">      h = PF.convolution(c, 64, (3, 3), pad=(1, 1))</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array of input.</span>
<span class="sd">        clip_norm(float): Clip to the norm of input to `clip_norm` in the backward pass.</span>
<span class="sd">            [default= `1.0` ]</span>
<span class="sd">        axes(repeated int64): Axes to be reduced. If empty list is given, all dimensions are reduced to scalar. This is used in the forward pass.</span>
<span class="sd">            [default= `range(x.ndim)` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">clip_norm</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">clip_norm</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="k">if</span> <span class="n">axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ClipGradByNorm</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">clip_norm</span><span class="p">,</span> <span class="n">axes</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">sum</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a matrix along a specified axis with the sum function.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        axes(repeated int64): Axes to be reduced. If empty list is given, all dimensions are reduced to scalar.</span>
<span class="sd">            [default= `range(x.ndim)` ]</span>
<span class="sd">        keep_dims(bool): Flag whether the reduced axis is kept.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Sum</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<div class="viewcode-block" id="cumsum"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.cumsum">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">cumsum</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Cumulative sum along a given axis.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        axis(int): Axis along which cumulative sum is to be calculated</span>
<span class="sd">            [default= `0` ]</span>
<span class="sd">        exclusive(bool): If True, perform exclusive cumsum</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        reverse(bool): If True, perform cumsum in reverse direction</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">CumSum</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">exclusive</span><span class="p">,</span> <span class="n">reverse</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a matrix along a specified axis with the mean function.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        axes(repeated int64): Axes to be reduced.</span>
<span class="sd">            [default= `range(x.ndim)` ]</span>
<span class="sd">        keep_dims(bool): Flag whether the reduced axis is kept.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Mean</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">max</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">with_index</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">only_index</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduction along axis or axes with max operation.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        axes(repeated int64): Axes to be reduced.</span>
<span class="sd">            [default= `range(x.ndim)` ]</span>
<span class="sd">        keep_dims(bool): Flag whether the reduced axis is kept.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        with_index(bool): Return values and indices.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        only_index(bool): Return only indices.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Max</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">,</span> <span class="n">with_index</span><span class="p">,</span> <span class="n">only_index</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">min</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">with_index</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">only_index</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduction along axis or axes with min operation.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        axes(repeated int64): Axes to be reduced.</span>
<span class="sd">            [default= `range(x.ndim)` ]</span>
<span class="sd">        keep_dims(bool): Flag whether the reduced axis is kept.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        with_index(bool): Return values and indices.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        only_index(bool): Return only indices.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Min</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">,</span> <span class="n">with_index</span><span class="p">,</span> <span class="n">only_index</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">norm</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduction along axis or axes with norm operation.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y = \|x\|_p = \left( \sum_i |x_i|^p \right)^{\frac{1}{p}}</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        p(float): Order of the norm.</span>
<span class="sd">            [default= `2` ]</span>
<span class="sd">        axes(repeated int64): Axes to be reduced. If empty list is given, all dimensions are reduced to scalar.</span>
<span class="sd">            [default= `range(x.ndim)` ]</span>
<span class="sd">        keep_dims(bool): Flag whether the reduced axis is kept.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">p</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">p</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="k">if</span> <span class="n">axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Norm</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">prod</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduction along axis or axes with product operation.</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        Backward computation is not accurate in a zero value input.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        axes(repeated int64): Axes to be reduced.</span>
<span class="sd">            [default= `range(x.ndim)` ]</span>
<span class="sd">        keep_dims(bool): Flag whether the reduced axis is kept.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Prod</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<div class="viewcode-block" id="cumprod"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.cumprod">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">cumprod</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Cumulative product along a given axis.</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        Backward computation is not accurate in a zero value input.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        axis(int): Axis along which cumulative product is to be calculated</span>
<span class="sd">            [default= `0` ]</span>
<span class="sd">        exclusive(bool): If True, perform exclusive cumprod</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        reverse(bool): If True, perform cumprod in reverse direction</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">CumProd</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">exclusive</span><span class="p">,</span> <span class="n">reverse</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="reduce_sum"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.reduce_sum">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">reduce_sum</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduction along an axis with sum operation.</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        This is deprecated. Use ``sum`` instead.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="reduce_mean"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.reduce_mean">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">reduce_mean</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduction by mean along an axis.</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        This is deprecated. Use ``mean`` instead.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="add2"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.add2">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">add2</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise addition.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = x^{(0)}_i + x^{(1)}_i</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>
<span class="sd">        inplace(bool): The output array is shared with the 1st input array if True.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Add2</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="add_n"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.add_n">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">add_n</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise addition.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = x^{(0)}_i + . . . + x^{(n-1)}_i</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        *x(~nnabla.Variable): N-D arrays</span>
<span class="sd">            [variadic]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;add_n must take more than 1 inputs&quot;</span>
    <span class="n">n_outputs</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;n_outputs&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">AddN</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">bc_add2</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Note: This shouldn&#39;t be called by users.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>
<span class="sd">        inplace(bool): The output array is shared with the 1st input array if True.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BcAdd2</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<div class="viewcode-block" id="sub2"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.sub2">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">sub2</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise subtraction.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = x^{(0)}_i - x^{(1)}_i</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>
<span class="sd">        inplace(bool): The output array is shared with the 1st input array if True.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Sub2</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="mul2"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.mul2">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">mul2</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise multiplication.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = x^{(0)}_i x^{(1)}_i</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>
<span class="sd">        inplace(bool): This option is obsolete and ignored. Output is never in-placed with input.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Mul2</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="mul_n"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.mul_n">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">mul_n</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise multiplication.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = x^{(0)}_i . . . x^{(n-1)}_i</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        *x(~nnabla.Variable): N-D arrays</span>
<span class="sd">            [variadic]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;mul_n must take more than 1 inputs&quot;</span>
    <span class="n">n_outputs</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;n_outputs&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">MulN</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="div2"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.div2">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">div2</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise division.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = \frac{x^{(0)}_i} {x^{(1)}_i}</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>
<span class="sd">        inplace(bool): This option is obsolete and ignored. Output is never in-placed with input.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Div2</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="pow2"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.pow2">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">pow2</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise power function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = {(x^{(0)}_i)} ^ {x^{(1)}_i}</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>
<span class="sd">        inplace(bool): This option is obsolete and ignored. Output is never in-placed with input.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Pow2</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="add_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.add_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">add_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise scalar addition.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = x_i + v</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        inplace(bool): The output array is shared with the input array if True.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">AddScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="mul_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.mul_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">mul_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise scalar multiplication.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = v x_i</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        inplace(bool): The output array is shared with the input array if True.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">MulScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="pow_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.pow_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">pow_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise scalar power function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = (x_i) ^ v</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        inplace(bool): This option is obsolete and ignored. Output is never in-placed with input.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">PowScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="r_sub_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.r_sub_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">r_sub_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise scalar subtraction.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = v - x_i</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default= `1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">RSubScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="r_div_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.r_div_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">r_div_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise scalar division.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \frac{v}{x_i}</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default= `1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">RDivScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="r_pow_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.r_pow_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">r_pow_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise scalar power function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = v ^ {x_i}</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default= `1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">RPowScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="sign"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.sign">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">sign</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise sign function.</span>
<span class="sd">    </span>
<span class="sd">    In the forward pass, it is defined as</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        f(x) = \begin{cases}</span>
<span class="sd">            1  &amp; (x &gt; 0) \\</span>
<span class="sd">            -1 &amp; (x &lt; 0) \\</span>
<span class="sd">            \alpha &amp; (x = 0)</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    In the backward pass, it is defined as</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \frac{\partial f(x)}{\partial x} = 1,</span>
<span class="sd">    </span>
<span class="sd">    or in other words, it behaves as the identity function for the gradient in the backward pass.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input</span>
<span class="sd">        alpha(float): Value in case of :math:`x = 0`.</span>
<span class="sd">            [default= `1.0` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Sign</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="minimum2"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.minimum2">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">minimum2</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise minimum.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = \min(x^{(0)}_i, x^{(1)}_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array of min value</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Minimum2</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="maximum2"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.maximum2">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">maximum2</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise maximum.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = \max(x^{(0)}_i, x^{(1)}_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array of max value</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Maximum2</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="minimum_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.minimum_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">minimum_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise scalar minimum.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \min(x_i, v)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default= `1.0` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">MinimumScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="maximum_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.maximum_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">maximum_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise scalar maximum.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \max (x_i, v)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default= `1.0` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">MaximumScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="logical_and"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.logical_and">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">logical_and</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Elementwise logical AND.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x^{(0)}_i,x^{(1)}_i) = \begin{cases}</span>
<span class="sd">            1 &amp; (x^{(0)}_i \neq 0 \;\&amp;\; x^{(1)}_i \neq 0) \\</span>
<span class="sd">            0 &amp; otherwise</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: No Description</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LogicalAnd</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="logical_or"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.logical_or">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">logical_or</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Elementwise logical OR.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x^{(0)}_i,x^{(1)}_i) = \begin{cases}</span>
<span class="sd">            0 &amp; (x^{(0)}_i = 0 \;\&amp;\; x^{(1)}_i = 0) \\</span>
<span class="sd">            1 &amp; otherwise</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: No Description</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LogicalOr</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="logical_xor"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.logical_xor">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">logical_xor</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Elementwise logical XOR.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x^{(0)}_i,x^{(1)}_i) = \begin{cases}</span>
<span class="sd">            1 &amp; (x^{(0)}_i = 0 \;\&amp;\; x^{(1)}_i = 0) \\</span>
<span class="sd">            1 &amp; (x^{(0)}_i \neq 0 \;\&amp;\; x^{(1)}_i \neq 0) \\</span>
<span class="sd">            0 &amp; otherwise</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: No Description</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LogicalXor</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="equal"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.equal">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">equal</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element wise &#39;equal&#39;</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x^{(0)}_i,x^{(1)}_i) = \begin{cases}</span>
<span class="sd">            1 &amp; (x^{(0)}_i = x^{(1)}_i) \\</span>
<span class="sd">            0 &amp; otherwise</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: No Description</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Equal</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="not_equal"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.not_equal">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">not_equal</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    </span>
<span class="sd">    Element wise &#39;not equal&#39;</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x^{(0)}_i,x^{(1)}_i) = \begin{cases}</span>
<span class="sd">            0 &amp; (x^{(0)}_i = x^{(1)}_i) \\</span>
<span class="sd">            1 &amp; otherwise</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: No Description</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">NotEqual</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="greater_equal"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.greater_equal">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">greater_equal</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element wise comparison. The :math:`i^{th}` element of the output is:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        f(x^{(0)}_i,x^{(1)}_i) = \begin{cases}</span>
<span class="sd">            1  &amp; (x^{(0)}_i \geq x^{(1)}_i) \\</span>
<span class="sd">            0 &amp; (x^{(0)}_i &lt; x^{(1)}_i)</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: No Description</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">GreaterEqual</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="greater"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.greater">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">greater</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element wise comparison. The :math:`i^{th}` element of the output is:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        f(x^{(0)}_i,x^{(1)}_i) = \begin{cases}</span>
<span class="sd">            1  &amp; (x^{(0)}_i &gt; x^{(1)}_i) \\</span>
<span class="sd">            0 &amp; (x^{(0)}_i \leq x^{(1)}_i)</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: No Description</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Greater</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="less_equal"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.less_equal">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">less_equal</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element wise comparison. The :math:`i^{th}` element of the output is:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        f(x^{(0)}_i,x^{(1)}_i) = \begin{cases}</span>
<span class="sd">            1  &amp; (x^{(0)}_i \leq x^{(1)}_i) \\</span>
<span class="sd">            0 &amp; (x^{(0)}_i &gt; x^{(1)}_i)</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: No Description</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LessEqual</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="less"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.less">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">less</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element wise comparison. The :math:`i^{th}` element of the output is:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        f(x^{(0)}_i,x^{(1)}_i) = \begin{cases}</span>
<span class="sd">            1  &amp; (x^{(0)}_i &lt; x^{(1)}_i) \\</span>
<span class="sd">            0 &amp; (x^{(0)}_i \geq x^{(1)}_i)</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: No Description</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Less</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">searchsorted</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">sorted_sequence</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finds indices in the innermost dimension of a sorted sequance where values must be inserted in order to maintain value</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        sorted_sequence(~nnabla.Variable): N-D array of sorted sequence where search is to be performed. Note that this must be a sorted array</span>
<span class="sd">        values(~nnabla.Variable): N-D array of Search values</span>
<span class="sd">        right(bool): :If True, given a value v, the function returns index i such that sorted_sequence[i-1] &lt;= v &lt; sorted_sequence[i] (index of closest upper bound of v). By default, this is false so the function returns index i such that a[i-1] &lt; v &lt;= a[i] (index of closest lower bound of v)</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array containing the required indices</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">right</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">right</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">SearchSorted</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">right</span><span class="p">)(</span><span class="n">sorted_sequence</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<div class="viewcode-block" id="logical_and_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.logical_and_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">logical_and_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Elementwise logical AND with scalar.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x_i,v) = \begin{cases}</span>
<span class="sd">            1 &amp; (x_i \neq 0 \;\&amp;\; v \neq 0) \\</span>
<span class="sd">            0 &amp; otherwise</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Input variable</span>
<span class="sd">        val(bool): No Description</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LogicalAndScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="logical_or_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.logical_or_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">logical_or_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Elementwise logical OR with scalar.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x_i,v) = \begin{cases}</span>
<span class="sd">            0 &amp; (x_i = 0 \;\&amp;\; v = 0) \\</span>
<span class="sd">            1 &amp; otherwise</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Input variable</span>
<span class="sd">        val(bool): No Description</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LogicalOrScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="logical_xor_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.logical_xor_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">logical_xor_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Elementwise logical XOR with scalar.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x_i,v) = \begin{cases}</span>
<span class="sd">            1 &amp; (x_i = 0 \;\&amp;\; v = 0) \\</span>
<span class="sd">            1 &amp; (x_i \neq 0 \;\&amp;\; v \neq 0) \\</span>
<span class="sd">            0 &amp; otherwise</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Input variable</span>
<span class="sd">        val(bool): No Description</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LogicalXorScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="equal_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.equal_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">equal_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element wise &#39;equal&#39; with a scalar</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x_i,v) = \begin{cases}</span>
<span class="sd">            1 &amp; (x_i = v) \\</span>
<span class="sd">            0 &amp; otherwise</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default= `1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">EqualScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="not_equal_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.not_equal_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">not_equal_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element wise &#39;not equal&#39; with a scalar</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x_i,v) = \begin{cases}</span>
<span class="sd">            0 &amp; (x_i = v) \\</span>
<span class="sd">            1 &amp; otherwise</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default= `1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">NotEqualScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="greater_equal_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.greater_equal_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">greater_equal_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element wise comparison with a scalar. The :math:`i^{th}` element of the output is:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        f(x^{(0)}_i,v) = \begin{cases}</span>
<span class="sd">            1  &amp; (x^{(0)}_i \geq v \\</span>
<span class="sd">            0 &amp; (x^{(0)}_i &lt; v</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default= `1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">GreaterEqualScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="greater_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.greater_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">greater_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element wise comparison with a scalar. The :math:`i^{th}` element of the output is:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        f(x^{(0)}_i,v) = \begin{cases}</span>
<span class="sd">            1  &amp; (x^{(0)}_i &gt; v \\</span>
<span class="sd">            0 &amp; (x^{(0)}_i \leq v</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default= `1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">GreaterScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="less_equal_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.less_equal_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">less_equal_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element wise comparison with a scalar. The :math:`i^{th}` element of the output is:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        f(x^{(0)}_i,v) = \begin{cases}</span>
<span class="sd">            1  &amp; (x^{(0)}_i \leq v) \\</span>
<span class="sd">            0 &amp; (x^{(0)}_i &gt; v)</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default= `1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LessEqualScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="less_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.less_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">less_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element wise comparison with a scalar. The :math:`i^{th}` element of the output is:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        f(x^{(0)}_i,v) = \begin{cases}</span>
<span class="sd">            1  &amp; (x^{(0)}_i &lt; v) \\</span>
<span class="sd">            0 &amp; (x^{(0)}_i \geq v)</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default= `1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LessScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="logical_not"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.logical_not">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">logical_not</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise logical NOT operation</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x_i) = \begin{cases}</span>
<span class="sd">            1 &amp; (x_i = 0) \\</span>
<span class="sd">            0 &amp; otherwise</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Input variable</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LogicalNot</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="isnan"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.isnan">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">isnan</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Test element-wise for NaN and return a ``0/1`` array.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Input variable</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">IsNaN</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="isinf"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.isinf">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">isinf</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Test element-wise for ``inf/-inf`` and return a ``0/1`` array.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Input variable</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">IsInf</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="reset_nan"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.reset_nan">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">reset_nan</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace NaNs with a scalar value specified by ``val``.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default= `0` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ResetNaN</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="reset_inf"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.reset_inf">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">reset_inf</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace ``-inf/inf`` with a scalar value specified by ``val``.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default= `0` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ResetInf</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="where"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.where">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">where</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">condition</span><span class="p">,</span> <span class="n">x_true</span><span class="p">,</span> <span class="n">x_false</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return elements, either from ``x_true`` or ``x_false``, depending on ``condition``.</span>
<span class="sd">    </span>
<span class="sd">    If rank of ``condition`` is higher than those of ``x_true`` and ``x_false``, the first dimensions of ``x_true`` and ``x_false`` must match the dimensions of ``condition``.</span>
<span class="sd">    </span>
<span class="sd">    Example:</span>
<span class="sd">    </span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">      import numpy as np</span>
<span class="sd">      import nnabla as nn</span>
<span class="sd">      import nnabla.functions as F</span>
<span class="sd">    </span>
<span class="sd">      a = nn.Variable.from_numpy_array(np.random.rand(2, 3))</span>
<span class="sd">      x = nn.Variable.from_numpy_array(np.random.rand(2, 3, 4))</span>
<span class="sd">      y = nn.Variable.from_numpy_array(np.random.rand(2, 3, 4))</span>
<span class="sd">      z = F.where(F.greater_scalar(a, 0.5), x, y)</span>
<span class="sd">      z.forward()</span>
<span class="sd">    </span>
<span class="sd">      # Numpy equivalent</span>
<span class="sd">      z_numpy = np.where(a.d &gt; 0.5, x.d, y.d)</span>
<span class="sd">      assert np.allclose(z_numpy, z.d)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        condition(~nnabla.Variable): N-d array. For all i, when ``condition[i] == true``, yield ``x_true[i]``, otherwise ``x_false[i]``.</span>
<span class="sd">        x_true(~nnabla.Variable): N-d array with higher or equal rank to ``condition``.</span>
<span class="sd">        x_false(~nnabla.Variable): N-d array with higher or equal rank to ``condition``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as condition</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Where</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">condition</span><span class="p">,</span> <span class="n">x_true</span><span class="p">,</span> <span class="n">x_false</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="constant"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.constant">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">constant</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate a constant-valued array.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        val(float): Constant value.</span>
<span class="sd">            [default= `0` ]</span>
<span class="sd">        shape(:obj:`tuple` of :obj:`int`): Shape of the output array.</span>
<span class="sd">            [default= `[]` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array where all values are the specified constant.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">shape</span><span class="p">)(</span><span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="arange"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.arange">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">arange</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate a range of values within the half-open interval</span>
<span class="sd">    ``[start, stop)`` (the interval including start but excluding</span>
<span class="sd">    stop) with `step` increments.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        start(float): Start value.</span>
<span class="sd">        stop(float): End value.</span>
<span class="sd">        step(float): Step value.</span>
<span class="sd">            [default= `1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: 1-D array with the generated values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Arange</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">,</span> <span class="n">step</span><span class="p">)(</span><span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="abs"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.abs">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">abs</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise absolute value function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = |x_i|</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Element-wise absolute variable</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Abs</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="exp"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.exp">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">exp</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise natural exponential function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = \exp(x_i).</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Element-wise exp variable</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Exp</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="log"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.log">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">log</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise natural logarithm function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = \ln(x_i).</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Element-wise log variable</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Log</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="identity"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.identity">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">identity</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y = x</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Identity</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="batch_matmul"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.batch_matmul">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">batch_matmul</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">transpose_a</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Batch matrix multiplication.</span>
<span class="sd">    </span>
<span class="sd">    Two of batchs of matrices are multiplied for each sample in a batch.</span>
<span class="sd">    A batch of matrices is composed as [..., P, Q] where the last two dimensions compose matrix dimensions,</span>
<span class="sd">    and the first dimensions up to the third last dimension are considered as batch samples.</span>
<span class="sd">    These batch dimensions are internally broadcasted when the size of a dimension is 1.</span>
<span class="sd">    </span>
<span class="sd">    Example:</span>
<span class="sd">    </span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">      import nnabla as nn</span>
<span class="sd">      import nnabla.functions as F</span>
<span class="sd">      import numpy as np</span>
<span class="sd">    </span>
<span class="sd">      nn.set_auto_forward(True)</span>
<span class="sd">    </span>
<span class="sd">      # Same batch size</span>
<span class="sd">      a = nn.Variable.from_numpy_array(np.random.rand(2, 2, 3, 4))</span>
<span class="sd">      b = nn.Variable.from_numpy_array(np.random.rand(2, 2, 4, 3))</span>
<span class="sd">      c = F.batch_matmul(a, b)</span>
<span class="sd">    </span>
<span class="sd">      # Different batch size with the broadcast</span>
<span class="sd">      a = nn.Variable.from_numpy_array(np.random.rand(2, 1, 3, 4))</span>
<span class="sd">      b = nn.Variable.from_numpy_array(np.random.rand(1, 3, 4, 3))</span>
<span class="sd">      c = F.batch_matmul(a, b)</span>
<span class="sd">    </span>
<span class="sd">    .. WARNING::</span>
<span class="sd">      Since the version 1.13, the behavior of the batch dimensions changed, it supported the internal</span>
<span class="sd">      broadcast when the size of a dimension is 1. Accordingly, this function does not supports different</span>
<span class="sd">      batch dimensions between two inputs even if the total sample size for each input is same.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        a(~nnabla.Variable): N-D array with &gt;= 2-dim. The last two dimensions will be treated as a matrix.</span>
<span class="sd">        b(~nnabla.Variable): N-D array with &gt;= 2-dim. The last two dimensions will be treated as a matrix. The product of the size of 0-th dimension through the size of the third last dimension must be same as that of the input ``a``.</span>
<span class="sd">        transpose_a(bool): Transpose the last two axes of ``a`` in matrix multiplication.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        transpose_b(bool): Transpose the last two axes of ``b`` in matrix multiplication.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output of sample-wise matrix multiplication in a batch. When ``a`` is of a shape of [N, P, Q], ``b`` is of a shape of [N, Q, R], and transpose options are all False, the output will be a shape of [N, P, R].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BatchMatmul</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">transpose_a</span><span class="p">,</span> <span class="n">transpose_b</span><span class="p">)(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="round"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.round">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">round</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise round function.</span>
<span class="sd">    </span>
<span class="sd">    In the forward pass, this function simply computes `round` to the nearest integer value.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = round(x_i).</span>
<span class="sd">    </span>
<span class="sd">    In the backward pass, the simple Straight-Through Estimator (STE) is applied,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \frac{\partial y_i}{\partial x_i} = 1.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Round</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="ceil"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.ceil">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">ceil</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise ceil function.</span>
<span class="sd">    </span>
<span class="sd">    In the forward pass, this function simply returns the smallest integer which is not less than the input.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = ceil(x_i).</span>
<span class="sd">    </span>
<span class="sd">    In the backward pass, the simple Straight-Through Estimator (STE) is applied,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \frac{\partial y_i}{\partial x_i} = 1.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Ceil</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="floor"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.floor">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">floor</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise floor function.</span>
<span class="sd">    </span>
<span class="sd">    In the forward pass, this function simply returns the largest integer which is not greater than the input.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = floor(x_i).</span>
<span class="sd">    </span>
<span class="sd">    In the backward pass, the simple Straight-Through Estimator (STE) is applied,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \frac{\partial y_i}{\partial x_i} = 1.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Floor</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="sin"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.sin">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">sin</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise sine (sin) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \sin (x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Sin</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="cos"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.cos">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">cos</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise cosine (cos) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \cos (x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Cos</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="tan"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.tan">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">tan</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise tangent (tan) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \tan (x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Tan</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="sinh"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.sinh">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">sinh</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise hyperbolic sine (sinh) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \sinh (x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Sinh</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="cosh"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.cosh">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">cosh</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise hyperbolic cosine (cosh) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \cosh (x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Cosh</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="asin"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.asin">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">asin</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise arcsine (asin) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \arcsin (x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ASin</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="acos"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.acos">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">acos</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise arccosine (acos) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \arccos (x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ACos</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="atan"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.atan">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">atan</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise arctangent (atan) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \arctan (x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ATan</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="atan2"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.atan2">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">atan2</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise arctangent (atan) function with 2 input variables.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \arctan2 (x_{i1}, x_{i2})</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as input variables</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ATan2</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="asinh"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.asinh">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">asinh</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise hyperbolic arcsine (asinh) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \text{arcsinh} (x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ASinh</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="acosh"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.acosh">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">acosh</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise hyperbolic arccosine (acosh) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \text{arccosh} (x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ACosh</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="atanh"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.atanh">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">atanh</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise hyperbolic arctangent (atanh) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \text{arctanh} (x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ATanh</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="concatenate"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.concatenate">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">concatenate</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Concatenate a variable number of input arrays along the specified axis.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        *x(~nnabla.Variable): N-D arrays.</span>
<span class="sd">            [variadic]</span>
<span class="sd">        axis(int): Axis</span>
<span class="sd">            [default= `len(x[0].shape) - 1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Concatenate variable</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;concatenate must take more than 1 inputs&quot;</span>
    <span class="n">n_outputs</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;n_outputs&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Concatenate</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">)(</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Split arrays at the specified axis.</span>
<span class="sd">    </span>
<span class="sd">    note:</span>
<span class="sd">        This function should not be called directly when constructing models.</span>
<span class="sd">        Instead, use :meth:`nnabla.functions.split` which</span>
<span class="sd">        automatically sets `n_output` from the input&#39;s shape and axis.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        axis(int): Axis</span>
<span class="sd">            [default= `0` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: list of N-D arrays</span>
<span class="sd">            [variadic][parameter]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<div class="viewcode-block" id="stack"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.stack">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">stack</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Joins two or more arrays on a new axis.</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        Unlike :meth:`nnabla.functions.concatenate` , which joins arrays on an existing axis,</span>
<span class="sd">        Stack joins arrays on a new axis.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        *x(~nnabla.Variable): N-D arrays. The sizes of all the arrays to be stacked must be the same.</span>
<span class="sd">            [variadic]</span>
<span class="sd">        axis(int): The axis on which to concatenate arrays. Axis indices take on values 0, 1, 2, and so on from the left. For example, to stack four (3,28,28) inputs on the second axis, specify 1. In this case, the output size will be (3,4,28,28).</span>
<span class="sd">            [default= `0` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;stack must take more than 1 inputs&quot;</span>
    <span class="n">n_outputs</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;n_outputs&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Stack</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">)(</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="slice"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.slice">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">slice</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Slice arrays along specified axis.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        start(repeated int64): Start indices for each axis</span>
<span class="sd">            [default= `(0,) * len(x.shape)` ]</span>
<span class="sd">        stop(repeated int64): Stop indices for each axis</span>
<span class="sd">            [default= `tuple(x.shape)` ]</span>
<span class="sd">        step(repeated int64): Step indices for each axis</span>
<span class="sd">            [default= `(1,) * len(x.shape)` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Sliced N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">start</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">start</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">stop</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stop</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">step</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">step</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Slice</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">,</span> <span class="n">step</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="pad"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.pad">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">pad</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">pad_width</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">constant_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pad the input N-D array `x` over the number of dimensions given</span>
<span class="sd">    by half the length of the `pad_width` iterable, where every two</span>
<span class="sd">    values in `pad_width` determine the before and after pad size of</span>
<span class="sd">    an axis. The `pad_width` iterable must hold an even number of</span>
<span class="sd">    positive values which may cover all or fewer dimensions of the</span>
<span class="sd">    input variable `x`. If `pad_width` covers fewer dimensions then</span>
<span class="sd">    it applies to the innermost dimensions of `x`.</span>
<span class="sd">    </span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">      x = nn.Variable.from_numpy_array(np.ones((2, 3, 4)))</span>
<span class="sd">      assert F.pad(x, (1, 1, 2, 2)).shape == (2, 5, 8)</span>
<span class="sd">    </span>
<span class="sd">    Padding is performed according to the requested `mode`:</span>
<span class="sd">    </span>
<span class="sd">    constant</span>
<span class="sd">      Pads with a value given by the keyword argument `constant_value`.</span>
<span class="sd">    </span>
<span class="sd">      .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">        x = nn.Variable.from_numpy_array(np.array([1, 2, 3, 4], dtype=np.int))</span>
<span class="sd">        y = F.pad(x, (3, 3), &#39;constant&#39;, constant_value = -1)</span>
<span class="sd">        y.forward()</span>
<span class="sd">        assert np.all(y.d == np.array([-1, -1, -1, 1, 2, 3, 4, -1, -1, -1]))</span>
<span class="sd">    </span>
<span class="sd">    reflect</span>
<span class="sd">      Pads with the reflection of the vector mirrored on the first</span>
<span class="sd">      and last values of the vector along each axis.</span>
<span class="sd">    </span>
<span class="sd">      .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">        x = nn.Variable.from_numpy_array(np.array([1, 2, 3, 4], dtype=np.int))</span>
<span class="sd">        y = F.pad(x, (3, 3), &#39;reflect&#39;)</span>
<span class="sd">        y.forward()</span>
<span class="sd">        assert np.all(y.d == np.array([4, 3, 2, 1, 2, 3, 4, 3, 2, 1]))</span>
<span class="sd">    </span>
<span class="sd">    repeat</span>
<span class="sd">      Pads with the edge value of the vector along each axis.</span>
<span class="sd">    </span>
<span class="sd">      .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">        x = nn.Variable.from_numpy_array(np.array([1, 2, 3, 4], dtype=np.int))</span>
<span class="sd">        y = F.pad(x, (3, 3), &#39;repeat&#39;)</span>
<span class="sd">        y.forward()</span>
<span class="sd">        assert np.all(y.d == np.array([1, 1, 1, 1, 2, 3, 4, 4, 4, 4]))</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        pad_width(repeated int64): Iterable of *before* and *after* pad values.</span>
<span class="sd">        mode(string): Padding mode string.</span>
<span class="sd">            [default= `&#39;constant&#39;` ]</span>
<span class="sd">        constant_value(float): Fill value if mode is `constant`.</span>
<span class="sd">            [default= `0` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: </span>
<span class="sd">            Padded N-D array with the same number of dimensions as the input.</span>
<span class="sd">            </span>
<span class="sd">            .. code-block:: python</span>
<span class="sd">            </span>
<span class="sd">              x = nn.Variable((3, 3, 4, 2))  # a shape like (B, C, H, W)</span>
<span class="sd">              # 1-D padding: last dim by 1 left and 2 on the right side</span>
<span class="sd">              assert F.pad(x, (1, 2)).shape == (3, 3, 4, 5)</span>
<span class="sd">              # 2-D padding: last dim by (1, 1) and 2nd to last by (2, 2)</span>
<span class="sd">              assert F.pad(x, (2, 2, 1, 1)).shape == (3, 3, 8, 4)</span>
<span class="sd">              # 3-D padding: dims C by (0, 1), H by (2, 1), and W by (3, 3)</span>
<span class="sd">              assert F.pad(x, (0, 1, 2, 1, 3, 3)).shape == (3, 4, 7, 8)</span>
<span class="sd">            </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Pad</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">pad_width</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">constant_value</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="transpose"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.transpose">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transposes tensor dimensions.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        axes(repeated int64): Source axis indices for each axis.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Transposed N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Transpose</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axes</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="broadcast"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.broadcast">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Broadcasting ND-array to the specified shape.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        shape(:obj:`tuple` of :obj:`int`): Shape broadcasted to. The size must be the same in axis where ``x``&#39;s shape is not 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Broadcasted N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Broadcast</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">shape</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="broadcast_to"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.broadcast_to">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">broadcast_to</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;.. WARNING::</span>
<span class="sd">      This function is experimental support, so please do not actively use it.</span>
<span class="sd">    </span>
<span class="sd">    Broadcasting ND-array to the specified buffer.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        y(~nnabla.Variable): N-D array</span>
<span class="sd">        axis(int): Target axis to start broadcasting. If this is not set, broadcast will try to fit y to x starting from the last dimension</span>
<span class="sd">            [default= `-1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Broadcasted N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BroadcastTo</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">tile</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">reps</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward input `x` repeated the number of times given by `reps`. If `reps`</span>
<span class="sd">    is a sequence, the output has dimension of ``d = max(len(reps), x.ndim)``</span>
<span class="sd">    and either `x` is promoted to be d-dimensional by prepending new axes or</span>
<span class="sd">    `reps` is promoted to x.ndim by prepending 1&#39;s.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        reps(repeated int64): The number of repetitions of `x` along each axis.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Tile</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">reps</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<div class="viewcode-block" id="one_hot"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.one_hot">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">one_hot</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function creates one-hot vector based on input indices.</span>
<span class="sd">    </span>
<span class="sd">            Example:</span>
<span class="sd">    </span>
<span class="sd">            .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">              import nnabla as nn</span>
<span class="sd">              import nnabla.functions as F</span>
<span class="sd">              import numpy as np</span>
<span class="sd">    </span>
<span class="sd">              labels = nn.Variable.from_numpy_array(np.array([[9], [4], [5], [1], [0]]))</span>
<span class="sd">              print(labels.shape)  # (5, 1)</span>
<span class="sd">    </span>
<span class="sd">              num_class = 10</span>
<span class="sd">    </span>
<span class="sd">              y_train = F.one_hot(labels, shape=(num_class, ))</span>
<span class="sd">              y_train.forward()</span>
<span class="sd">    </span>
<span class="sd">              print(y_train.shape)  # (5, 10)</span>
<span class="sd">              print(y_train.d)</span>
<span class="sd">    </span>
<span class="sd">              # [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]</span>
<span class="sd">              #  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]</span>
<span class="sd">              #  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]</span>
<span class="sd">              #  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]</span>
<span class="sd">              #  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]</span>
<span class="sd">    </span>
<span class="sd">              # Can also be used for ndarray.</span>
<span class="sd">    </span>
<span class="sd">              labels = nn.Variable.from_numpy_array(np.array([[1, 7], [4, 7], [8, 6], [5, 0], [2, 6]]))</span>
<span class="sd">              print(labels.shape)  # (5, 2)</span>
<span class="sd">    </span>
<span class="sd">              num_class_1, num_class_2  = 10, 8</span>
<span class="sd">    </span>
<span class="sd">              y_train = F.one_hot(labels, shape=(num_class_1, num_class_2))</span>
<span class="sd">              y_train.forward()</span>
<span class="sd">    </span>
<span class="sd">              print(y_train.shape)  # (5, 10, 8)</span>
<span class="sd">              print(y_train.d)</span>
<span class="sd">    </span>
<span class="sd">              # [[[0. 0. 0. 0. 0. 0. 0. 0.]          [[0. 0. 0. 0. 0. 0. 0. 0.]</span>
<span class="sd">              #   [0. 0. 0. 0. 0. 0. 0. 1.]           [0. 0. 0. 0. 0. 0. 0. 0.]</span>
<span class="sd">              #   [0. 0. 0. 0. 0. 0. 0. 0.]           [0. 0. 0. 0. 0. 0. 1. 0.]</span>
<span class="sd">              #   [0. 0. 0. 0. 0. 0. 0. 0.]           [0. 0. 0. 0. 0. 0. 0. 0.]</span>
<span class="sd">              #   [0. 0. 0. 0. 0. 0. 0. 0.]           [0. 0. 0. 0. 0. 0. 0. 0.]</span>
<span class="sd">              #   [0. 0. 0. 0. 0. 0. 0. 0.]    ...    [0. 0. 0. 0. 0. 0. 0. 0.]</span>
<span class="sd">              #   [0. 0. 0. 0. 0. 0. 0. 0.]           [0. 0. 0. 0. 0. 0. 0. 0.]</span>
<span class="sd">              #   [0. 0. 0. 0. 0. 0. 0. 0.]           [0. 0. 0. 0. 0. 0. 0. 0.]</span>
<span class="sd">              #   [0. 0. 0. 0. 0. 0. 0. 0.]           [0. 0. 0. 0. 0. 0. 0. 0.]</span>
<span class="sd">              #   [0. 0. 0. 0. 0. 0. 0. 0.]],         [0. 0. 0. 0. 0. 0. 0. 0.]]]</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array representing label&#39;s indice.</span>
<span class="sd">        shape(:obj:`tuple` of :obj:`int`): Number of classes. Note that it must be exactly the same as the number of classes included in label data. Passing incorrect numbers might cause an unexpected error and currently this function doesn&#39;t check if the input is valid or not. Also, when nd-labels are given, dimensions must match. See the example above.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array one-hot vector/tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">OneHot</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">shape</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="flip"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.flip">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">flip</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the order of elements of the specified dimension of an array.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        axes(repeated int64): The index of the dimension to reverse the order of the elements. Axis indices take on values 0, 1, 2, and so on from the left. For example, to flip a 32 (W) by 24 (H) 100 RGB image (100,3,24,32) vertically and horizontally, specify (2,3).</span>
<span class="sd">            [default= `[len(x.shape) - 1]` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Flip</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axes</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="shift"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.shift">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">shift</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">shifts</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">border_mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Shifts the array elements by the specified amount.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        shifts(repeated int64): The amount to shift elements. For example, to shift image data to the right by 2 pixels and up 3 pixels, specify (-3,2).</span>
<span class="sd">            [default= `(0,) * len(x.shape)` ]</span>
<span class="sd">        border_mode(string): Specify how to process the ends of arrays whose values will be undetermined as a result of shifting. nearest: The data at the ends of the original      array is copied and used. reflect: Original data reflected      at the ends of the original array is used.</span>
<span class="sd">            [default= `&#39;nearest&#39;` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">shifts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">shifts</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Shift</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">shifts</span><span class="p">,</span> <span class="n">border_mode</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">sort</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">with_index</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">only_index</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sorts the elements of `x` along a given `axis` in ascending</span>
<span class="sd">    order by value. A negative `axis` counts from the last dimension</span>
<span class="sd">    of `x`, so the default of -1 sorts along the last dimension. If</span>
<span class="sd">    `reverse` is True, then the elements are soreted in descending</span>
<span class="sd">    order.</span>
<span class="sd">    </span>
<span class="sd">    If `with_index` is True, result is a tuple ``(sorted, indices)``</span>
<span class="sd">    or only ``indices`` if `only_index` is True. Setting</span>
<span class="sd">    `only_index` to True implies that `with_index` is also True.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        axis(int): Axis along which to sort.</span>
<span class="sd">            [default= `-1` ]</span>
<span class="sd">        reverse(bool): Sort in descending order.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        with_index(bool): Return sorted values and index.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        only_index(bool): Return only the sort index.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: list of N-D arrays</span>
<span class="sd">            [variadic][parameter]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Sort</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">reverse</span><span class="p">,</span> <span class="n">with_index</span><span class="p">,</span> <span class="n">only_index</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<div class="viewcode-block" id="reshape"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.reshape">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">reshape</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reshapes the input variable in-place. It does not create a copy of the variable.</span>
<span class="sd">    The output variable (y) has a new shape but points to the same data as the input variable (x).</span>
<span class="sd">    This means that if the data in the output variable (y) is modified, the data in the input</span>
<span class="sd">    variable (x) also gets modified since the reshape was done in-place.</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        This function has the same behavior as the :meth:`nnabla.Variable.reshape` method.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        shape(:obj:`tuple` of :obj:`int`): Dimensions for each axis. ``-1`` can be specified only in one shape dimension. The value is calculated from the size of the array and remaining dimensions.</span>
<span class="sd">        inplace(bool): The output array is shared with the input array if True.</span>
<span class="sd">            [default= `True` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Reshaped N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Reshape</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="matrix_diag"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.matrix_diag">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">matrix_diag</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns an array where the last two dimensions consist of the diagonal matrix.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array with shape (:math:`M_0 \times \ldots \times M_N`).</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with shape (:math:`M_0 \times \ldots \times M_N \times M_N`).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">MatrixDiag</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="matrix_diag_part"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.matrix_diag_part">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">matrix_diag_part</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns an array in which the values of the last dimension consist of the diagonal</span>
<span class="sd">    elements of the last two dimensions of an input array.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array with shape (:math:`M_0 \times \ldots \times M_N \times M_N`).</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with shape (:math:`M_0 \times \ldots \times M_N`).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">MatrixDiagPart</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">meshgrid</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return coordinate matrices from coordinate vectors. Given N 1-D arrays, this function returns N-D coordinate arrays for vectorized evaluations on an N-D grid.</span>
<span class="sd">    Example: </span>
<span class="sd">      &gt;&gt;&gt; x,y = F.meshgrid(F.arange(0,3), F.arange(0,2))</span>
<span class="sd">      &gt;&gt;&gt; x.d</span>
<span class="sd">      array([[0., 1., 2.],</span>
<span class="sd">             [0., 1., 2.]], dtype=float32)</span>
<span class="sd">     &gt;&gt;&gt; y.d </span>
<span class="sd">     array([[0., 0., 0.],</span>
<span class="sd">            [1., 1., 1.]], dtype=float32)</span>
<span class="sd">    </span>
<span class="sd">     &gt;&gt;&gt; i,j = F.meshgrid(F.arange(0,3), F.arange(0,2), ij_indexing=True)</span>
<span class="sd">     &gt;&gt;&gt; i.d </span>
<span class="sd">     array([[0., 0.],</span>
<span class="sd">            [1., 1.],</span>
<span class="sd">            [2., 2.]], dtype=float32)</span>
<span class="sd">     &gt;&gt;&gt; j.d </span>
<span class="sd">     array([[0., 1.],</span>
<span class="sd">            [0., 1.],</span>
<span class="sd">            [0., 1.]], dtype=float32)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        *x(~nnabla.Variable): N-D arrays.</span>
<span class="sd">            [variadic]</span>
<span class="sd">        ij_indexing(bool): If set true (Matrix (&#39;ij&#39;) indexing ), the broadcasting dimensions are swapped. Default is False (Cartesian (&#39;xy&#39;) indexing ).</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D arrays</span>
<span class="sd">            [variadic]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;meshgrid must take more than 1 inputs&quot;</span>
    <span class="n">n_outputs</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;n_outputs&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">ij_indexing</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;ij_indexing&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Meshgrid</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">ij_indexing</span><span class="p">)(</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<div class="viewcode-block" id="batch_det"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.batch_det">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">batch_det</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Batch-wise determinant function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">      Y_b = \det(X_b), </span>
<span class="sd">    </span>
<span class="sd">    where :math:`X_b` and :math:`Y_b` are the :math:`b`-th input and output, respectively. </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): batched N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: batched N-D array of determinant</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BatchDet</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="batch_inv"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.batch_inv">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">batch_inv</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns an array of inverted matrix</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): batched N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: batched N-D array of inverted matrix</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BatchInv</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="batch_logdet"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.batch_logdet">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">batch_logdet</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Batch-wise log absolute determinant function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">      Y_b = \log(|\det(X_b)|), </span>
<span class="sd">    </span>
<span class="sd">    where :math:`X_b` and :math:`Y_b` are the :math:`b`-th input and output, respectively. </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): batched N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: batched N-D array of log absolute determinant</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BatchLogdet</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="assign"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.assign">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">assign</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Assign source array to destination array just like `tf.assign`.</span>
<span class="sd">    This is useful to synchronize or manually update parameters.</span>
<span class="sd">    </span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">      dst = nn.Variable((2, 3, 4))</span>
<span class="sd">      src = nn.Variable((2, 3, 4))</span>
<span class="sd">      assign = F.assign(dst, src)</span>
<span class="sd">    </span>
<span class="sd">      assign.forward()</span>
<span class="sd">      assert np.allclose(dst.d, src.d) # dst and src have identical values.</span>
<span class="sd">      assert np.allclose(assign.d dst.d) # returned Variable is also identical to dst.</span>
<span class="sd">    </span>
<span class="sd">    Unlike TensorFlow, the returned Variable has a backward path to `dst`:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">      g_{dst} = g_{y}</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        dst(~nnabla.Variable): A destination N-D array</span>
<span class="sd">        src(~nnabla.Variable): A source N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: An assigned array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Assign</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">dst</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="gather"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.gather">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">gather</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">Indices</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Gather from the input data according to the index.</span>
<span class="sd">    </span>
<span class="sd">    Given the input data :math:`X` of :math:`(D_{0}, \ldots, D_{N-1})` shape and</span>
<span class="sd">    the indices :math:`IDX` of :math:`(I_{0}, \ldots, I_{M-1})` shape, in case of `batch_dims = 0`,</span>
<span class="sd">    the gather outputs</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">      &amp;&amp; Y[d_{0}, \ldots, d_{axis - 1}, i_{0}, \ldots, i_{M-1}, d_{axis + 1}, \ldots, d_{N-1}] = \\</span>
<span class="sd">      &amp;&amp; X[d_{0}, \ldots, d_{axis - 1}, IDX[i_{0}, \ldots, i_{M-1}], d_{axis + 1}, \ldots, d_{N-1}].</span>
<span class="sd">    </span>
<span class="sd">    Generally, the gather ouptuts</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">      &amp;&amp; Y[d_{0}, \ldots, d_{axis - 1}, i_{B}, \ldots, i_{M-1}, d_{axis + 1}, \ldots, d_{N-1}] = \\</span>
<span class="sd">      &amp;&amp; X[d_{0}, \ldots, d_{axis - 1}, IDX[i_{0}, \ldots, i_{B - 1}, i_{B} \ldots, i_{M-1}], d_{axis + 1}, \ldots d_{N-1}].</span>
<span class="sd">    </span>
<span class="sd">    where :math:`B` = `batch_dims`.</span>
<span class="sd">    </span>
<span class="sd">    `x.shape[:batch_dims]` must be equal to `indices.shape[:batch_dims]`.</span>
<span class="sd">    </span>
<span class="sd">    Output shape is `x.shape[:axis] + indices.shape[batch_dims:] + x.shape[axis + 1]`.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Data from which to gather.</span>
<span class="sd">        Indices(~nnabla.Variable): Index with which to gather.</span>
<span class="sd">        axis(int): Axis in `x` to gather from. `axis` must be greater than or equal to `batch_dims`.</span>
<span class="sd">            [default= `0` ]</span>
<span class="sd">        batch_dims(int): The number of batch dimensions.</span>
<span class="sd">            [default= `0` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Gathered output.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">batch_dims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">batch_dims</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Gather</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">batch_dims</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">Indices</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">gather_nd</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gather elements or slices from `data` according to `indices`, which must</span>
<span class="sd">    be at least two-dimensional with the first dimension :math:`M` being less</span>
<span class="sd">    or equal to the :math:`N` dimensions of `data`. Given `data` with shape</span>
<span class="sd">    :math:`(X_0, X_1, ..., X_{N-1})` and indices with shape</span>
<span class="sd">    :math:`(M, Y_0, ..., Y_{K-1})` output has shape</span>
<span class="sd">    :math:`(Y_0, ..., Y_{K-1}, X_M, ..., X_{N-1})`. If :math:`M == N`, output</span>
<span class="sd">    shape is simply :math:`(Y_0, ..., Y_{K-1})`.</span>
<span class="sd">    </span>
<span class="sd">    The forward of :func:`~nnabla.functions.gather_nd` is equivalent to:</span>
<span class="sd">    </span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">      def gather_nd(data, index):</span>
<span class="sd">          import numpy as np</span>
<span class="sd">          tmp_index = index.reshape(index.shape[0], -1)</span>
<span class="sd">          tmp_index = (idx + (Ellipsis,) for idx in zip(*new_index))</span>
<span class="sd">          out_shape = index.shape[1:] + data.shape[index.shape[0]:]</span>
<span class="sd">          return np.vstack(data[idx] for idx in tmp_index).reshape(*out_shape)</span>
<span class="sd">    </span>
<span class="sd">    Examples:</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; import numpy as np, nnabla as nn, nnabla.functions as F</span>
<span class="sd">    &gt;&gt;&gt; nn.set_auto_forward(True)</span>
<span class="sd">    &gt;&gt;&gt; data = F.arange(1, 11).reshape([2, 5])</span>
<span class="sd">    &gt;&gt;&gt; print(data.d)</span>
<span class="sd">    [[ 1.  2.  3.  4.  5.]</span>
<span class="sd">     [ 6.  7.  8.  9. 10.]]</span>
<span class="sd">    &gt;&gt;&gt; F.gather_nd(data, [[1, 1, 0]]).shape</span>
<span class="sd">    (3, 5)</span>
<span class="sd">    &gt;&gt;&gt; F.gather_nd(data, [[1, 1, 0], [0, 1, 0]]).shape</span>
<span class="sd">    (3,)</span>
<span class="sd">    &gt;&gt;&gt; print(F.gather_nd(data, [[1, 1, 0], [0, 1, 0]]).d)</span>
<span class="sd">    [6. 7. 1.]</span>
<span class="sd">    &gt;&gt;&gt; print(F.gather_nd(data, [[1, 1, 0]]).d)</span>
<span class="sd">    [[ 6.  7.  8.  9. 10.]</span>
<span class="sd">     [ 6.  7.  8.  9. 10.]</span>
<span class="sd">     [ 1.  2.  3.  4.  5.]]</span>
<span class="sd">    </span>
<span class="sd">    When `indices` is provided as a :obj:`~nnabla.Variable` it will be</span>
<span class="sd">    possible to change the actual index values after function creation.</span>
<span class="sd">    It is important to note that out-of-bound indices raise error when</span>
<span class="sd">    running on CPU but are ignored when using an accelerated computation</span>
<span class="sd">    context.</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; indices = nn.Variable((2, 1))</span>
<span class="sd">    &gt;&gt;&gt; indices.d = [[0], [0]]</span>
<span class="sd">    &gt;&gt;&gt; y = F.gather_nd(data, indices)</span>
<span class="sd">    &gt;&gt;&gt; print(y.d)</span>
<span class="sd">    [1.]</span>
<span class="sd">    &gt;&gt;&gt; indices.d = [[1], [4]]</span>
<span class="sd">    &gt;&gt;&gt; y.forward()</span>
<span class="sd">    &gt;&gt;&gt; print(y.d)</span>
<span class="sd">    [10.]</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array input data</span>
<span class="sd">        indices(~nnabla.Variable): N-D array indices</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">GatherNd</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">scatter_nd</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Scatter `data` into a new array of given `shape` according to `indices`.</span>
<span class="sd">    This operation is the inverse of :func:`~nnabla.functions.gather_nd`.</span>
<span class="sd">    </span>
<span class="sd">    The forward of :func:`~nnabla.functions.scatter_nd` is equivalent to:</span>
<span class="sd">    </span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">      def scatter_nd(data, indices, shape):</span>
<span class="sd">          import numpy as np</span>
<span class="sd">          if isinstance(indices, np.ndarray)</span>
<span class="sd">              indices = indices.tolist()</span>
<span class="sd">          result = np.zeros(shape, dtype=data.dtype)</span>
<span class="sd">          result[indices] = data</span>
<span class="sd">          return result</span>
<span class="sd">    </span>
<span class="sd">    Examples:</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; import numpy as np, nnabla as nn, nnabla.functions as F</span>
<span class="sd">    &gt;&gt;&gt; nn.set_auto_forward(True)</span>
<span class="sd">    &gt;&gt;&gt; data = nn.Variable.from_numpy_array(np.array([9, 10, 11, 12]))</span>
<span class="sd">    &gt;&gt;&gt; indices = nn.Variable.from_numpy_array(np.array([[4, 3, 1, 7]]))</span>
<span class="sd">    &gt;&gt;&gt; scattered = F.scatter_nd(data, indices, shape=(8,))</span>
<span class="sd">    &gt;&gt;&gt; print(scatterd.d)</span>
<span class="sd">    [ 0. 11.  0. 10.  9.  0.  0. 12.]</span>
<span class="sd">    &gt;&gt;&gt; print(F.gather_nd(scattered, indices).d)</span>
<span class="sd">    [ 9. 10. 11. 12.]</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        data(~nnabla.Variable): N-D array input data.</span>
<span class="sd">        indices(~nnabla.Variable): N-D array scatter indices.</span>
<span class="sd">        out(~nnabla.Variable): existing output array</span>
<span class="sd">            [optional]</span>
<span class="sd">        shape(repeated int64): Shape of output variable.</span>
<span class="sd">            [default= `None` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array of given `shape`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">data</span><span class="p">,</span> <span class="n">indices</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">out</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ScatterNd</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">shape</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">scatter_add</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Add all values from `x1` into the `x0` according to index specified by `indices`.</span>
<span class="sd">    This function adds `x1` into the copy of `x0` and outputs the copy.</span>
<span class="sd">    The original `x0` will not be changed.</span>
<span class="sd">    `x0`, `indices` and `x1` must have same number of dimensions.</span>
<span class="sd">    </span>
<span class="sd">    The forward of :func:`~nnabla.functions.scatter_add` is equivalent to:</span>
<span class="sd">    </span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">      def scatter_add(x0, indices, x1, axis):</span>
<span class="sd">          # Assuming each input is 3 dimensional</span>
<span class="sd">          import numpy as np</span>
<span class="sd">          output = np.copy(x0)</span>
<span class="sd">          for i in range(indices.shape[0]):</span>
<span class="sd">              for j in range(indices.shape[1]):</span>
<span class="sd">                  for k in range(indices.shape[2]):</span>
<span class="sd">                      if axis == 0:</span>
<span class="sd">                          output[indices[i][j][k]][j][k] += x1[i][j][k]</span>
<span class="sd">                      elif axis == 1:</span>
<span class="sd">                          output[i][indices[i][j][k]][k] += x1[i][j][k]</span>
<span class="sd">                      elif axis == 2:</span>
<span class="sd">                          output[i][j][indices[i][j][k]] += x1[i][j][k]</span>
<span class="sd">          return output</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array which the data is added to its copy.</span>
<span class="sd">        indices(~nnabla.Variable): N-D array scatter indices. The size of each dimension must be equal or smaller than that of x0 except for the specified axis. The value of indices must be smaller than the size of specified axis&#39; dimension of x0. The size of each dimension must be equal or smaller than that of x1. Indices must not be negative.</span>
<span class="sd">        x1(~nnabla.Variable): N-D array which is scattered and added to x0.</span>
<span class="sd">        axis(int): Axis along which to index. The axis must not exceed the inputs&#39; dimension.</span>
<span class="sd">            [default= `0` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array which contains the result of scatter addition. The shape is same as x0.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ScatterAdd</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<div class="viewcode-block" id="pack_padded_sequence"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.pack_padded_sequence">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">pack_padded_sequence</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">padded_sequence</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Pack a padded variable-length sequences.</span>
<span class="sd">    </span>
<span class="sd">    This method packs a padded variable-length sequences.</span>
<span class="sd">    </span>
<span class="sd">    :math:`T_i` is the length of the :math:`i`-th Variable in the sequences.</span>
<span class="sd">    :math:`B` is the batch size equal to the length of the sequences.</span>
<span class="sd">    :math:`T` is the max of :math:`T_i` for all :math:`i`.</span>
<span class="sd">    :math:`*` is the remaining dimensions including none.</span>
<span class="sd">    </span>
<span class="sd">    .. note::</span>
<span class="sd">      This function assumes the length-sorted padded sequence in the decreasing order</span>
<span class="sd">      and must be used by :func:`~nnabla.utils.rnn.pack_padded_sequence` in the dynamic computation mode.</span>
<span class="sd">      See :</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        padded_sequence(~nnabla.Variable): Padded sequence of (:math:`T \times B \times *`) or (:math:`B \times T \times *`) shape.</span>
<span class="sd">            </span>
<span class="sd">        lengths(~nnabla.Variable): Sequence length for each batch and always resides in CPU.</span>
<span class="sd">        batch_first(bool): `padded_sequence` is of (:math:`T`, :math:`B`, :math:`*`) shape if False,</span>
<span class="sd">            otherwise (:math:`B`, :math:`T`, :math:`*`).</span>
<span class="sd">            </span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Packed sequence of (:math:`N`, :math:`*`) shape.</span>
<span class="sd">        ~nnabla.Variable: Batch size for each time and always resides in CPU.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">PackPaddedSequence</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">batch_first</span><span class="p">)(</span><span class="n">padded_sequence</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="pad_packed_sequence"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.pad_packed_sequence">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">pad_packed_sequence</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">packed_sequence</span><span class="p">,</span> <span class="n">batch_sizes</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">total_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Pad packed sequence.</span>
<span class="sd">    </span>
<span class="sd">    This method unpacks the packed sequqnce and pad it, the inverse operation of :func:`pack_padded_sequence`.</span>
<span class="sd">    </span>
<span class="sd">    :math:`T_i` is the length of the :math:`i`-th Variable in the sequences.</span>
<span class="sd">    :math:`B` is the batch size equal to the length of the sequences.</span>
<span class="sd">    :math:`T` is the max of :math:`T_i` for all :math:`i`.</span>
<span class="sd">    :math:`*` is the remaining dimensions including none.</span>
<span class="sd">    </span>
<span class="sd">    .. note::</span>
<span class="sd">      This function assumes the output of the length-sorted padded sequence in the decreasing order</span>
<span class="sd">      and must be used by :func:`~nnabla.utils.rnn.pad_packed_sequence` in the dynamic computation mode.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        packed_sequence(~nnabla.Variable): Packed sequence of (:math:`N`, :math:`*`) shape.</span>
<span class="sd">            </span>
<span class="sd">        batch_sizes(~nnabla.Variable): Batch size for each time and always resides in CPU.</span>
<span class="sd">        batch_first(bool): `padded_sequence` is of (:math:`T`, :math:`B`, :math:`*`) shape if False,</span>
<span class="sd">            otherwise (:math:`B`, :math:`T`, :math:`*`).</span>
<span class="sd">            </span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        padding_value(float): Padding value.</span>
<span class="sd">            [default= `0.0` ]</span>
<span class="sd">        total_length(int): If not None, the outputs are padded up to the `total_length`.</span>
<span class="sd">            If the `total_length` is less than the max length in the `sequences`,</span>
<span class="sd">            the error is thrown.</span>
<span class="sd">            </span>
<span class="sd">            [default= `-1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Padded sequence of (:math:`T \times B \times *`) or (:math:`B \times T \times *`) shape.</span>
<span class="sd">        ~nnabla.Variable: Sequence length for each batch and always resides in CPU.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">padding_value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">padding_value</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">if</span> <span class="n">total_length</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">total_length</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">PadPackedSequence</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">batch_first</span><span class="p">,</span> <span class="n">padding_value</span><span class="p">,</span> <span class="n">total_length</span><span class="p">)(</span><span class="n">packed_sequence</span><span class="p">,</span> <span class="n">batch_sizes</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">interpolate</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">half_pixel</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_for_nn</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">channel_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Resize an ND array with interpolation.</span>
<span class="sd">    </span>
<span class="sd">    The last ``len(output_size)`` dimensions of the input ``x`` are considered as the spatial dimensions to be resized.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        output_size(repeated int64): Output size.</span>
<span class="sd">        mode(string): Interpolation mode chosen from (&#39;nearest&#39;|&#39;linear&#39;).</span>
<span class="sd">        align_corners(bool): If true, the corner pixels of input and output arrays are aligned, such that the output corner pixels have the same values with the input corner pixels. The default is ``None``, and it becomes `True` if mode is &#39;linear&#39;, otherwise `False`.</span>
<span class="sd">            [default= `True` ]</span>
<span class="sd">        half_pixel(bool): If true, in the coordinate transformation, 0.5 is added to the output coordinate and 0.5 is subtracted from the input coordinate after scaling.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        half_pixel_for_nn(bool): This is a special argument to support the backward-compatibility of the nearest neighbor interpolation. Default is `False`. When in ``True``, the implementation of nearest neighbor interpolation is the old one.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        channel_last(bool): If True, the last dimension is considered as channel dimension, a.k.a NHWC order.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Interpolate</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">,</span> <span class="n">half_pixel</span><span class="p">,</span> <span class="n">half_pixel_for_nn</span><span class="p">,</span> <span class="n">channel_last</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<div class="viewcode-block" id="fft"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.fft">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">fft</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">signal_ndim</span><span class="p">,</span> <span class="n">normalized</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Complex-to-complex Discrete Fourier Transform,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">      X_{k_1, \ldots, k_d} = \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x_{n_1, \ldots, n_d} \exp\left(-2 \pi j \left( \sum_{i=0}^{d} \frac{k_i n_i}{N_i} \right) \right),</span>
<span class="sd">    </span>
<span class="sd">    where</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">      k_i = 0, \ldots, N_i - 1.</span>
<span class="sd">    </span>
<span class="sd">    This function now supports 1-D, 2-D, and 3-D DFT with or without the leading batch dimension(s).</span>
<span class="sd">    </span>
<span class="sd">    The input is expected to be complex-valued with at least signal_ndim + 1 dimensions.</span>
<span class="sd">    The last dimension has a shape of two where x[..., 0] is the real part and x[..., 1] the imaginary part.</span>
<span class="sd">    </span>
<span class="sd">    Example:</span>
<span class="sd">    </span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">      import numpy as np</span>
<span class="sd">      import nnabla as nn</span>
<span class="sd">      import nnabla.functions as F</span>
<span class="sd">      from nnabla.ext_utils import get_extension_context</span>
<span class="sd">    </span>
<span class="sd">      ctx = get_extension_context(&quot;cudnn&quot;)</span>
<span class="sd">      nn.set_default_context(ctx)</span>
<span class="sd">    </span>
<span class="sd">      # Example for a batched 2D-FFT and 2D-IFFT (batch-size: 2, data-size: 4x3)</span>
<span class="sd">      x_data = np.random.rand(2, 4, 3) + 1j * np.random.rand(2, 4, 3)</span>
<span class="sd">      x = nn.Variable.from_numpy_array(np.stack([np.real(x_data), np.imag(x_data)], axis=3))</span>
<span class="sd">      y = F.fft(x, signal_ndim=2, normalized=True)</span>
<span class="sd">      z = F.ifft(y, signal_ndim=2, normalized=True)</span>
<span class="sd">      z.forward()</span>
<span class="sd">    </span>
<span class="sd">      np.allclose(z.d[..., 0] + 1j*z.d[...,1], x_data)</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input.</span>
<span class="sd">        signal_ndim(int): The number of dimensions for each signal. It must be 1, 2, or 3.</span>
<span class="sd">        normalized(bool): Use unitary normalization. If `True`, the normalization constant :math:`\sqrt{\frac{1}{\prod_{i=1}^{d} N_i}}` is multiplied.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: FFT transformed signal.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">FFT</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">signal_ndim</span><span class="p">,</span> <span class="n">normalized</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="ifft"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.ifft">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">ifft</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">signal_ndim</span><span class="p">,</span> <span class="n">normalized</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Complex-to-complex inverse Discrete Fourier Transform,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">      X_{k_1, \ldots, k_d} = \frac{1}{\prod_{i=1}^{d} N_i} \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x_{n_1, \ldots, n_d} \exp\left(2 \pi j \left( \sum_{i=0}^{d} \frac{k_i n_i}{N_i} \right) \right),</span>
<span class="sd">    </span>
<span class="sd">    where</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">      k_i = 0, \ldots, N_i - 1.</span>
<span class="sd">    </span>
<span class="sd">    This function now supports 1-D, 2-D, and 3-D DFT with or without the leading batch dimension(s).</span>
<span class="sd">    </span>
<span class="sd">    The input is expected to be complex-valued with at least signal_ndim + 1 dimensions.</span>
<span class="sd">    The last dimension has a shape of two where x[..., 0] is the real part and x[..., 1] the imaginary part.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input.</span>
<span class="sd">        signal_ndim(int): The number of dimensions for each signal. It must be 1, 2, or 3.</span>
<span class="sd">        normalized(bool): Use unitary normalization. If `True`, the normalization constant :math:`\frac{1}{\prod_{i=1}^{d} N_i}` becomes :math:`\sqrt{\frac{1}{\prod_{i=1}^{d} N_i}}`.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: IFFT transformed signal.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">IFFT</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">signal_ndim</span><span class="p">,</span> <span class="n">normalized</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">stft</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">window_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">fft_size</span><span class="p">,</span> <span class="n">window_type</span><span class="o">=</span><span class="s1">&#39;hanning&#39;</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;reflect&#39;</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Short-time Fourier transform.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Time domain sequence of size `batch_size x sample_size`.</span>
<span class="sd">        window_size(int): Size of STFT analysis window.</span>
<span class="sd">        stride(int): Number of samples that we shift the window, also called `hop size`.</span>
<span class="sd">        fft_size(int): Size of the FFT, the output will have `fft_size // 2+ 1` frequency bins.</span>
<span class="sd">        window_type(string): Analysis window, can be either `hanning`, `hamming` or `rectangular`.</span>
<span class="sd">            [default= `&#39;hanning&#39;` ]</span>
<span class="sd">        center(bool): If `True`, then the signal `x` is padded by half the FFT size using reflection padding.</span>
<span class="sd">            [default= `True` ]</span>
<span class="sd">        pad_mode(string): Padding mode, which can be `&#39;constant&#39;` or `&#39;reflect&#39;`. `&#39;constant&#39;` pads with `0`.</span>
<span class="sd">            [default= `&#39;reflect&#39;` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Real part of STFT of size `batch_size x fft_size//2 + 1 x frame_size`.</span>
<span class="sd">        ~nnabla.Variable: Imaginary part of STFT of size `batch_size x fft_size//2 + 1 x frame_size`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">STFT</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">window_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">fft_size</span><span class="p">,</span> <span class="n">window_type</span><span class="p">,</span> <span class="n">center</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">istft</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">y_r</span><span class="p">,</span> <span class="n">y_i</span><span class="p">,</span> <span class="n">window_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">fft_size</span><span class="p">,</span> <span class="n">window_type</span><span class="o">=</span><span class="s1">&#39;hanning&#39;</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Inverse short-time Fourier transform.</span>
<span class="sd">    </span>
<span class="sd">    .. note::</span>
<span class="sd">      We use a constant square inverse window for the reconstruction of the time-domain signal, therefore, the first and last `window_size - stride` are not perfectly reconstructed.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        y_r(~nnabla.Variable): Real part of STFT of size `batch_size x fft_size//2 + 1 x frame_size`.</span>
<span class="sd">        y_i(~nnabla.Variable): Imaginary part of STFT of size `batch_size x fft_size//2 + 1 x frame_size`.</span>
<span class="sd">        window_size(int): Size of STFT analysis window.</span>
<span class="sd">        stride(int): Number of samples that we shift the window, also called `hop size`.</span>
<span class="sd">        fft_size(int): Size of the FFT, the output will have `fft_size // 2+ 1` frequency bins.</span>
<span class="sd">        window_type(string): Analysis window, can be either `hanning`, `hamming` or `rectangular`.</span>
<span class="sd">            [default= `&#39;hanning&#39;` ]</span>
<span class="sd">        center(bool): If `True`, then the signal `x` is padded by half the FFT size using reflection padding.</span>
<span class="sd">            [default= `True` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Time domain sequence of size `batch_size x sample_size`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ISTFT</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">window_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">fft_size</span><span class="p">,</span> <span class="n">window_type</span><span class="p">,</span> <span class="n">center</span><span class="p">)(</span><span class="n">y_r</span><span class="p">,</span> <span class="n">y_i</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">dropout</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_mask</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Dropout.</span>
<span class="sd">    Samples a number :math:`u` from a uniform distribution in :math:`[0, 1]` ,</span>
<span class="sd">    and ignores the input if :math:`u \leq p`.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y = \left\{</span>
<span class="sd">        \begin{array}{ll}</span>
<span class="sd">          \frac{x}{1 - p} &amp; (u &gt; p) \\</span>
<span class="sd">          0 &amp; ({\rm otherwise})</span>
<span class="sd">        \end{array} \right.</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        Usually dropout only applied during training as below</span>
<span class="sd">        (except `MC dropout`_). If you want to use dropout as an MC dropout, remove &#39;if train:&#39;.</span>
<span class="sd">    </span>
<span class="sd">        .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">            h = PF.affine(x, num_hidden)</span>
<span class="sd">            if train:</span>
<span class="sd">                h = F.dropout(h, 0.5)</span>
<span class="sd">    </span>
<span class="sd">    .. _MC dropout: https://arxiv.org/abs/1506.02142</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        p(float): :math:`p` in definition.</span>
<span class="sd">            [default= `0.5` ]</span>
<span class="sd">        seed(int): Random seed. When -1, seed is sampled from global random number generator.</span>
<span class="sd">            [default= `-1` ]</span>
<span class="sd">        output_mask(bool): Whether or not to output mask.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">output_mask</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<div class="viewcode-block" id="top_k_data"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.top_k_data">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">top_k_data</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="nb">abs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Select the `k` largest values from each sample in `x` to</span>
<span class="sd">    propagate unmodified and set all other values to 0. If `abs` is</span>
<span class="sd">    True, the `k` largest values are selected by magnitude. If</span>
<span class="sd">    `reduce` is True (the default), all feature dimensions are</span>
<span class="sd">    reduced to a single dimension of size `k` that propagates only</span>
<span class="sd">    the `k` largest values. Otherwise, if `reduce` is False, input</span>
<span class="sd">    and output dimensions are identical. Dimensions before</span>
<span class="sd">    `base_axis` are treated as number of sample dimensions and `k`</span>
<span class="sd">    values get selected from all elements of a sample (dimensions</span>
<span class="sd">    from `base_axis`) regardless of shape.</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; import nnabla as nn, nnabla.functions as F</span>
<span class="sd">    &gt;&gt;&gt; x = nn.Variable((4, 5, 6))</span>
<span class="sd">    &gt;&gt;&gt; F.top_k_data(x, 3, reduce=False).shape</span>
<span class="sd">    (4, 5, 6)</span>
<span class="sd">    &gt;&gt;&gt; F.top_k_data(x, 3, reduce=True).shape</span>
<span class="sd">    (4, 3)</span>
<span class="sd">    &gt;&gt;&gt; F.top_k_data(x, 3, reduce=True, base_axis=2).shape</span>
<span class="sd">    (4, 5, 3)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        k(int): Number of largest data values to propagate.</span>
<span class="sd">        abs(bool): Determine largest data values by magnitude.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        reduce(bool): Reduce feature size to one dimension of size `k`.</span>
<span class="sd">            [default= `True` ]</span>
<span class="sd">        base_axis(int): First dimension of the sample shape.</span>
<span class="sd">            [default= `1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">TopKData</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="nb">abs</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="top_k_grad"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.top_k_grad">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">top_k_grad</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="nb">abs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Select the `k` largest gradients for each sample in `x` to</span>
<span class="sd">    back-propagate unmodified and set all other gradients to 0. If</span>
<span class="sd">    `abs` is True, the `k` largest gradients are selected by</span>
<span class="sd">    magnitude. Dimensions before `base_axis` are treated as number</span>
<span class="sd">    of sample dimensions and `k` gradients get selected from all</span>
<span class="sd">    gradients of a sample (dimensions from `base_axis`) regardless</span>
<span class="sd">    of shape.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        k(int): Number of largest gradients to propagate.</span>
<span class="sd">        abs(bool): Determine largest gradients by magnitude.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        base_axis(int): First dimension of the sample shape.</span>
<span class="sd">            [default= `1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with same shape and data as `x`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">TopKGrad</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="nb">abs</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="rand"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.rand">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">rand</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Samples numbers from a uniform distribution :math:`x \sim U(low, high)`</span>
<span class="sd">    given lowest value :math:`low`, upper bound :math:`high`,</span>
<span class="sd">    and shape of the returned Variable.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        low(float): :math:`low` in definition.</span>
<span class="sd">            [default= `0` ]</span>
<span class="sd">        high(float): :math:`high` in definition.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        shape(:obj:`tuple` of :obj:`int`): Shape of returned variable.</span>
<span class="sd">            [default= `[]` ]</span>
<span class="sd">        seed(int): Random seed. When -1, seed is sampled from global random number generator.</span>
<span class="sd">            [default= `-1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Variable with the shape specified in the argument.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Rand</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">seed</span><span class="p">)(</span><span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="randint"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.randint">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">randint</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Samples integer numbers from a uniform distribution :math:`x \sim U(low, high)`</span>
<span class="sd">    given lowest value :math:`low`, upper bound :math:`high`,</span>
<span class="sd">    and shape of the returned Variable.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        low(int): :math:`low` in definition.</span>
<span class="sd">            [default= `0` ]</span>
<span class="sd">        high(int): :math:`high` in definition.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        shape(:obj:`tuple` of :obj:`int`): Shape of returned variable.</span>
<span class="sd">            [default= `[]` ]</span>
<span class="sd">        seed(int): Random seed. When -1, seed is sampled from global random number generator.</span>
<span class="sd">            [default= `-1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Variable with the shape specified in the argument. The dtype is int32.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Randint</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">seed</span><span class="p">)(</span><span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="randn"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.randn">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">randn</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Samples numbers from a normal distribution :math:`x \sim N(\mu, \sigma)`</span>
<span class="sd">    given mean :math:`\mu`, standard deviation :math:`\sigma`,</span>
<span class="sd">    and shape of the returned Variable.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        mu(float): :math:`\mu` in definition.</span>
<span class="sd">            [default= `0` ]</span>
<span class="sd">        sigma(float): :math:`\sigma` in definition.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        shape(:obj:`tuple` of :obj:`int`): Shape of returned variable.</span>
<span class="sd">            [default= `[]` ]</span>
<span class="sd">        seed(int): Random seed. When -1, seed is sampled from global random number generator.</span>
<span class="sd">            [default= `-1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Variable with the shape specified in the argument.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Randn</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">seed</span><span class="p">)(</span><span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="rand_binomial"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.rand_binomial">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">rand_binomial</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Samples numbers from a binomial distribution :math:`x \sim B(n, p)`</span>
<span class="sd">    given the numbers of trials :math:`n`, probability :math:`p`,</span>
<span class="sd">    and shape of the returned Variable.</span>
<span class="sd">    When :math:`n = 1`, this behaves like the Bernoulli distriburion.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        n(int): :math:`n` in definition, the number of trials.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        p(float): :math:`p` in definition, probability of success.</span>
<span class="sd">            [default= `0.5` ]</span>
<span class="sd">        shape(:obj:`tuple` of :obj:`int`): Shape of returned variable.</span>
<span class="sd">            [default= `[]` ]</span>
<span class="sd">        seed(int): Random seed. When -1, seed is sampled from global random number generator.</span>
<span class="sd">            [default= `-1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Variable with the shape specified in the argument.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">RandBinomial</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">seed</span><span class="p">)(</span><span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="rand_beta"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.rand_beta">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">rand_beta</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Samples numbers from a beta distribution :math:`x \sim \beta(\alpha, \beta)`.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        alpha(float): :math:`\alpha`, scale parameter.</span>
<span class="sd">            [default= `0.5` ]</span>
<span class="sd">        beta(float): :math:`\beta`, scale parameter.</span>
<span class="sd">            [default= `0.5` ]</span>
<span class="sd">        shape(:obj:`tuple` of :obj:`int`): Shape of returned variable.</span>
<span class="sd">            [default= `[]` ]</span>
<span class="sd">        seed(int): Random seed. When -1, seed is sampled from global random number generator.</span>
<span class="sd">            [default= `-1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Variable with the shape specified in the argument.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">RandBeta</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">seed</span><span class="p">)(</span><span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="rand_gamma"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.rand_gamma">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">rand_gamma</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Samples numbers from a gamma distribution :math:`x \sim \frac {\gamma(k, \frac {x}{\theta})}{\Gamma(k)}`.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        k(float): k, scale parameter.</span>
<span class="sd">            [default= `0.5` ]</span>
<span class="sd">        theta(float): :math:`\theta`, scale parameter.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        shape(:obj:`tuple` of :obj:`int`): Shape of returned variable.</span>
<span class="sd">            [default= `[]` ]</span>
<span class="sd">        seed(int): Random seed. When -1, seed is sampled from global random number generator.</span>
<span class="sd">            [default= `-1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Variable with the shape specified in the argument.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">RandGamma</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">seed</span><span class="p">)(</span><span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="random_choice"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.random_choice">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">random_choice</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate random samples from population `x` with selection probabilities</span>
<span class="sd">    determined by the relative weights `w`. The number of samples to draw is</span>
<span class="sd">    given by the product of `shape`\s dimensions, and the samples are returned</span>
<span class="sd">    with the given `shape`. By default, samples are drawn with replacement,</span>
<span class="sd">    i.e. selection of a specific population member is solely determined by</span>
<span class="sd">    its associated weight. Sampling without replacement, where any population</span>
<span class="sd">    member may be drawn only once, is used if `replace` is set to False.</span>
<span class="sd">    </span>
<span class="sd">    For both `x` and `w` the innermost dimension corresponds to the individual</span>
<span class="sd">    populations and their weights from which samples are returned with the</span>
<span class="sd">    requested `shape` following all outermost dimensions of the input.</span>
<span class="sd">    </span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">      import nnabla as nn</span>
<span class="sd">      import nnabla.functions as F</span>
<span class="sd">      import numpy as np</span>
<span class="sd">      nn.set_auto_forward(True)</span>
<span class="sd">    </span>
<span class="sd">      # x holds two populations</span>
<span class="sd">      x = nn.Variable.from_numpy_array(np.array([[11, 22, 33], [110, 220, 330]]))</span>
<span class="sd">      # w holds the weights for each population</span>
<span class="sd">      w = nn.Variable.from_numpy_array(np.array([[10, 20, 70], [70, 20, 10]]))</span>
<span class="sd">    </span>
<span class="sd">      # draw one sample from each population</span>
<span class="sd">      y = F.random_choice(x, w)  # y.shape =&gt; (2, 1)</span>
<span class="sd">    </span>
<span class="sd">      # draw 12 samples with shape (3, 4) from each population</span>
<span class="sd">      y = F.random_choice(x, w, shape=(3, 4))  # y.shape =&gt; (2, 3, 4)</span>
<span class="sd">    </span>
<span class="sd">    Note that weights must not be less than zero and for each population the</span>
<span class="sd">    sum of weights must be greater than zero. Additionally, sampling without</span>
<span class="sd">    replacement requires that the number of non-zero weights is not less than</span>
<span class="sd">    the number of samples to be drawn. These conditions are verified in &quot;cpu&quot;</span>
<span class="sd">    computation context but not when using &quot;cuda&quot; or &quot;cudnn&quot; acceleration</span>
<span class="sd">    (this would require additional device synchronization steps penalizing</span>
<span class="sd">    performance).</span>
<span class="sd">    </span>
<span class="sd">    Random sampling from an implicit array of index values (like categorical</span>
<span class="sd">    or multinomial) can be realized with input `x` constructed as indices.</span>
<span class="sd">    </span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">      w = nn.Variable.from_numpy_array(np.array([1, 2, 3, 2, 1]))</span>
<span class="sd">      y = F.random_choice(F.arange(0, 5), w)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array from which a random sample is generated.</span>
<span class="sd">        w(~nnabla.Variable): N-D array of associated weights of elements in `x`.</span>
<span class="sd">        shape(:obj:`tuple` of :obj:`int`): Number and shape of generated samples.</span>
<span class="sd">            [default= `[]` ]</span>
<span class="sd">        replace(bool): Whether sampling is with or without replacement.</span>
<span class="sd">            [default= `True` ]</span>
<span class="sd">        seed(int): Random seed.</span>
<span class="sd">            [default= `-1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">RandomChoice</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">replace</span><span class="p">,</span> <span class="n">seed</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="random_crop"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.random_crop">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">random_crop</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    RandomCrop randomly extracts a portion of an array.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        shape(:obj:`tuple` of :obj:`int`): The data size to extract. For example, to randomly extract a portion of the image (3,48,48) from a 3,64,64 image, specify (3,48,48).</span>
<span class="sd">            [default= `x.shape` ]</span>
<span class="sd">        base_axis(int): No Description</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        seed(int): Random seed. When -1, seed is sampled from global random number generator.</span>
<span class="sd">            [default= `-1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">RandomCrop</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">seed</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="random_flip"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.random_flip">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">random_flip</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the order of elements of the specified dimension of an array at 50% probability.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        axes(repeated int64): The index of the axis to reverse the order of the elements. Axis indices take on values 0, 1, 2, and so on from the left. For example, to flip a 32 (W) by 24 (H) 100 RGB images (100, 3,24,32) vertically and horizontally at random, specify (2,3).</span>
<span class="sd">            [default= `[len(x.shape) - 1]` ]</span>
<span class="sd">        base_axis(int): No Description</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        seed(int): Random seed. When -1, seed is sampled from global random number generator.</span>
<span class="sd">            [default= `-1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">RandomFlip</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">seed</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="random_shift"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.random_shift">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">random_shift</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">shifts</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">border_mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">constant_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Randomly shifts the array elements within the specified range.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        shifts(repeated int64): Max absolute amount to shift elements. For example, to shift image data horizontally by :math:`\pm 2` pixels and vertically by :math:`\pm 3` pixels, specify (3,2).</span>
<span class="sd">            [default= `(0,) * len(x.shape)` ]</span>
<span class="sd">        border_mode(string): Specify how to process the ends of arrays whose values will be undetermined as a result of shifting. nearest: The data at the ends of the   original array is copied and used. reflect: Original data reflected at   the ends of the original array is used. constant: Constant value is used.</span>
<span class="sd">            [default= `&#39;nearest&#39;` ]</span>
<span class="sd">        constant_value(float): Value used for outside of the original array if border_mode=&#39;constant&#39;.</span>
<span class="sd">            [default= `0` ]</span>
<span class="sd">        base_axis(int): No Description</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        seed(int): Random seed. When -1, seed is sampled from global random number generator.</span>
<span class="sd">            [default= `-1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">shifts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">shifts</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">RandomShift</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">shifts</span><span class="p">,</span> <span class="n">border_mode</span><span class="p">,</span> <span class="n">constant_value</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">seed</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="random_erase"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.random_erase">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">random_erase</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">area_ratios</span><span class="o">=</span><span class="p">(</span><span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">),</span> <span class="n">aspect_ratios</span><span class="o">=</span><span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">3.3333</span><span class="p">),</span> <span class="n">replacements</span><span class="o">=</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">255.0</span><span class="p">),</span> <span class="n">n</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">share</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">channel_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ste_fine_grained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Randomly erase patches of the inputs and replace with random values.</span>
<span class="sd">    </span>
<span class="sd">    Erasing is applied for each sample and for each `n` with the given probability, the randomly</span>
<span class="sd">    selected area ratio and aspect ratio if `share` is `True`;</span>
<span class="sd">    otherwise (`share`=`False`), for each feature additionally.</span>
<span class="sd">    </span>
<span class="sd">    Random patch are selected by random coordinates as the following,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">      S_e &amp;&amp;= Uniform(s_l, s_h) \times S \\</span>
<span class="sd">      r_e &amp;&amp;= Uniform(r_l, r_h) \\</span>
<span class="sd">      H_e &amp;&amp;= \sqrt{S_e \times r_e} \\</span>
<span class="sd">      W_e &amp;&amp;= \sqrt{S_e / r_e} \\</span>
<span class="sd">      y_e &amp;&amp;= Uniform(0, H - H_e) \\</span>
<span class="sd">      x_e &amp;&amp;= Uniform(0, W - W_e),</span>
<span class="sd">    </span>
<span class="sd">    where :math:`S` is the area, :math:`s_l` and :math:`s_h` are the low and high values of</span>
<span class="sd">    the area ratio range, :math:`r_l` and :math:`r_h` are the low and high values</span>
<span class="sd">    of the aspect ratio range, :math:`H_e` and :math:`W_e` are height and width of a patch,</span>
<span class="sd">    and :math:`y_e` and :math:`x_e` are the start coordinates of a patch. If a pixel of the inputs</span>
<span class="sd">    falls in this patch, the value of that pixel is replaced with a random value in `replacements`</span>
<span class="sd">    range.</span>
<span class="sd">    </span>
<span class="sd">    Backward is implemented as passing gradients if `ste_fine_grained` is False; otherwise,</span>
<span class="sd">    the backward only occurs in regions not erased.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">      * `Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, Yi Yang,</span>
<span class="sd">        Random Erasing Data Augmentation,</span>
<span class="sd">        &lt;https://arxiv.org/abs/1708.04896&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        prob(float): Probability to erase.</span>
<span class="sd">            [default= `0.5` ]</span>
<span class="sd">        area_ratios(repeated float): Low and high of the area ratio range.</span>
<span class="sd">            [default= `(0.02, 0.4)` ]</span>
<span class="sd">        aspect_ratios(repeated float): Low and high of the aspect ratios range.</span>
<span class="sd">            [default= `(0.3, 3.3333)` ]</span>
<span class="sd">        replacements(repeated float): Low and high of the replacement value range.</span>
<span class="sd">            [default= `(0.0, 255.0)` ]</span>
<span class="sd">        n(int): Max number of patches to be erased.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        share(bool): Use a same bounding box randomly picked over the feature dimension when being True. Default is False.</span>
<span class="sd">            [default= `True` ]</span>
<span class="sd">        inplace(bool): The output array is shared with the input array if True.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        base_axis(int): Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        seed(int): Random seed. When -1, seed is sampled from global random number generator.</span>
<span class="sd">            [default= `-1` ]</span>
<span class="sd">        channel_last(bool): If True, the last dimension is considered as channel dimension, a.k.a NHWC order.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        ste_fine_grained(bool): Straight Through Estimator is fine-grained or not. Default is True.</span>
<span class="sd">            [default= `True` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">n</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">n</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">RandomErase</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">area_ratios</span><span class="p">,</span> <span class="n">aspect_ratios</span><span class="p">,</span> <span class="n">replacements</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">share</span><span class="p">,</span> <span class="n">inplace</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">channel_last</span><span class="p">,</span> <span class="n">ste_fine_grained</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="image_augmentation"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.image_augmentation">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">image_augmentation</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">min_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">max_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">angle</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">aspect_ratio</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">distortion</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">flip_lr</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">flip_ud</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">brightness</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">brightness_each</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">contrast</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">contrast_center</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">contrast_each</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ImageAugmentation randomly alters the input image.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        shape(:obj:`tuple` of :obj:`int`): The output image data size.</span>
<span class="sd">            [default= `x.shape` ]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): Border padding values for each spatial axis. Padding will be added both sides of the dimension.</span>
<span class="sd">            [default= `(0, 0)` ]</span>
<span class="sd">        min_scale(float): The minimum scale ratio when randomly scaling the image. For example, to scale down to 0.8 times the size of the original image, specify &quot;0.8&quot;. To not apply random scaling, set both min_scale and max_scale to &quot;1.0&quot;.</span>
<span class="sd">            [default= `1.0` ]</span>
<span class="sd">        max_scale(float): The maximum scale ratio when randomly scaling the image. For example, to scale down to 2 times the size of the original image, specify &quot;2.0&quot;.</span>
<span class="sd">            [default= `1.0` ]</span>
<span class="sd">        angle(float): The rotation angle range in radians when randomly rotating the image. The image is randomly rotated in the -Angle to +Angle range. For example, to rotate in a +-15 degree range, specify &quot;0.26&quot; (15 degrees/360 degrees * 2PI). To not apply random rotation, specify &quot;0.0&quot;.</span>
<span class="sd">            [default= `0.0` ]</span>
<span class="sd">        aspect_ratio(float): The aspect ratio range when randomly deforming the image. For example, to deform aspect ratio of image from 1:1.3 to 1.3:1, specify &quot;1.3&quot;. To not apply random deforming, specify &quot;1.0&quot;.</span>
<span class="sd">            [default= `1.0` ]</span>
<span class="sd">        distortion(float): The distortion range when randomly distorting the image. To not apply distortion, specify &quot;0.0&quot;.</span>
<span class="sd">            [default= `0.0` ]</span>
<span class="sd">        flip_lr(bool): Whether to randomly flip the image horizontally at 50% probability.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        flip_ud(bool): Whether to randomly flip the image vertically at 50% probability.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        brightness(float): The absolute range of values to randomly add to the brightness. A random value in the -Brightness to +Brightness range is added to the brightness. For example, to vary the brightness in the -0.05 to +0.05 range, specify &quot;0.05&quot;. To not apply random addition to brightness, specify &quot;0.0&quot;.</span>
<span class="sd">            [default= `0.0` ]</span>
<span class="sd">        brightness_each(bool): Whether to apply the random addition to brightness (as specified by brightness) to each color channel. True: brightness is added based on a different random number for each channel. False: brightness is added based on a random number common to all channels.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        contrast(float): The range in which to randomly vary the image contrast. The contrast is varied in the 1/Contrast times to Contrast times range. The output brightness is equal to (input - contrast_center) * contrast + contrast_center. For example, to vary the contrast in the 0.91 times to 1.1 times range, specify &quot;1.1&quot;. To not apply random contrast variation, specify &quot;1.0&quot;.</span>
<span class="sd">            [default= `1.0` ]</span>
<span class="sd">        contrast_center(float): Intensity center used for applying contrast.</span>
<span class="sd">            [default= `0.0` ]</span>
<span class="sd">        contrast_each(bool): Whether to apply the random contrast variation (as specified by contrast) to each color channel. True: contrast is varied based on a different random number for each channel. False: contrast is varied based on a random number common to all channels.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        noise(float): Sigma of normal random number to be added.</span>
<span class="sd">            [default= `0.0` ]</span>
<span class="sd">        seed(int): Random seed. When -1, seed is sampled from global random number generator.</span>
<span class="sd">            [default= `-1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ImageAugmentation</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">min_scale</span><span class="p">,</span> <span class="n">max_scale</span><span class="p">,</span> <span class="n">angle</span><span class="p">,</span> <span class="n">aspect_ratio</span><span class="p">,</span> <span class="n">distortion</span><span class="p">,</span> <span class="n">flip_lr</span><span class="p">,</span> <span class="n">flip_ud</span><span class="p">,</span> <span class="n">brightness</span><span class="p">,</span> <span class="n">brightness_each</span><span class="p">,</span> <span class="n">contrast</span><span class="p">,</span> <span class="n">contrast_center</span><span class="p">,</span> <span class="n">contrast_each</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">seed</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="sigmoid_cross_entropy"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.sigmoid_cross_entropy">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">sigmoid_cross_entropy</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise cross entropy between `x` and the target variables, passed to a sigmoid function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = - \left(x^{(1)}_i \ln \left(\sigma \left(x^{(0)}_i \right)\right) + \</span>
<span class="sd">        \left(1 - x^{(1)}_i\right) \ln \left(1 - \sigma \left(x^{(0)}_i \</span>
<span class="sd">        \right)\right)\right)</span>
<span class="sd">    </span>
<span class="sd">    where :math:`\sigma(s)=\frac{1}{1+\exp(-s)}`.</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        SigmoidCrossEntropy is equivalent to Sigmoid+BinaryCrossEntropy, but computing them at once has the effect of reducing computational error.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array. Typically indicates a score. The value lies in :math:`[-\infty, \infty]`</span>
<span class="sd">            [parameter]</span>
<span class="sd">        target(~nnabla.Variable): N-D array of labels. Only 0 or 1 value is allowed.</span>
<span class="sd">            [parameter]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array of element-wise losses.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">SigmoidCrossEntropy</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="binary_cross_entropy"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.binary_cross_entropy">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise cross entropy between `x` and the target variables.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = - \left(x^{(1)}_i * \ln \left(x^{(0)}_i\right) + \left(1 - \</span>
<span class="sd">        x^{(1)}_i\right) * \ln \left(1 - x^{(0)}_i\right)\right).</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Probabilities N-D array. :math:`-\infty` to :math:`\infty`.</span>
<span class="sd">        target(~nnabla.Variable): N-D array of labels. Usually set as 0 or 1, but, unlike SigmoidCrossEntropy, it allows probability (0 to 1) as inputs and backpropagation can be done.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array of element-wise losses.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BinaryCrossEntropy</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="softmax_cross_entropy"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.softmax_cross_entropy">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">softmax_cross_entropy</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise cross entropy between the variables and the variables of a label given by a category index with Softmax normalization.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_{j} = -\ln \left(\frac{\exp(x_{j,t_j})}{\sum_{i&#39;} \exp(x_{j,i&#39;})}\right)</span>
<span class="sd">    </span>
<span class="sd">    along dimension specified by axis (:math:`i` is the axis where normalization is performed on).</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        SoftmaxCrossEntropy is equivalent to Softmax+CategoricalCrossEntropy, but computing them at once has the effect of reducing computational error.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array. Typically indicates a score. :math:`(D_1 \times ... \times D_i \times ... \times D_N)`</span>
<span class="sd">            [parameter]</span>
<span class="sd">        target(~nnabla.Variable): N-D array of labels. :math:`(D_1 \times ... \times 1 \times ... \times D_N)`</span>
<span class="sd">            [parameter]</span>
<span class="sd">        axis(int): Axis normalization is taken.</span>
<span class="sd">            [default= `len(x.shape) - 1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array of element-wise losses. :math:`(D_1 \times ... \times 1 \times ... \times D_N)`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">SoftmaxCrossEntropy</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="categorical_cross_entropy"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.categorical_cross_entropy">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">categorical_cross_entropy</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise cross entropy between `x` and the target `t` where targets are given by a category index.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_{j} = -\ln \left( x_{j, t_j} \right)</span>
<span class="sd">    </span>
<span class="sd">    along dimension specified by axis (:math:`i` is the axis where normalization is performed on).</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array. Typically indicates a score. :math:`(D_1 \times ... \times D_i \times ... \times D_N)`</span>
<span class="sd">            [parameter]</span>
<span class="sd">        target(~nnabla.Variable): N-D array of labels. :math:`(D_1 \times ... \times 1 \times ... \times D_N)`</span>
<span class="sd">            [parameter]</span>
<span class="sd">        axis(int): Axis normalization is taken.</span>
<span class="sd">            [default= `len(x.shape) - 1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array of element-wise losses. :math:`(D_1 \times ... \times 1 \times ... \times D_N)`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">CategoricalCrossEntropy</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="squared_error"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.squared_error">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">squared_error</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise squared error</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \left(x^{(0)}_i - x^{(1)}_i\right)^2.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array.</span>
<span class="sd">        x1(~nnabla.Variable): N-D array.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">SquaredError</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="absolute_error"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.absolute_error">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">absolute_error</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise absolute error</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = | x^{(0)}_i - x^{(1)}_i |.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array.</span>
<span class="sd">        x1(~nnabla.Variable): N-D array.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">AbsoluteError</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="huber_loss"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.huber_loss">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">huber_loss</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise Huber loss</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i= \left\{</span>
<span class="sd">        \begin{array}{ll}</span>
<span class="sd">          d^2 &amp; (|d| &lt; \delta)\\</span>
<span class="sd">          \delta (2 |d| - \delta) &amp; ({\rm otherwise})</span>
<span class="sd">        \end{array} \right.</span>
<span class="sd">    </span>
<span class="sd">    where :math:`d = x^{(0)}_i - x^{(1)}_i`</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array.</span>
<span class="sd">        x1(~nnabla.Variable): N-D array.</span>
<span class="sd">        delta(float): Delta</span>
<span class="sd">            [default= `1.0` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array of element-wise losses.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">HuberLoss</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">delta</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="epsilon_insensitive_loss"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.epsilon_insensitive_loss">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">epsilon_insensitive_loss</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise Epsilon Insensitive Loss</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i= \left\{</span>
<span class="sd">        \begin{array}{ll}</span>
<span class="sd">          | x^{(0)}_i - x^{(1)}_i | - \epsilon &amp; if \ \ | x^{(0)}_i - x^{(1)}_i | &gt; \epsilon \\</span>
<span class="sd">    			0 &amp; otherwise</span>
<span class="sd">        \end{array} \right.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array.</span>
<span class="sd">        x1(~nnabla.Variable): N-D array.</span>
<span class="sd">        epsilon(float): Insensitive parameter.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array of element-wise losses.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">EpsilonInsensitiveLoss</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="kl_multinomial"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.kl_multinomial">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">kl_multinomial</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The Kullback Leibler Divergence for multinomial distributions.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        D = \sum_i p_i \log \left( \frac{p_i}{q_i} \right)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        p(~nnabla.Variable): N-D array of the source categorical probabilities</span>
<span class="sd">        q(~nnabla.Variable): N-D array of the target categorical probabilities</span>
<span class="sd">        base_axis(int): Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">            [default= `1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Kullback Leibler divergence :math:`KL(p \parallel q)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">KLMultinomial</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="affine_grid"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.affine_grid">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">affine_grid</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Generate the source grid based on the normalized target grid with `size`.</span>
<span class="sd">    The target grid is first normalized in [-1, 1], then</span>
<span class="sd">    tranformed by the affine transformation :math:`\theta` to generate</span>
<span class="sd">    the source grid. 2D and 3D grid are supported now.</span>
<span class="sd">    </span>
<span class="sd">    This function is normally used with the `warp_by_grid` function for</span>
<span class="sd">    constructing the spatial transformer.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        theta(~nnabla.Variable): N-D array with the shape (:math:`B \times 2 \times 3`), the sample-wise affine transformation matrix.</span>
<span class="sd">        size(repeated int64): The grid size of (:math:`H \times W`) for 2D and (:math:`D \times H \times W`) for 3D.</span>
<span class="sd">        align_corners(bool): If `True`, the top-left and bottom-right pixels correspond to (-1, -1) and (1, 1) respectively since a pixel is located on the corner of a grid, and the target grid is normalized in [-1, 1].</span>
<span class="sd">            If `False`, the normalized target grid in [-1, 1] is scaled by `size - 1 / size` according to the respective spatial size (e.g., :math:`H` and :math:`W`) before the transformation since a pixel is located on a center of a cell in a grid.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the shape (:math:`B \times H \times W \times 2`) for 2D and (:math:`B \times D \times H \times W \times 3`) for 3D. The last dimension of 2 is for (x, y) and of 3 for (x, y, z). The `gird` is used as the source grid for the warping.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">AffineGrid</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="warp_by_grid"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.warp_by_grid">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">warp_by_grid</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zero&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">channel_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Warp the input data by the grid.</span>
<span class="sd">    This function is normally used with the generated normalized grid by</span>
<span class="sd">    the `affine_grid` function for constructing the spatial transformer.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input data to be warped with the shape (:math:`B \times C \times H_{in} \times W_{in}`) for 2D and (:math:`B \times C \times D_{in} \times H_{in} \times W_{in}`) for 3D.</span>
<span class="sd">        grid(~nnabla.Variable): Grid warping the input data with the shape (:math:`B \times H_{out} \times W_{out} \times 2`) for 2D and (:math:`B \times D_{out} \times H_{out} \times W_{out} \times 3`) for 3D. The last dimension of 2 is for (x, y) or 3 for (x, y, z).</span>
<span class="sd">        mode(string): Interpolation mode, linear or nearest.</span>
<span class="sd">            [default= `&#39;linear&#39;` ]</span>
<span class="sd">        padding_mode(string): Padding mode when the grid value is outside [-1, 1]. If this is &quot;zero&quot;, 0 is used for padding. &quot;reflect&quot; uses the values reflected at the ends of the original input data like the mirror. &quot;repeat&quot; used the values at the ends of the original input data.</span>
<span class="sd">            [default= `&#39;zero&#39;` ]</span>
<span class="sd">        align_corners(bool): The target grid normalized in [-1, 1] is scaled by `size - 1 / size` according to the respective spatial size (e.g., :math:`H` and :math:`W`) before the transformation if this is `False`. If this is `True`, the top-left and bottom-right pixels correspond to (-1, -1) and (1, 1) respectively.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        channel_last(bool): If True, the last dimension is considered as channel dimension, a.k.a NHWC order.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output data warped by the grid.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">WarpByGrid</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">,</span> <span class="n">channel_last</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="warp_by_flow"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.warp_by_flow">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">warp_by_flow</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">flow</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transform the image(s) *data* by *flow* field(s) of offset vectors such</span>
<span class="sd">    that each output pixel corresponds to the input image pixel at the</span>
<span class="sd">    relative offset location given by horizontal and vertical flow values</span>
<span class="sd">    (in other words, the flow field describes the coordinate displacements</span>
<span class="sd">    for each output pixel to the corresponding input pixel). Both *data* and</span>
<span class="sd">    *flow* are 4-D variables (in &quot;NCHW&quot; layout) with identical shape except</span>
<span class="sd">    the *flow* channel dimension (which is always 2).</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        output_{n,c,y,x} = data_{n,c,y&#39;,x&#39;},</span>
<span class="sd">    </span>
<span class="sd">    where</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y&#39; &amp;=&amp; y + flow_{n,1,y,x}, \\</span>
<span class="sd">        x&#39; &amp;=&amp; x + flow_{n,0,y,x}.</span>
<span class="sd">    </span>
<span class="sd">    The output pixel values at :math:`y&#39;` and :math:`x&#39;` locations are</span>
<span class="sd">    obtained by bilinear interpolating between the 4 closest pixels of the</span>
<span class="sd">    input image. Pixel values outside of the input image are implicitly</span>
<span class="sd">    padded with the value of the closest boundary pixel.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        data(~nnabla.Variable): Input image data with shape `(N, Channels, Height, Width)`.</span>
<span class="sd">        flow(~nnabla.Variable): Flow field vectors with shape `(N, 2, Height, Width)`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Transformed image data with shape `(N, Channels, Height, Width)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">WarpByFlow</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">data</span><span class="p">,</span> <span class="n">flow</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="binary_sigmoid"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.binary_sigmoid">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">binary_sigmoid</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise binary sigmoid function. In the forward pass, it computes</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x) = \begin{cases}</span>
<span class="sd">            1 &amp; (x &gt; 0) \\</span>
<span class="sd">            0 &amp; ({\rm otherwise})\end{cases},</span>
<span class="sd">    </span>
<span class="sd">    but in the backward pass, a straight-through approximation of the gradient</span>
<span class="sd">    is used, i.e.,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \frac{\partial f(x)}{\partial x} =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            0 &amp; (|x| \geq 1) \\</span>
<span class="sd">            \frac{1}{2} &amp; ({\rm otherwise})</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">        * `Courbariaux, Matthieu, and Yoshua Bengio. Binarynet: Training deep</span>
<span class="sd">          neural networks with weights and activations constrained to+ 1 or-1.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1602.02830&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input .</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BinarySigmoid</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="binary_tanh"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.binary_tanh">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">binary_tanh</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise binary tanh function. In the forward pass, it computes</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x) = \begin{cases}</span>
<span class="sd">            1 &amp; (x &gt; 0) \\</span>
<span class="sd">            -1 &amp; ({\rm otherwise})</span>
<span class="sd">        \end{cases},</span>
<span class="sd">    </span>
<span class="sd">    but in the backward pass, a straight-through approximation of the gradient</span>
<span class="sd">    is used, i.e.,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \frac{\partial f(x)}{\partial x} =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            0 &amp; (|x| \geq 1) \\</span>
<span class="sd">            1 &amp; ({\rm otherwise}) \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">        * `Courbariaux, Matthieu, and Yoshua Bengio. Binarynet: Training deep</span>
<span class="sd">          neural networks with weights and activations constrained to+ 1 or-1.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1602.02830&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input .</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BinaryTanh</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="binary_connect_affine"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.binary_connect_affine">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">binary_connect_affine</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">binary_weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">quantize_zero_to</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function provides a BinaryConnect affine layer. It computes in</span>
<span class="sd">    the forward pass</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        y_j = \sum_{i} sign(w_{j,i}) x_i,</span>
<span class="sd">    </span>
<span class="sd">    i.e., the weights :math:`w_{j,i}` are binarized to :math:`sign(w_{j,i})` and,</span>
<span class="sd">    hence, each weight is in :math:`\{-1,\,1\}`. By this weight binarization, the</span>
<span class="sd">    inner product computations do not require any multiplications anymore as</span>
<span class="sd">    they turn into additions/subtractions.</span>
<span class="sd">    </span>
<span class="sd">    This function should be used together with</span>
<span class="sd">    :meth:`~nnabla.functions.batch_normalization`.</span>
<span class="sd">    </span>
<span class="sd">    .. note::</span>
<span class="sd">    </span>
<span class="sd">        1) If you would like to share the binary weights between other</span>
<span class="sd">        layers, please use the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the binary weights (`binary_weight`).</span>
<span class="sd">    </span>
<span class="sd">        2) The weights and the binary weights become in sync only after a call to</span>
<span class="sd">        :meth:`~nnabla.Variable.forward`, and not after a call to</span>
<span class="sd">        :meth:`~nnabla.Variable.backward`. If you wish to store the parameters of</span>
<span class="sd">        the network, remember to call :meth:`~nnabla.Variable.forward`, once before</span>
<span class="sd">        doing so, otherwise the weights and the binary weights will not be in sync.</span>
<span class="sd">    </span>
<span class="sd">        3) CPU and GPU implementations now use floating values for `binary_weight`,</span>
<span class="sd">        since this function is for simulation purposes.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">        * `M. Courbariaux, Y. Bengio, and J.-P. David. BinaryConnect:</span>
<span class="sd">          Training Deep Neural Networks with binary weights during propagations.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1511.00363&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input .</span>
<span class="sd">        weight(~nnabla.Variable): Weight .</span>
<span class="sd">            [parameter]</span>
<span class="sd">        binary_weight(~nnabla.Variable): Binarized weight .</span>
<span class="sd">            [parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Bias.</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        base_axis(int): Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        quantize_zero_to(float): Input value at zero is quantized to this value.</span>
<span class="sd">            [default= `1.0` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">binary_weight</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BinaryConnectAffine</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">quantize_zero_to</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="binary_connect_convolution"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.binary_connect_convolution">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">binary_connect_convolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">binary_weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">quantize_zero_to</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function provides a BinaryConnect convolution layer. It computes in</span>
<span class="sd">    the forward pass</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        y_{n, a, b} = \sum_{m} \sum_{i} \sum_{j} sign(w_{n, m, i, j}) x_{m, a + i, b + j},</span>
<span class="sd">    </span>
<span class="sd">    i.e., the weights :math:`w_{n, m, i, j}` are binarized to</span>
<span class="sd">    :math:`sign(w_{n, m, i, j})` and, hence,</span>
<span class="sd">    each weight is in :math:`\{-1,\,1\}`. By this weight binarization, the</span>
<span class="sd">    inner product computations do not require any multiplications anymore as</span>
<span class="sd">    they turn into additions/subtractions.</span>
<span class="sd">    </span>
<span class="sd">    This function should be used together with :meth:`~nnabla.functions.batch_normalization`.</span>
<span class="sd">    </span>
<span class="sd">    Reference</span>
<span class="sd">    </span>
<span class="sd">        * `M. Courbariaux, Y. Bengio, and J.-P. David. BinaryConnect:</span>
<span class="sd">          Training Deep Neural Networks with binary weights during propagations.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1511.00363&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    .. note::</span>
<span class="sd">    </span>
<span class="sd">        1) If you would like to share the binary weights between other</span>
<span class="sd">        layers, please use the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the binary weights (`binary_weight`).</span>
<span class="sd">    </span>
<span class="sd">        2) The weights and the binary weights become in sync only after a call to</span>
<span class="sd">        :meth:`~nnabla.Variable.forward`, and not after a call to</span>
<span class="sd">        :meth:`~nnabla.Variable.backward`. If you wish to store the parameters of</span>
<span class="sd">        the network, remember to call :meth:`~nnabla.Variable.forward`, once before</span>
<span class="sd">        doing so, otherwise the weights and the binary weights will not be in sync.</span>
<span class="sd">    </span>
<span class="sd">        3) CPU and GPU implementations now use floating values for `binary_weight`,</span>
<span class="sd">        since this function is for simulation purposes.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input.</span>
<span class="sd">        weight(~nnabla.Variable): Weight.</span>
<span class="sd">            [parameter]</span>
<span class="sd">        binary_weight(~nnabla.Variable): Binarized weight.</span>
<span class="sd">            [parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Bias.</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        base_axis(int): Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">            [default= `(0,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        stride(:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">            [default= `(1,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        dilation(:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">            [default= `(1,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        group(int): Number of groups of channels. This makes the connection across channels sparser, by grouping connections along the mapping direction.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        quantize_zero_to(float): Input value at zero is quantized to this value.</span>
<span class="sd">            [default= `1.0` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">pad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">dilation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">binary_weight</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BinaryConnectConvolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">quantize_zero_to</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="binary_weight_affine"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.binary_weight_affine">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">binary_weight_affine</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">binary_weight</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">quantize_zero_to</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function provides a Binary Weight Network affine layer. It computes in</span>
<span class="sd">    the forward pass</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        y_j = \frac{1}{\|\mathbf{w}_j\|_{\ell_1}} \sum_{i} sign(w_{j,i}) x_i</span>
<span class="sd">    </span>
<span class="sd">    i.e., the weights :math:`w_{j,i}` are binarized to :math:`sign(w_{j,i})` and,</span>
<span class="sd">    hence, each weight is in :math:`\{-1,\,1\}`. By this weight binarization, the</span>
<span class="sd">    inner product computations turn into additions/subtractions which are followed</span>
<span class="sd">    by multiplication with the scaling factor</span>
<span class="sd">    :math:`\alpha_j = \frac{1}{\|\mathbf{w}_j\|_{\ell_1}}`.</span>
<span class="sd">    </span>
<span class="sd">    Reference</span>
<span class="sd">    </span>
<span class="sd">        * `Rastegari, Mohammad, et al. XNOR-Net: ImageNet Classification Using</span>
<span class="sd">          Binary Convolutional Neural Networks.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1603.05279&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    .. note::</span>
<span class="sd">    </span>
<span class="sd">        1) If you would like to share the binary weights with other layers, please</span>
<span class="sd">        use the standard, floating value weights (`weight`) and not the binary</span>
<span class="sd">        weights (`binary_weight`).</span>
<span class="sd">    </span>
<span class="sd">        2) The weights and the binary weights become in sync only after a call to</span>
<span class="sd">        :meth:`~nnabla.Variable.forward`, and not after a call to</span>
<span class="sd">        :meth:`~nnabla.Variable.backward`. If you wish to store the parameters of</span>
<span class="sd">        the network, remember to call :meth:`~nnabla.Variable.forward`, once before</span>
<span class="sd">        doing so, otherwise the weights and the binary weights will not be in sync.</span>
<span class="sd">    </span>
<span class="sd">        3) CPU and GPU implementations now use floating values for `binary_weight`,</span>
<span class="sd">        since this function is for simulation purposes.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input .</span>
<span class="sd">        weight(~nnabla.Variable): Weight.</span>
<span class="sd">            [parameter]</span>
<span class="sd">        binary_weight(~nnabla.Variable): Binarized weight.</span>
<span class="sd">            [parameter]</span>
<span class="sd">        alpha(~nnabla.Variable): Alpha.</span>
<span class="sd">            [parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Bias.</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        base_axis(int): Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        quantize_zero_to(float): Input value at zero is quantized to this value.</span>
<span class="sd">            [default= `1.0` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">binary_weight</span><span class="p">,</span> <span class="n">alpha</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BinaryWeightAffine</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">quantize_zero_to</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="binary_weight_convolution"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.binary_weight_convolution">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">binary_weight_convolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">binary_weight</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">quantize_zero_to</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function provides a Binary Weight Network convolution layer. It computes in</span>
<span class="sd">    the forward pass</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        y_{n, a, b} = \frac{1}{\|\mathbf{w}_n\|_{\ell_1}} \sum_{m} \sum_{i} \sum_{j} sign(w_{n, m, i, j}) x_{m, a + i, b + j}.</span>
<span class="sd">    </span>
<span class="sd">    i.e., the weights :math:`w_{n, m, i, j}` are binarized to</span>
<span class="sd">    :math:`sign(w_{n, m, i, j})` and, hence, each weight is in :math:`\{-1,\,1\}`.</span>
<span class="sd">    By this weight binarization, the inner product computations turn into</span>
<span class="sd">    additions/subtractions which are followed by multiplication with the scaling</span>
<span class="sd">    factor :math:`\alpha_n = \frac{1}{\|\mathbf{w}_n\|_{\ell_1}}`.</span>
<span class="sd">    </span>
<span class="sd">    Reference</span>
<span class="sd">    </span>
<span class="sd">        * `Rastegari, Mohammad, et al. XNOR-Net: ImageNet Classification Using</span>
<span class="sd">          Binary Convolutional Neural Networks.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1603.05279&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    .. note::</span>
<span class="sd">    </span>
<span class="sd">        1) If you would like to share the binary weights between other standard layers, please</span>
<span class="sd">        use the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the binary weights (`binary_weight`).</span>
<span class="sd">    </span>
<span class="sd">        2) The weights and the binary weights become in sync only after a call to</span>
<span class="sd">        :meth:`~nnabla.Variable.forward`, and not after a call to</span>
<span class="sd">        :meth:`~nnabla.Variable.backward`. If you wish to store the parameters of</span>
<span class="sd">        the network, remember to call :meth:`~nnabla.Variable.forward`, once</span>
<span class="sd">        before doing so, otherwise the weights and the binary weights will not be</span>
<span class="sd">        in sync.</span>
<span class="sd">    </span>
<span class="sd">        3) CPU and GPU implementations now use floating values for `binary_weight`,</span>
<span class="sd">        since this function is for simulation purposes.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input.</span>
<span class="sd">        weight(~nnabla.Variable): Weight.</span>
<span class="sd">            [parameter]</span>
<span class="sd">        binary_weight(~nnabla.Variable): Binarized weight.</span>
<span class="sd">            [parameter]</span>
<span class="sd">        alpha(~nnabla.Variable): Alpha.</span>
<span class="sd">            [parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Bias.</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        base_axis(int): Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">            [default= `(0,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        stride(:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">            [default= `(1,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        dilation(:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">            [default= `(1,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        group(int): Number of groups of channels. This makes the connection across channels sparser, by grouping connections along the mapping direction.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        quantize_zero_to(float): Input value at zero is quantized to this value.</span>
<span class="sd">            [default= `1.0` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">pad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">dilation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">binary_weight</span><span class="p">,</span> <span class="n">alpha</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BinaryWeightConvolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">quantize_zero_to</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="inq_affine"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.inq_affine">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">inq_affine</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">indicator_fixedweights</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">inq_iterations</span><span class="o">=</span><span class="p">(),</span> <span class="n">selection_algorithm</span><span class="o">=</span><span class="s1">&#39;largest_abs&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function provides a INQ affine layer. It computes in</span>
<span class="sd">    the forward pass</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        y_j = \sum_{i} w_{j,i} x_i,</span>
<span class="sd">    </span>
<span class="sd">    where the weights :math:`w_{j,i}` are quantized sequentially during</span>
<span class="sd">    training to power-of-two numbers. In the backward pass, only the non-fixed</span>
<span class="sd">    (i.e., learnable) weights are updated.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">        * `Zhou A, Yao A, Guo Y, Xu L, Chen Y. Incremental network quantization:</span>
<span class="sd">          Towards lossless CNNs with low-precision weights.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1702.03044&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input .</span>
<span class="sd">        weight(~nnabla.Variable): Weight .</span>
<span class="sd">            [parameter]</span>
<span class="sd">        indicator_fixedweights(~nnabla.Variable): Indicates which weights are already fixed (0 = not fixed, 1 = fixed) .</span>
<span class="sd">            [parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Bias.</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        base_axis(int): Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        num_bits(int): Number of bits per weight. Needs to be &gt;= 2 as two bits are used to code `zero` and sign of weight.</span>
<span class="sd">            [default= `4` ]</span>
<span class="sd">        inq_iterations(repeated int64): List which specifies after how many forward passes we fix 50% of the learnable weights. If we have done as many iterations as specified in the last element of `inq_iterations`, then all weights are fixed.</span>
<span class="sd">            [default= `()` ]</span>
<span class="sd">        selection_algorithm(string): Chooses algorithm that we use for selecting the weights to fix (&quot;largest_abs&quot; ... fix weights with largest absolute value, &quot;random&quot; ... fix weights randomly)</span>
<span class="sd">            [default= `&#39;largest_abs&#39;` ]</span>
<span class="sd">        seed(int): Random seed. When -1, seed is sampled from global random number generator.</span>
<span class="sd">            [default= `-1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">indicator_fixedweights</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">INQAffine</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">num_bits</span><span class="p">,</span> <span class="n">inq_iterations</span><span class="p">,</span> <span class="n">selection_algorithm</span><span class="p">,</span> <span class="n">seed</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="inq_convolution"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.inq_convolution">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">inq_convolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">indicator_fixedweights</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">inq_iterations</span><span class="o">=</span><span class="p">(),</span> <span class="n">selection_algorithm</span><span class="o">=</span><span class="s1">&#39;largest_abs&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function provides a INQ convolution layer. It computes in</span>
<span class="sd">    the forward pass</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        y_{n, a, b} = \sum_{m} \sum_{i} \sum_{j} w_{n, m, i, j} x_{m, a + i, b + j},</span>
<span class="sd">    </span>
<span class="sd">    where the weights :math:`w_{j,i}` are quantized sequentially during</span>
<span class="sd">    training to power-of-two numbers. In the backward pass, only the non-fixed</span>
<span class="sd">    (i.e., learnable) weights are updated.</span>
<span class="sd">    </span>
<span class="sd">    Reference</span>
<span class="sd">    </span>
<span class="sd">        * `Zhou A, Yao A, Guo Y, Xu L, Chen Y. Incremental network quantization:</span>
<span class="sd">          Towards lossless CNNs with low-precision weights.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1702.03044&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input.</span>
<span class="sd">        weight(~nnabla.Variable): Weight.</span>
<span class="sd">            [parameter]</span>
<span class="sd">        indicator_fixedweights(~nnabla.Variable): Indicates which weights are already fixed (0 = not fixed, 1 = fixed) .</span>
<span class="sd">            [parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Bias.</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        base_axis(int): Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">            [default= `(0,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        stride(:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">            [default= `(1,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        dilation(:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">            [default= `(1,) * (len(x.shape) - (base_axis+1))` ]</span>
<span class="sd">        group(int): Number of groups of channels. This makes the connection across channels sparser, by grouping connections along the mapping direction.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        num_bits(int): Number of bits per weight. Needs to be &gt;= 2 as two bits are used to code `zero` and sign of weight.</span>
<span class="sd">            [default= `4` ]</span>
<span class="sd">        inq_iterations(repeated int64): List which specifies after how many forward passes we fix 50% of the learnable weights. If we have done as many iterations as specified in the last element of `inq_iterations`, then all weights are fixed.</span>
<span class="sd">            [default= `()` ]</span>
<span class="sd">        selection_algorithm(string): Chooses algorithm that we use for selecting the weights to fix (&quot;largest_abs&quot; ... fix weights with largest absolute value, &quot;random&quot; ... fix weights randomly)</span>
<span class="sd">            [default= `&#39;largest_abs&#39;` ]</span>
<span class="sd">        seed(int): Random seed. When -1, seed is sampled from global random number generator.</span>
<span class="sd">            [default= `-1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">pad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">dilation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">indicator_fixedweights</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">INQConvolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">num_bits</span><span class="p">,</span> <span class="n">inq_iterations</span><span class="p">,</span> <span class="n">selection_algorithm</span><span class="p">,</span> <span class="n">seed</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">fixed_point_quantize</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sign</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="mf">0.0625</span><span class="p">,</span> <span class="n">ste_fine_grained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;This function simulates to uniformly quantize values in fixed-point number representation.</span>
<span class="sd">    </span>
<span class="sd">    In the forward pass,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">       q_i= \left\{</span>
<span class="sd">    	   \begin{array}{ll}</span>
<span class="sd">    			max &amp; if \ \ \ x_i &gt; max \\</span>
<span class="sd">    		  sign(x_i) \times floor(|x_i| \delta^{-1} + 2^{-1}) \times \delta &amp; if \ \ min \le x_i \le max \\</span>
<span class="sd">    	  	min &amp; if \ \ x_i &lt; min \\</span>
<span class="sd">    	   \end{array} \right.,</span>
<span class="sd">    </span>
<span class="sd">    where :math:`\delta` is the step size,</span>
<span class="sd">    :math:`(min, max) :=(- (2^{n-1} - 1)\delta, (2^{n-1} - 1)\delta)` if :math:`sign` is true,</span>
<span class="sd">    :math:`(min, max) := (0, (2^n - 1) \delta)` otherwise, and</span>
<span class="sd">    :math:`n` is the total bit-width used.</span>
<span class="sd">    </span>
<span class="sd">    In the backward pass when using `ste_fine_grained` as false,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">       \frac{\partial q_i}{\partial x_i} = 1.</span>
<span class="sd">    </span>
<span class="sd">    In the backward pass when using `ste_fine_grained` as true,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">       \frac{\partial q_i}{\partial x_i}= \left\{</span>
<span class="sd">    	   \begin{array}{ll}</span>
<span class="sd">    			0 &amp; if \ \ \ x_i &gt; max \\</span>
<span class="sd">    		  1 &amp; if \ \ min \le x_i \le max \\</span>
<span class="sd">    	  	0 &amp; if \ \ x_i &lt; min \\</span>
<span class="sd">    	   \end{array} \right..</span>
<span class="sd">    </span>
<span class="sd">    .. note::</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    	Quantized values are stored as floating point number, since this function is for simulation purposes.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        sign(bool): Indicate the signed number or the unsigned number. Default is true.</span>
<span class="sd">            [default= `True` ]</span>
<span class="sd">        n(int): Bit width used. Note that `sign` consumes one bit. :math:`n-1` is used for number representation in `signed` case.</span>
<span class="sd">            [default= `8` ]</span>
<span class="sd">        delta(float): Step size.</span>
<span class="sd">            [default= `0.0625` ]</span>
<span class="sd">        ste_fine_grained(bool): Straight Through Estimator is fine-grained or not.</span>
<span class="sd">            [default= `True` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">FixedPointQuantize</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">sign</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">ste_fine_grained</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">min_max_quantize</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">qr_min</span><span class="p">,</span> <span class="n">qr_max</span><span class="p">,</span> <span class="n">ql_min</span><span class="p">,</span> <span class="n">ql_max</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">x_min_max</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ema</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ste_fine_grained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;This function simulates to uniformly quantize values in the range of min and max quantization levels.</span>
<span class="sd">    </span>
<span class="sd">    Min-max quantization is defined as the following equation</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        y = round \left(\frac{\min(\max(x, m), M) - m}{scale} \right) \times scale + m,</span>
<span class="sd">    </span>
<span class="sd">    where the :math:`scale` is defined as</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        scale = \frac{M - m}{M_q - m_q},</span>
<span class="sd">    </span>
<span class="sd">    and</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        m_q = ql_{min}, \\</span>
<span class="sd">        M_q = ql_{max}, \\</span>
<span class="sd">        m = qr_{min}, \\</span>
<span class="sd">        M = qr_{max}.</span>
<span class="sd">    </span>
<span class="sd">    In the backward pass when using `ste_fine_grained` as false,</span>
<span class="sd">    </span>
<span class="sd">        .. math::</span>
<span class="sd">    </span>
<span class="sd">          \frac{\partial q_i}{\partial x_i} = 1.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    In the backward pass when using `ste_fine_grained` as true,</span>
<span class="sd">    </span>
<span class="sd">        .. math::</span>
<span class="sd">    </span>
<span class="sd">           \frac{\partial q_i}{\partial x_i}= \left\{</span>
<span class="sd">         \begin{array}{ll}</span>
<span class="sd">           0 &amp; if \ \ \ x_i &gt; M \\</span>
<span class="sd">           1 &amp; if \ \ m \le x_i \le M \\</span>
<span class="sd">           0 &amp; if \ \ x_i &lt; m \\</span>
<span class="sd">         \end{array} \right..</span>
<span class="sd">    </span>
<span class="sd">    :math:`qr_{min}` and :math:`qr_{max}` are treaded as follows.</span>
<span class="sd">    </span>
<span class="sd">        * `x_min_max` is `True` and `ema` is `True`:</span>
<span class="sd">          Exponential moving average are computed for each :math:`min(x)` and :math:`max(x)`</span>
<span class="sd">          then stored in :math:`qr_{min}` and :math:`qr_{max}`.</span>
<span class="sd">        * `x_min_max` is `True` and `ema` is `False`:</span>
<span class="sd">          :math:`min(x)` and :math:`max(x)` are computed then stored in :math:`qr_{min}` and :math:`qr_{max}`.</span>
<span class="sd">        * `x_min_max` is `False` and `ema` is `True`:</span>
<span class="sd">          Exponential moving average stored in :math:`qr_{min}` and :math:`qr_{max}` are used.</span>
<span class="sd">        * `x_min_max` is `False` and `ema` is `False`</span>
<span class="sd">          Gradients of :math:`qr_{min}` and :math:`qr_{max}` are computed in the backward pass.</span>
<span class="sd">    </span>
<span class="sd">    More precisely, in inference of the min-max quantization, one has to consider *zero-point (zp)*</span>
<span class="sd">    which corresponds</span>
<span class="sd">    to the real value 0, and its data type is an integer. *zero-point* is defined as</span>
<span class="sd">    </span>
<span class="sd">        .. math::</span>
<span class="sd">    </span>
<span class="sd">           &amp;&amp; zp_f = ql_{min} -\frac{qr_{min}}{scale}, \\</span>
<span class="sd">           &amp;&amp; zp = \left\{</span>
<span class="sd">         \begin{array}{ll}</span>
<span class="sd">           ql_{max} &amp; if \ \ \ zp_f &gt;= ql_{max} \\</span>
<span class="sd">           round(zp_f) &amp; if \ \ otherwise \\</span>
<span class="sd">           ql_{min}  &amp; if \ \ zp_f &lt;= ql_{min} \\</span>
<span class="sd">         \end{array} \right..</span>
<span class="sd">    </span>
<span class="sd">    Accordingly, in order to simulate quantization effect of *zero-point*,</span>
<span class="sd">    during both forward and backward pass, :math:`qr_{min}` and :math:`qr_{max}` are adjusted as follows,</span>
<span class="sd">    </span>
<span class="sd">        .. math::</span>
<span class="sd">    </span>
<span class="sd">           qr_{min}^{adj} = ql_{min} - zp * scale, \\</span>
<span class="sd">           qr_{max}^{adj} = ql_{max} - zp * scale.</span>
<span class="sd">    </span>
<span class="sd">    These operations are often called *nudge*.</span>
<span class="sd">    </span>
<span class="sd">    Finally, in the formulas of the min-max quantization, :math:`m` and :math:`M` are replaced by</span>
<span class="sd">    :math:`qr_{min}^{adj}` and :math:`qr_{max}^{adj}` respectively.</span>
<span class="sd">    </span>
<span class="sd">    .. note::</span>
<span class="sd">    </span>
<span class="sd">    	Quantized values are stored as floating point number, since this function is for simulation purposes.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array innput.</span>
<span class="sd">        qr_min(~nnabla.Variable): Minimum value for the quantization range, modified during forward execution when x_min_max is True.</span>
<span class="sd">        qr_max(~nnabla.Variable): Maximum value for the quantization range, modified during forward execution when x_min_max is True.</span>
<span class="sd">        ql_min(~nnabla.Variable): Minimum value for the quantization level, typically 0.</span>
<span class="sd">        ql_max(~nnabla.Variable): Maximum value for the quantization level, typically 255.</span>
<span class="sd">        decay(float): Decay rate for the exponential moving average.</span>
<span class="sd">            [default= `0.999` ]</span>
<span class="sd">        x_min_max(bool): Use the min and max of x to compute quantization ranges.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        ema(bool): Use the exponential moving average for the min and max quantization ranges.</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        ste_fine_grained(bool): Straight Through Estimator is fine-grained or not.</span>
<span class="sd">            [default= `True` ]</span>
<span class="sd">        eps(float): Epsilon, or small value to ensure :math:`qr_{max} - qr_{min}` must be greater than the epsilon.</span>
<span class="sd">            [default= `0.01` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">MinMaxQuantize</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">decay</span><span class="p">,</span> <span class="n">x_min_max</span><span class="p">,</span> <span class="n">ema</span><span class="p">,</span> <span class="n">ste_fine_grained</span><span class="p">,</span> <span class="n">eps</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">qr_min</span><span class="p">,</span> <span class="n">qr_max</span><span class="p">,</span> <span class="n">ql_min</span><span class="p">,</span> <span class="n">ql_max</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">pow2_quantize</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sign</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">with_zero</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ste_fine_grained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function simulates to quantize values in the power of 2 number representation,</span>
<span class="sd">    in other words, it is linear (uniform) quantization in :math:`log_2` domain.</span>
<span class="sd">    </span>
<span class="sd">    In the forward pass of `signed` case,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">       q_i= \left\{</span>
<span class="sd">    	   \begin{array}{ll}</span>
<span class="sd">    			max_{+} &amp; if \ \ \overline{q_i} &gt; max_{+} \\</span>
<span class="sd">    			\overline{q_i} &amp; if \ \ min_{+} \le \overline{q_i} \le max_{+} \\</span>
<span class="sd">    		  min_{+} &amp; if \ \ 0 \le \overline{q_i} &lt; min_{+} \\</span>
<span class="sd">    		  min_{-} &amp; if \ \ min_{-} &lt; \overline{q_i} &lt; 0 \\</span>
<span class="sd">    		  \overline{q_i} &amp; if \ \ max_{-} \le \overline{q_i} \le min_{-}\\</span>
<span class="sd">    	  	max_{-} &amp; if \ \ \overline{q_i} &lt; max_{-} \\</span>
<span class="sd">    	   \end{array} \right.,</span>
<span class="sd">    </span>
<span class="sd">    where</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">       &amp;&amp; max_{+} = 2^{m}, min_{+} = 2^{m - (2^{n-1} - 1)},\\</span>
<span class="sd">       &amp;&amp; max_{-} = -2^{m}, min_{-} = -2^{m - (2^{n-1} - 1)},\\</span>
<span class="sd">       &amp;&amp; \overline{q_i} = sign(x_i) \times 2^{round(\log_2 |x_i|)}.</span>
<span class="sd">    </span>
<span class="sd">    This quantization uses the geometric mean between two power-of-two numbers</span>
<span class="sd">    as quantization threshold.</span>
<span class="sd">    </span>
<span class="sd">    In the forward pass of `unsigned` case,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">       q_i= \left\{</span>
<span class="sd">    	   \begin{array}{ll}</span>
<span class="sd">    			max &amp; if \ \ \overline{q_i} &gt; max \\</span>
<span class="sd">    			\overline{q_i} &amp; if \ \ min \le \overline{q_i} \le max \\</span>
<span class="sd">    		  min &amp; if \ \ 0 &lt; \overline{q_i} &lt; min \\</span>
<span class="sd">    	   \end{array} \right.,</span>
<span class="sd">    </span>
<span class="sd">    where</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">       &amp;&amp; max = 2^{m}, min = 2^{m - (2^{n} - 1)},\\</span>
<span class="sd">       &amp;&amp; \overline{q_i} = 2^{int(\log_2 |x_i|)}.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    When using `with_zero` as true, a pruning threshold is used to round an input to</span>
<span class="sd">    0 or :math:`min`. The pruning threshold is defined in this function as the following,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">       pruning\ threshold = min \times 2^{-\frac{1}{2}}.</span>
<span class="sd">    </span>
<span class="sd">    If an absolute value of the input is lesser than this value, the input is rounded to 0, otherwise :math:`min`.</span>
<span class="sd">    </span>
<span class="sd">    In the backward pass when using ste_fine_grained as false,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">       \frac{\partial q_i}{\partial x_i} = 1.</span>
<span class="sd">    </span>
<span class="sd">    In the backward pass when using ste_fine_grained as true,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">       \frac{\partial q_i}{\partial x_i}= \left\{</span>
<span class="sd">    	   \begin{array}{ll}</span>
<span class="sd">    			0 &amp; if \ \ \overline{q_i} &gt; max_{+} \\</span>
<span class="sd">    			1 &amp; if \ \ otherwise \\</span>
<span class="sd">    	  	0 &amp; if \ \ \overline{q_i} &lt; max_{-} \\</span>
<span class="sd">    	   \end{array} \right..</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    There are some literatures using pow2 quantization in their proposed methods.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">      * `Miyashita Daisuke, Lee H. Edward, Murmann Boris.</span>
<span class="sd">        Convolutional Neural Networks using Logarithmic Data Representation.</span>
<span class="sd">        &lt;https://arxiv.org/abs/1603.01025&gt;`_</span>
<span class="sd">    </span>
<span class="sd">      * `Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, Yurong Chen.</span>
<span class="sd">        Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights.</span>
<span class="sd">        &lt;https://arxiv.org/abs/1702.03044&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    .. note::</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    	Quantized values are stored as floating point number, since this function is for simulation purposes.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        sign(bool): Indicate the signed number or the unsigned number. Default is true.</span>
<span class="sd">            [default= `True` ]</span>
<span class="sd">        with_zero(bool): Indicate using zero as a quantized value. Default is true. Note that `zero` consumes one bit.</span>
<span class="sd">            [default= `True` ]</span>
<span class="sd">        n(int): Bit width used, Note that `sign` consumes one bit. :math:`n-1` is used for number representation in `signed` case. Default is 8.</span>
<span class="sd">            [default= `8` ]</span>
<span class="sd">        m(int): :math:`2^m` is the upper bound of the dynamic range and :math:`-2^m` is the lower bound, :math:`m \in \mathcal{Z}`. Default is 1.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        ste_fine_grained(bool): Straight Through Estimator is fine-grained or not.</span>
<span class="sd">            [default= `True` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Pow2Quantize</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">sign</span><span class="p">,</span> <span class="n">with_zero</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">ste_fine_grained</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<div class="viewcode-block" id="prune"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.prune">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">prune</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Prune the input as the following equation,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        q_i = \left \{</span>
<span class="sd">          \begin{array}{ll}</span>
<span class="sd">          0   &amp; abs(x_i) &lt; threshold \\</span>
<span class="sd">          x_i &amp; otherwise</span>
<span class="sd">          \end{array}</span>
<span class="sd">          \right.</span>
<span class="sd">    </span>
<span class="sd">    where :math:`threshold` is determined by `threshold = np.sort(np.abs(x))[int((x.size - 1) * rate)]`.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        rate(float): Sparse rate, or pruning rate.</span>
<span class="sd">            [default= `0.9` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Prune</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">rate</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">quantize_linear</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">,</span> <span class="n">round_mode</span><span class="o">=</span><span class="s1">&#39;HALF_AWAY_FROM_ZERO&#39;</span><span class="p">,</span> <span class="n">narrow_range</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Quantize linearly inputs with the scale and zero point.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        y = saturate(round(x / scale) + zero_point).</span>
<span class="sd">    </span>
<span class="sd">    :math:`saturate` rage is determined by `dtype` and :math:`round` mode is selected</span>
<span class="sd">    by `round_mode`. :math:`zero_point` is constrained by the `dtype` range and its values are</span>
<span class="sd">    rounded by `round_mode`.</span>
<span class="sd">    </span>
<span class="sd">    This function aligns with ONNX.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input N-D array.</span>
<span class="sd">        scale(~nnabla.Variable): Scale N-D array. The values must be positive number.</span>
<span class="sd">        zero_point(~nnabla.Variable): Zero point N-D array.</span>
<span class="sd">        round_mode(string): Rounding mode. HALF_AWAY_FROM_ZERO or HALF_TO_EVEN.</span>
<span class="sd">            [default= `&#39;HALF_AWAY_FROM_ZERO&#39;` ]</span>
<span class="sd">        narrow_range(bool): If true, this function does not use the minimum quantized value. For example, if `dtype` is int8 (the range is in [-128, 127]), the output range is corrected in [-127, 127].</span>
<span class="sd">            [default= `False` ]</span>
<span class="sd">        dtype(int): Data type for the output. The int value is compatible to the enumtype for the data type defined in `the numpy &lt;https://github.com/numpy/numpy/blob/master/numpy/core/include/numpy/ndarraytypes.h&gt;`_.</span>
<span class="sd">            [default= `1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Input N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">QuantizeLinear</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">round_mode</span><span class="p">,</span> <span class="n">narrow_range</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">dequantize_linear</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Dequantize linearly inputs with the scale and zero point.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        y = (x - zero_point) * scale.</span>
<span class="sd">    </span>
<span class="sd">    :math:`zero_point` is constrained by the `dtype` range.</span>
<span class="sd">    </span>
<span class="sd">    This function aligns with ONNX.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input N-D array.</span>
<span class="sd">        scale(~nnabla.Variable): Scale N-D array. The values must be positive number. This should be same as one used in QuantizeLinear.</span>
<span class="sd">        zero_point(~nnabla.Variable): Zero point N-D array. This should be same as one used in QuantizeLinear.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Input N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">DequantizeLinear</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">zero_point</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<div class="viewcode-block" id="top_n_error"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.top_n_error">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">top_n_error</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Top N error along the dimension specified by the axis, the element of outputs is</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        y_i = \left \{</span>
<span class="sd">        \begin{array}{l}</span>
<span class="sd">        1 \ (x_i \ is \ not \ within \ N-th \ place) \\</span>
<span class="sd">        0 \ (x_i \ is \ within \ N-th \ place)</span>
<span class="sd">        \end{array}</span>
<span class="sd">        \right.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Probabilities N-D array. :math:`D_1 \times ... \times D_i \times ... \times D_N`</span>
<span class="sd">        target(~nnabla.Variable): N-D array of labels. :math:`D_1 \times ... \times 1 \times ... \times D_N`</span>
<span class="sd">        axis(int): Axis on which the top N error is calculated.</span>
<span class="sd">            [default= `len(x.shape) - 1` ]</span>
<span class="sd">        n(int): top N</span>
<span class="sd">            [default= `1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Element-wise error N-D array. (:math:`D_1 \times ... \times 1 \times ... \times D_N`)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">TopNError</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">n</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="binary_error"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.binary_error">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">binary_error</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Elementwise binary error.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \left \{</span>
<span class="sd">        \begin{array}{l}</span>
<span class="sd">        0 ((x^{(0)} \geq 0.5) = (x^{(1)} \geq 0.5)) \\</span>
<span class="sd">        1 ((x^{(0)} \geq 0.5) \neq (x^{(1)} \geq 0.5))</span>
<span class="sd">        \end{array}</span>
<span class="sd">        \right.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Probabilities N-D array. :math:`-\infty` to :math:`\infty`.</span>
<span class="sd">        target(~nnabla.Variable): Labels N-D array. Usually set as 0 or 1, but, it allows probability (0 to 1) as inputs.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Element-wise errors N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BinaryError</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="confusion_matrix"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.confusion_matrix">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">confusion_matrix</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Confusion matrix.</span>
<span class="sd">    The return value is already summed over samples.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Probabilities N-D array. (:math:`D_1 \times ... \times D_i \times ... \times D_N`)</span>
<span class="sd">        target(~nnabla.Variable): Labels N-D array. (:math:`D_1 \times ... \times 1 \times ... \times D_N`)</span>
<span class="sd">        axis(int): Axis on which the confusion matrix is calculated.</span>
<span class="sd">            [default= `len(x.shape) - 1` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Confusion matrix 2-D array. Col index is estimated class. Row index is label class.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ConfusionMatrix</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="vat_noise"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.vat_noise">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">vat_noise</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Noise for virtual adversarial training.</span>
<span class="sd">    </span>
<span class="sd">    This layer is a special layer for GUI network designing, specialized for getting</span>
<span class="sd">    the noise of virtual adversarial training.</span>
<span class="sd">    </span>
<span class="sd">    In the backward process, the weight parameter will be replaced with the gradient.</span>
<span class="sd">    </span>
<span class="sd">    Forward</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \frac{\epsilon x_i}{\sqrt{\sum_k x_k^2 + c}}</span>
<span class="sd">    </span>
<span class="sd">    Backward</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \delta x_i = 0</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        w_i = \epsilon \delta y_i</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        This layer is a special layer for GUI network designing.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">        * `Miyato et.al, Distributional Smoothing with Virtual Adversarial Training.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1507.00677&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array of noise input. Noise is standard Gaussian noise initially, but the next step, fed back gradient variable.</span>
<span class="sd">        w(~nnabla.Variable): N-D array for keep gradient values.</span>
<span class="sd">        base_axis(int): Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">            [default= `1` ]</span>
<span class="sd">        eps(float): Noise norm (l2) factor.</span>
<span class="sd">            [default= `1.0` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">VATNoise</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">eps</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="unlink"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.unlink">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">unlink</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function behaves as an identity function on the forward pass,</span>
<span class="sd">    and deletes the gradient for the background pass.</span>
<span class="sd">    </span>
<span class="sd">    This layer is a special layer for GUI network designing, used for getting</span>
<span class="sd">    zero backward operation by adding this layer.</span>
<span class="sd">    </span>
<span class="sd">    Forward</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = x_i</span>
<span class="sd">    </span>
<span class="sd">    Backward</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \delta x_i = 0</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        This layer is a special layer for GUI network designing.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Unlink</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="sink"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.sink">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">sink</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a dummy variable used to call forward or backward function</span>
<span class="sd">    of multiple variables at one place.</span>
<span class="sd">    </span>
<span class="sd">    This takes any numbers of input variables with any shape,</span>
<span class="sd">    and creates a single 0-shape outputs.</span>
<span class="sd">    The forward pass does nothing. The backward pass set ones</span>
<span class="sd">    to the input grads if one_input_grad is set as true.</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        ``sink`` can only be called at the very end of the graph, and</span>
<span class="sd">        ``grad`` of input variables are cleared</span>
<span class="sd">         when ``y.backward(clear_buffer=True)`` is called.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        *x(~nnabla.Variable): Any number of inputs with any shape.</span>
<span class="sd">            [variadic]</span>
<span class="sd">        one_input_grad(bool): Set grads of inputs as one during backward. It is useful to set false if you want to set external gradients to the input variables.</span>
<span class="sd">            [default= `True` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Dummy variable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;sink must take more than 1 inputs&quot;</span>
    <span class="n">n_outputs</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;n_outputs&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">one_input_grad</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;one_input_grad&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Sink</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">one_input_grad</span><span class="p">)(</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="nms_detection2d"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.nms_detection2d">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">nms_detection2d</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">thresh</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nms</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nms_per_class</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Non-Maximum Suppression (NMS) to 2D Object detector output.</span>
<span class="sd">    The input is a 3-dimensional tensor with shape of ``(B, N, 5 + C)``</span>
<span class="sd">    where ``B`` denotes batch size, ``N`` denotes the number of detection box</span>
<span class="sd">    candidates, and ``C`` denotes the number of classes of object detection.</span>
<span class="sd">    ``5 + C`` consists of the box coordinates ``x, y, w, h`` in normalized</span>
<span class="sd">    coordinates (size of each x and y are 1.0), objectness</span>
<span class="sd">    (learned to predict IoU value to ground truth box), and the class probabilities of ``C`` classes.</span>
<span class="sd">    It outputs a tensor with the same dimensions as the input, where all</span>
<span class="sd">    values are copied from the input to the output, except the class</span>
<span class="sd">    probabilities are multiplied by objectness, and possibly suppressed to 0</span>
<span class="sd">    by NMS.</span>
<span class="sd">    During NMS, all of combination of pairs of bounding boxes is compared.</span>
<span class="sd">    For each pair, the bounding box with a lower detection score</span>
<span class="sd">    (described below) is suppressed if the overlap ratio (the IoU)</span>
<span class="sd">    is greater than the value of ``nms``.</span>
<span class="sd">    </span>
<span class="sd">    There are two suppression modes for NMS.</span>
<span class="sd">    </span>
<span class="sd">    1. Suppress by class probability (``nms_per_class`` is ``True``):</span>
<span class="sd">    For each bounding box, the detection score is calculated by</span>
<span class="sd">    ``objectness * probability[class_id]`` for each class.</span>
<span class="sd">    The suppression is done for each class independently.</span>
<span class="sd">    </span>
<span class="sd">    2. Suppress by objectness (``nms_per_class`` is ``False``):</span>
<span class="sd">    The suppression is done for each bounding box using ``objectness``</span>
<span class="sd">    as a detection score. All class probabilities becomes 0 for</span>
<span class="sd">    every suppressed boxes.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">        * `Joseph Redmon, Ali Farhadi, YOLO9000: Better, Faster, Stronger.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1612.08242&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): A 3-dimensional array.</span>
<span class="sd">        thresh(float): Detection score threshold.</span>
<span class="sd">            [default= `0.5` ]</span>
<span class="sd">        nms(float): IoU threshold for Non-maximum suppression (NMS).</span>
<span class="sd">            [default= `0.45` ]</span>
<span class="sd">        nms_per_class(bool): If true, NMS is applied for each class.</span>
<span class="sd">            [default= `True` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: A 3-dim array with the same dimensions with the input.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">thresh</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">thresh</span> <span class="o">=</span> <span class="mf">0.5</span>
    <span class="k">if</span> <span class="n">nms</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">nms</span> <span class="o">=</span> <span class="mf">0.45</span>
    <span class="k">if</span> <span class="n">nms_per_class</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">nms_per_class</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">NmsDetection2d</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">thresh</span><span class="p">,</span> <span class="n">nms</span><span class="p">,</span> <span class="n">nms_per_class</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">max_pooling_backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_border</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">channel_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Max pooling backward. This aims to support the n-th order gradients of </span>
<span class="sd">    the max pooling. The document of this function must not be shown, and </span>
<span class="sd">    the function must not be called in the end-user side.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        dy(~nnabla.Variable): Input variable.</span>
<span class="sd">        x(~nnabla.Variable): Input variable.</span>
<span class="sd">        kernel(:obj:`tuple` of :obj:`int`): Kernel sizes for each spatial axis.</span>
<span class="sd">        stride(:obj:`tuple` of :obj:`int`): Subsampling factors for each spatial axis.</span>
<span class="sd">            [default= `kernel` ]</span>
<span class="sd">        ignore_border(bool): If false, kernels covering borders are also considered for the output.</span>
<span class="sd">            [default= `True` ]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): Border padding values for each spatial axis. Padding will be added both sides of the dimension.</span>
<span class="sd">            [default= `(0,) * len(kernel)` ]</span>
<span class="sd">        channel_last(bool): If True, the last dimension is considered as channel dimension, a.k.a. NHWC order.</span>
<span class="sd">            [default= `False` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">kernel</span>
    <span class="k">if</span> <span class="n">pad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">MaxPoolingBackward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">ignore_border</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">channel_last</span><span class="p">)(</span><span class="n">dy</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">patch_correlation</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">patch</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">shift</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">patch_step</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">shift_step</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">      Multiplicative patch-wise comparision between inputs `x1` and `x2`, which</span>
<span class="sd">      must both be 4-dimensional NCHW (with `channel_last=False`) or NHWC (with</span>
<span class="sd">      `channel_last=True`) arrays (where *N* is the number of samples, *H* and</span>
<span class="sd">      *W* are the sample height and width and *C* is the number of channels).</span>
<span class="sd">      The function returns a 5-D array with shape :math:`(N, C_y, C_x, H_o, W_o)`</span>
<span class="sd">      where :math:`H_o, W_o` are determined by the possible patch locations within</span>
<span class="sd">      the, optionally padded, input image sizeand :math:`C_y, C_x` are determined</span>
<span class="sd">      by the optionally shifted patch positions.</span>
<span class="sd">    </span>
<span class="sd">      Mathmatically, the patch correlation is formulated as</span>
<span class="sd">    </span>
<span class="sd">      .. math::</span>
<span class="sd">    </span>
<span class="sd">        O(s_y, s_x, h_0, w_0) =</span>
<span class="sd">        \sum_{c} \sum_{k_h} \sum_{k_w} I_1(c, h + k_h, w + k_w) \times I_2(c, h + k_h + s_h, w + k_w + s_w),</span>
<span class="sd">    </span>
<span class="sd">      where :math:`I_1(c, h, w)` and :math:`I_2(c, h, w)` are the inputs at :math:`c`-th channel,</span>
<span class="sd">      :math:`h`-th height, and :math:`w`-th width, :math:`k_h, k_w` indices for the patch size</span>
<span class="sd">      and :math:`s_h, s_w` indices for the shifts.</span>
<span class="sd">    </span>
<span class="sd">      A single correlation value (per sample) is produced if the patch extends</span>
<span class="sd">      to the image dimensions and all other parameters use the default values.</span>
<span class="sd">    </span>
<span class="sd">      &gt;&gt;&gt; import numpy as np, nnabla as nn, nnabla.functions as F</span>
<span class="sd">      &gt;&gt;&gt; N, C, H, W = (1, 2, 3, 4)</span>
<span class="sd">      &gt;&gt;&gt; x = nn.Variable.from_numpy_array(np.ones([N, C, H, W]))</span>
<span class="sd">      &gt;&gt;&gt; F.patch_correlation(x, x, patch=(H, W)).d</span>
<span class="sd">      array([[[[[24.]]]]], dtype=float32)</span>
<span class="sd">    </span>
<span class="sd">      A patch that is smaller than the image size moves horizontal and vertical</span>
<span class="sd">      producing a value per position. The `patch_step` argument may be used to</span>
<span class="sd">      control the position increments.</span>
<span class="sd">    </span>
<span class="sd">      &gt;&gt;&gt; F.patch_correlation(x, x, patch=(H-1, W-1)).d</span>
<span class="sd">      array([[[[[12., 12.],</span>
<span class="sd">                [12., 12.]]]]], dtype=float32)</span>
<span class="sd">      &gt;&gt;&gt; F.patch_correlation(x, x, patch=(H-1, W-1), patch_step=(2, 1)).d</span>
<span class="sd">      array([[[[[12., 12.]]]]], dtype=float32)</span>
<span class="sd">    </span>
<span class="sd">      Multiple correlations may be performed at each position between the patch</span>
<span class="sd">      from `x1` and patches from `x2` at relative offsets striding the maximum</span>
<span class="sd">      vertical and horizontal distance given by the `shift` values at increments</span>
<span class="sd">      of `shift_step`. The shifted correlation values can be obtained for the</span>
<span class="sd">      from the second and third output dimension for the vertical and horizontal</span>
<span class="sd">      shifts.</span>
<span class="sd">    </span>
<span class="sd">      &gt;&gt;&gt; F.patch_correlation(x, x, (H, 1), shift=(0, 1)).shape</span>
<span class="sd">      (1, 1, 3, 1, 4)</span>
<span class="sd">      &gt;&gt;&gt; F.patch_correlation(x, x, (H, 1), shift=(0, 1)).d</span>
<span class="sd">      array([[[[[0., 6., 6., 6.]],</span>
<span class="sd">               [[6., 6., 6., 6.]],</span>
<span class="sd">               [[6., 6., 6., 0.]]]]], dtype=float32)</span>
<span class="sd">      &gt;&gt;&gt; F.patch_correlation(x, x, (H, 1), shift=(0, 1), shift_step=(1, 2)).d</span>
<span class="sd">      array([[[[[0., 6., 6., 6.]],</span>
<span class="sd">               [[6., 6., 6., 0.]]]]], dtype=float32)</span>
<span class="sd">    </span>
<span class="sd">      Padding with zero values may be applied individually to the top, bottom,</span>
<span class="sd">      left and right side of the input image.</span>
<span class="sd">    </span>
<span class="sd">      &gt;&gt;&gt; F.patch_correlation(x, x, patch=(H, W), padding=(0, 1, W, W)).d</span>
<span class="sd">      array([[[[[ 0.,  6., 12., 18., 24., 18., 12.,  6.,  0.],</span>
<span class="sd">                [ 0.,  4.,  8., 12., 16., 12.,  8.,  4.,  0.]]]]], dtype=float32)</span>
<span class="sd">    </span>
<span class="sd">      This function may be used to implement the FlowNetC correlation layer.</span>
<span class="sd">    </span>
<span class="sd">      &gt;&gt;&gt; N, C, H, W = (1, 256, 44, 60)</span>
<span class="sd">      &gt;&gt;&gt; x1, x2 = nn.Variable((N, C, H, W)), nn.Variable((N, C, H, W))</span>
<span class="sd">      &gt;&gt;&gt; F.patch_correlation(x1, x2, shift=20, shift_step=2).shape</span>
<span class="sd">      (1, 21, 21, 44, 60)</span>
<span class="sd">    </span>
<span class="sd">      References:</span>
<span class="sd">    </span>
<span class="sd">          * `Fischer et al., FlowNet: Learning Optical Flow with Convolutional</span>
<span class="sd">            Networks. &lt;https://arxiv.org/abs/1504.06852&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x1(~nnabla.Variable): Input N-D array with shape :math:`(N, H, W, C)`.</span>
<span class="sd">        x2(~nnabla.Variable): Input N-D array with shape :math:`(N, H, W, C)`.</span>
<span class="sd">        patch(:obj:`tuple` of :obj:`int`): A tuple with height and width of the correlation patch. A single integer expands to identical height and width.</span>
<span class="sd">            [default= `(1, 1)` ]</span>
<span class="sd">        shift(:obj:`tuple` of :obj:`int`): A tuple of maximum vertical and horizontal displacement of patches from `x2` that are correlated with a single patch from `x1`. A single integer expands to identical vertical and horizontal displacement.</span>
<span class="sd">            [default= `(0, 0)` ]</span>
<span class="sd">        patch_step(:obj:`tuple` of :obj:`int`): A tuple of vertical and horizontal increments for advancing the position of the correlation patch within the input image shape. A single integer expands to identical vertical and horizontal increments.</span>
<span class="sd">            [default= `(1, 1)` ]</span>
<span class="sd">        shift_step(:obj:`tuple` of :obj:`int`): A tuple of vertical and horizontal increments for advancing the relative offset position within the shift range. A single integer expands to identical vertical and horizontal increments.</span>
<span class="sd">            [default= `(1, 1)` ]</span>
<span class="sd">        padding(:obj:`tuple` of :obj:`int`): A tuple of top, bottom, left and right padding extent. A tuple of two values yields identical top/bottom and left/right padding from the first and second tuple value. A single integer expands to indential padding extent for all sides.</span>
<span class="sd">            [default= `(0, 0, 0, 0)` ]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: </span>
<span class="sd">            N-D array with shape :math:`(N, C_y, C_x, H_o, W_o)`.</span>
<span class="sd">            </span>
<span class="sd">            A spatial size of the output is calculated as</span>
<span class="sd">            </span>
<span class="sd">            .. math::</span>
<span class="sd">            </span>
<span class="sd">              H_o = \frac{H + (top\_pad + bottom\_pad) - patch_v }{patch\_step_v} + 1.</span>
<span class="sd">            </span>
<span class="sd">            A channel size of the ouptut is calculated as</span>
<span class="sd">            </span>
<span class="sd">            .. math::</span>
<span class="sd">            </span>
<span class="sd">              C_y = \frac{2 \times shift_v}{shift\_step_v} + 1.</span>
<span class="sd">            </span>
<span class="sd">            :math:`W_o` and :math:`C_x` are the same calculation with differenct components.</span>
<span class="sd">            </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">PatchCorrelation</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">patch</span><span class="p">,</span> <span class="n">shift</span><span class="p">,</span> <span class="n">patch_step</span><span class="p">,</span> <span class="n">shift_step</span><span class="p">,</span> <span class="n">padding</span><span class="p">)(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>

</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2017, Sony Corporation.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>