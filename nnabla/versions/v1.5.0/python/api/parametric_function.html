

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Parametric Functions &mdash; Neural Network Libraries 1.5.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Grad" href="grad.html" />
    <link rel="prev" title="Functions" href="function.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> Neural Network Libraries
          

          
          </a>

          
            
            
              <div class="version">
                1.5.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../python.html">Python Package</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../installation.html">Python Package Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial.html">Python API Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../command_line_interface.html">Python Command Line Interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html">Python API Examples</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../api.html">Python API Reference</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="common.html">Common</a></li>
<li class="toctree-l3"><a class="reference internal" href="nd_array.html">NdArray</a></li>
<li class="toctree-l3"><a class="reference internal" href="variable.html">Variable</a></li>
<li class="toctree-l3"><a class="reference internal" href="function.html">Functions</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Parametric Functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#module-nnabla.parameter">Parameter Management API</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-nnabla.parametric_functions">List of Parametric Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#parameter-initializer">Parameter Initializer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="grad.html">Grad</a></li>
<li class="toctree-l3"><a class="reference internal" href="solver.html">Solvers</a></li>
<li class="toctree-l3"><a class="reference internal" href="communicator.html">Communicator API</a></li>
<li class="toctree-l3"><a class="reference internal" href="monitor.html">Monitors</a></li>
<li class="toctree-l3"><a class="reference internal" href="utils.html">Utils</a></li>
<li class="toctree-l3"><a class="reference internal" href="ext.html">Extensions</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html">Pretrained Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="experimental.html">Experimental</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../cpp.html">C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data_exchange_file_format.html">Data exchange file format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../format.html">Data Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../file_format_converter/file_format_converter.html">File format converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../support_status.html">Support Status</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">Contributing Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../license.html">License</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Neural Network Libraries</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../python.html">Python Package</a> &raquo;</li>
        
          <li><a href="../api.html">Python API Reference</a> &raquo;</li>
        
      <li>Parametric Functions</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/python/api/parametric_function.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="parametric-functions">
<h1>Parametric Functions<a class="headerlink" href="#parametric-functions" title="Permalink to this headline">¶</a></h1>
<p>In NNabla, trainable models are created by composing functions that have optimizable parameters.
These functions are called parametric functions.
Parametric functions are provided by <a class="reference internal" href="#module-nnabla.parametric_functions" title="nnabla.parametric_functions"><code class="xref py py-mod docutils literal notranslate"><span class="pre">nnabla.parametric_functions</span></code></a>.</p>
<dl class="simple">
<dt>See also:</dt><dd><p><a class="reference external" href="http://nnabla.readthedocs.io/en/latest/python/tutorial/python_api.html">Python API Tutorial</a>.</p>
</dd>
</dl>
<div class="section" id="module-nnabla.parameter">
<span id="parameter-management-api"></span><span id="parameter"></span><h2>Parameter Management API<a class="headerlink" href="#module-nnabla.parameter" title="Permalink to this headline">¶</a></h2>
<p>The parameters registered by <a class="reference internal" href="#id1"><span class="std std-ref">List of Parametric Functions</span></a>
can be managed using APIs listed in this section.</p>
<dl class="function">
<dt id="nnabla.parameter.parameter_scope">
<code class="sig-prename descclassname">nnabla.parameter.</code><code class="sig-name descname">parameter_scope</code><span class="sig-paren">(</span><em class="sig-param">name</em>, <em class="sig-param">scope=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parameter.html#parameter_scope"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parameter.parameter_scope" title="Permalink to this definition">¶</a></dt>
<dd><p>Grouping parameters registered by parametric functions
listed in <a class="reference internal" href="#module-nnabla.parametric_functions" title="nnabla.parametric_functions"><code class="xref py py-mod docutils literal notranslate"><span class="pre">nnabla.parametric_functions</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – Parameter scope name.</p></li>
<li><p><strong>scope</strong> (<em>OrderedDict</em><em>, </em><em>optional</em>) – Specify current parameter scope as a local dictionary.
The default value is <code class="docutils literal notranslate"><span class="pre">None</span></code>. In this case,
the current parameter scope maintained in global is used.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nnabla</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">nnabla.parametric_functions</span> <span class="k">as</span> <span class="nn">PF</span>
<span class="kn">import</span> <span class="nn">nnabla.functions</span> <span class="k">as</span> <span class="nn">F</span>

<span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s1">&#39;conv1&#39;</span><span class="p">):</span>
    <span class="n">conv_out1</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">bn_out1</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span><span class="n">conv_out1</span><span class="p">)</span>
    <span class="n">act_out1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">bn_out1</span><span class="p">)</span>
<span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s1">&#39;conv2&#39;</span><span class="p">):</span>
    <span class="n">conv_out2</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">act_out1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="n">bn_out2</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span><span class="n">conv_out2</span><span class="p">)</span>
    <span class="n">act_out2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">bn_out2</span><span class="p">)</span>
</pre></div>
</div>
<p>Nesting <a class="reference external" href="https://docs.python.org/3.6/reference/compound_stmts.html#with" title="(in Python v3.6)"><code class="docutils literal notranslate"><span class="pre">The</span> <span class="pre">with</span> <span class="pre">statement</span></code></a> blocks allows you to nest parameter scopes.
This can also be done by using “/” inside the parameter names.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s1">&#39;network1&#39;</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s1">&#39;conv1&#39;</span><span class="p">):</span>
        <span class="n">conv_out1</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
        <span class="n">bn_out1</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span><span class="n">conv_out1</span><span class="p">)</span>
        <span class="n">act_out1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">bn_out1</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s1">&#39;conv2&#39;</span><span class="p">):</span>
        <span class="n">conv_out2</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">act_out1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="n">bn_out2</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span><span class="n">conv_out2</span><span class="p">)</span>
        <span class="n">act_out2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">bn_out2</span><span class="p">)</span>
</pre></div>
</div>
<p>is equivalent to</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s1">&#39;network1/conv1&#39;</span><span class="p">):</span>
    <span class="n">conv_out1</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">bn_out1</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span><span class="n">conv_out1</span><span class="p">)</span>
    <span class="n">act_out1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">bn_out1</span><span class="p">)</span>
<span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s1">&#39;network1/conv2&#39;</span><span class="p">):</span>
    <span class="n">conv_out2</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">act_out1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="n">bn_out2</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span><span class="n">conv_out2</span><span class="p">)</span>
    <span class="n">act_out2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">bn_out2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parameter.get_current_parameter_scope">
<code class="sig-prename descclassname">nnabla.parameter.</code><code class="sig-name descname">get_current_parameter_scope</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parameter.html#get_current_parameter_scope"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parameter.get_current_parameter_scope" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns current parameter scope.</p>
</dd></dl>

<dl class="function">
<dt id="nnabla.parameter.get_parameters">
<code class="sig-prename descclassname">nnabla.parameter.</code><code class="sig-name descname">get_parameters</code><span class="sig-paren">(</span><em class="sig-param">params=None</em>, <em class="sig-param">path=''</em>, <em class="sig-param">grad_only=True</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parameter.html#get_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parameter.get_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameter Variables under the current parameter scope.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a>) – Internal use. User doesn’t set it manually.</p></li>
<li><p><strong>path</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – Internal use.  User doesn’t set it manually.</p></li>
<li><p><strong>grad_only</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Retrieve all parameters under the current scope if
False, while only parameters with need_grad=True are retrieved
if True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>{<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code></a> : <a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Variable</span></code></a>}</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)">dict</a></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nnabla.parameter.clear_parameters">
<code class="sig-prename descclassname">nnabla.parameter.</code><code class="sig-name descname">clear_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parameter.html#clear_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parameter.clear_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Clear all parameters in the current scope.</p>
</dd></dl>

<dl class="function">
<dt id="nnabla.parameter.save_parameters">
<code class="sig-prename descclassname">nnabla.parameter.</code><code class="sig-name descname">save_parameters</code><span class="sig-paren">(</span><em class="sig-param">path</em>, <em class="sig-param">params=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parameter.html#save_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parameter.save_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Save all parameters into a file with the specified format.</p>
<p>Currently hdf5 and protobuf formats are supported.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> – path or file object</p></li>
<li><p><strong>params</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a><em>, </em><em>optional</em>) – Parameters to be saved. Dictionary is of a parameter name (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code></a>) to <a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Variable</span></code></a>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nnabla.parameter.load_parameters">
<code class="sig-prename descclassname">nnabla.parameter.</code><code class="sig-name descname">load_parameters</code><span class="sig-paren">(</span><em class="sig-param">path</em>, <em class="sig-param">proto=None</em>, <em class="sig-param">needs_proto=False</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parameter.html#load_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parameter.load_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Load parameters from a file with the specified format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>path</strong> – path or file object</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="nnabla.parameter.get_parameter_or_create">
<code class="sig-prename descclassname">nnabla.parameter.</code><code class="sig-name descname">get_parameter_or_create</code><span class="sig-paren">(</span><em class="sig-param">name</em>, <em class="sig-param">shape=None</em>, <em class="sig-param">initializer=None</em>, <em class="sig-param">need_grad=True</em>, <em class="sig-param">as_need_grad=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parameter.html#get_parameter_or_create"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parameter.get_parameter_or_create" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an existing parameter variable in current parameter scope
with the provided name.</p>
<p>If a variable with the provided name does not exist,
a new variable is created and registered to the current parameter scope
with the name, then returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – The name under the current scope. If it already exists, the name
is queried from the parameter manager.</p></li>
<li><p><strong>shape</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Shape of created parameter. The shape of the specified
parameter must match with this shape. The default is None which is
only valid if initializer is given as an <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>.</p></li>
<li><p><strong>initializer</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – An initialization function to be applied to the parameter.
<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a> can also be given to initialize parameters
from numpy array data.</p></li>
<li><p><strong>need_grad</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Register the parameter with the specified <code class="docutils literal notranslate"><span class="pre">need_grad</span></code> flag.
The default is True. If the flag is different from the previously
specified one, the flag will be overwritten, but the values will be
kept.</p></li>
<li><p><strong>as_need_grad</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Get a parameter variable with the specified <code class="docutils literal notranslate"><span class="pre">need_grad</span></code> flag.
Note that this doesn’t overwrite the flag of the registered parameter
variable with the provided name. Instead, if the given flag
mismatches with the previously registered <code class="docutils literal notranslate"><span class="pre">need_grad</span></code> flag, it
returns a new variable referring to the same array contents but with
<code class="docutils literal notranslate"><span class="pre">need_grad=as_need_grad</span></code>.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It returns a <a class="reference internal" href="../tutorial/python_api.html#variable"><span class="std std-ref">Variable</span></a> which is unlinked from the
registered one in the current parmeter scope
(using <a class="reference internal" href="variable.html#nnabla.Variable.get_unlinked_variable" title="nnabla.Variable.get_unlinked_variable"><code class="xref py py-meth docutils literal notranslate"><span class="pre">nnabla.Variable.get_unlinked_variable()</span></code></a>).
That means changing a <a class="reference internal" href="variable.html#nnabla.Variable.need_grad" title="nnabla.Variable.need_grad"><code class="xref any py py-attr docutils literal notranslate"><span class="pre">need_grad</span></code></a> attribute doesn’t affect
the variable existing in the current parameter scope.</p>
</div>
</dd></dl>

</div>
<div class="section" id="module-nnabla.parametric_functions">
<span id="list-of-parametric-functions"></span><span id="id1"></span><h2>List of Parametric Functions<a class="headerlink" href="#module-nnabla.parametric_functions" title="Permalink to this headline">¶</a></h2>
<p>Parametric functions are provided by <a class="reference internal" href="#module-nnabla.parametric_functions" title="nnabla.parametric_functions"><code class="xref py py-mod docutils literal notranslate"><span class="pre">nnabla.parametric_functions</span></code></a> , as listed below.
Like functions listed in <a class="reference internal" href="function.html#functions"><span class="std std-ref">Functions</span></a>, they take <a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Variable</span></code></a> (s) as
first argument(s) followed by options specific to a parametric function. In addition,
they register parameter <a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Variable</span></code></a> (s) into the parameter scope.</p>
<p>The parameter variables are registered with <code class="docutils literal notranslate"><span class="pre">need_grad</span></code> properties specific
to a parametric function. The variables with <code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code> flag will not
be updated by gradient descent. Hence, backward computation is not executed for
those variables. <code class="docutils literal notranslate"><span class="pre">False</span></code> is usually specified when the parameters are updated
during foward pass and/or backward pass, e.g., batch normalization.</p>
<p>All parametric functions take an optional argument <code class="docutils literal notranslate"><span class="pre">fix_parameters=False</span></code>.
By giving <code class="docutils literal notranslate"><span class="pre">True</span></code>, the associated parameter variables are connected to a
computation graph with a property <code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code> regardless properties
of the registered variables, then backward gradient
computation is not executed for those variables. This is useful when you create
a computation graph for evaluation purpose, fixing parameters partially in a
graph, and so on.</p>
<p>All parametric functions listed below are decorated with the following decorator.</p>
<dl class="function">
<dt id="nnabla.parametric_functions.parametric_function_api">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">parametric_function_api</code><span class="sig-paren">(</span><em class="sig-param">scope_name=None</em>, <em class="sig-param">param_desc=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#parametric_function_api"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.parametric_function_api" title="Permalink to this definition">¶</a></dt>
<dd><p>Decorator for parametric functions.</p>
<p>The decorated function is always called under
a parameter scope <code class="docutils literal notranslate"><span class="pre">scope_name</span></code>.
Also, the decorator adds an additional argument <code class="docutils literal notranslate"><span class="pre">name</span></code> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code></a>,
default is <code class="docutils literal notranslate"><span class="pre">None</span></code>) at the end. If <code class="docutils literal notranslate"><span class="pre">name</span></code> is specified, the
scope <code class="docutils literal notranslate"><span class="pre">scope_name</span></code> comes under a scope <code class="docutils literal notranslate"><span class="pre">name</span></code>. This feature
could reduce vertical space usage of the source code.
Any parametric function should be decorated by this.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>scope_name</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>, </em><em>optional</em>) – The original function will be called
under a parameter scope named by <code class="docutils literal notranslate"><span class="pre">scope_name</span></code>.</p></li>
<li><p><strong>param_desc</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#list" title="(in Python v3.6)"><em>list</em></a><em>, </em><em>optional</em>) – Descriptions of parameters will be automatically included into docstring.
This must be a list of tuples with 4 elements composed of
(name (str), description (str), shape info (str), need_grad (bool)).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A decorated parametric function.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>function</p>
</dd>
</dl>
</dd></dl>

<p>See <a class="reference internal" href="#parameter"><span class="std std-ref">Parameter Management API</span></a> to know how to query and manipulate registered variables.</p>
<p>Here is the list of parametric functions.</p>
<dl class="function">
<dt id="nnabla.parametric_functions.affine">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">affine</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">n_outmaps</em>, <em class="sig-param">base_axis=1</em>, <em class="sig-param">w_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">apply_w=None</em>, <em class="sig-param">apply_b=None</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#affine"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.affine" title="Permalink to this definition">¶</a></dt>
<dd><p>The affine layer, also known as the fully connected layer. Computes</p>
<div class="math notranslate nohighlight">
\[{\mathbf y} = {\mathbf A} {\mathbf x} + {\mathbf b}.\]</div>
<p>where <span class="math notranslate nohighlight">\({\mathbf x}, {\mathbf y}\)</span> are the inputs and outputs respectively,
and <span class="math notranslate nohighlight">\({\mathbf A}, {\mathbf b}\)</span> are constants.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape (<span class="math notranslate nohighlight">\(M_0 \times \ldots \times M_{B-1} \times D_B \times \ldots \times D_N\)</span>). Dimensions before and after base_axis are flattened as if it is a matrix.</p></li>
<li><p><strong>n_outmaps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a> or <a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Number of output neurons per data.</p></li>
<li><p><strong>base_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Dimensions up to <code class="xref any docutils literal notranslate"><span class="pre">base_axis</span></code> are treated as the sample dimensions.</p></li>
<li><p><strong>w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for bias. By default, it is initialized with zeros if <code class="xref any docutils literal notranslate"><span class="pre">with_bias</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random generator for Initializer.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Specify whether to include the bias term.</p></li>
<li><p><strong>apply_w</strong> (<em>function</em>) – Lambda, function, or callable object applied to the weights.</p></li>
<li><p><strong>apply_b</strong> (<em>function</em>) – Lambda, function, or callable object applied to the bias.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><span class="math notranslate nohighlight">\((B + 1)\)</span>-D array. (<span class="math notranslate nohighlight">\(M_0 \times \ldots \times M_{B-1} \times L\)</span>)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;affine&quot;</span></code>;</p>
<ul class="simple">
<li><p>W (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Weight matrix. (shape: <code class="docutils literal notranslate"><span class="pre">(inmaps,</span> <span class="pre">outmaps)</span></code>)</p></li>
<li><p>b (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : bias vector. (shape: <code class="docutils literal notranslate"><span class="pre">(outputs,)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">affine</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.convolution">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">convolution</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">outmaps</em>, <em class="sig-param">kernel</em>, <em class="sig-param">pad=None</em>, <em class="sig-param">stride=None</em>, <em class="sig-param">dilation=None</em>, <em class="sig-param">group=1</em>, <em class="sig-param">channel_last=False</em>, <em class="sig-param">w_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">base_axis=1</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">apply_w=None</em>, <em class="sig-param">apply_b=None</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#convolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.convolution" title="Permalink to this definition">¶</a></dt>
<dd><p>N-D Convolution with a bias term.</p>
<p>For Dilated Convolution (a.k.a. Atrous Convolution), refer to:</p>
<ul class="simple">
<li><p>Chen et al., DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. <a class="reference external" href="https://arxiv.org/abs/1606.00915">https://arxiv.org/abs/1606.00915</a></p></li>
<li><p>Yu et al., Multi-Scale Context Aggregation by Dilated Convolutions. <a class="reference external" href="https://arxiv.org/abs/1511.07122">https://arxiv.org/abs/1511.07122</a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Convolution is a computationally intensive operation that
should preferably be run with the <code class="xref any docutils literal notranslate"><span class="pre">cudnn</span></code> backend. NNabla
then uses CuDNN library functions to determine and cache the
fastest algorithm for the given set of convolution parameters,
which results in additional memory consumption which may pose
a problem for GPUs with insufficient memory size. In that
case, the <code class="xref any docutils literal notranslate"><span class="pre">NNABLA_CUDNN_WORKSPACE_LIMIT</span></code> environment variable
can be used to restrict the choice of algorithms to those that
fit the given workspace memory limit, expressed in bytes. In
some cases it may also be desired to restrict the automatic
search to algorithms that produce deterministic (reproducible)
results. This can be requested by setting the the environment
variable <code class="xref any docutils literal notranslate"><span class="pre">NNABLA_CUDNN_DETERMINISTIC</span></code> to a non-zero value.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – N-D array.</p></li>
<li><p><strong>outmaps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of convolution kernels (which is equal to the number of output channels). For example, to apply convolution on an input with 16 types of filters, specify 16.</p></li>
<li><p><strong>kernel</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</p></li>
<li><p><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Padding sizes for dimensions.</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Stride sizes for dimensions.</p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Dilation sizes for dimensions.</p></li>
<li><p><strong>group</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of groups of channels. This makes connections across channels more sparse by grouping connections along map direction.</p></li>
<li><p><strong>channel_last</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If True, the last dimension is considered as channel dimension, a.k.a. NHWC order.</p></li>
<li><p><strong>w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for bias. By default, it is initialized with zeros if <code class="xref any docutils literal notranslate"><span class="pre">with_bias</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>base_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Dimensions up to <code class="xref any docutils literal notranslate"><span class="pre">base_axis</span></code> are treated as the sample dimensions.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random generator for Initializer.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Specify whether to include the bias term.</p></li>
<li><p><strong>apply_w</strong> (<em>function</em>) – Lambda, function, or callable object applied to the weights.</p></li>
<li><p><strong>apply_b</strong> (<em>function</em>) – Lambda, function, or callable object applied to the bias.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-D array. See <a class="reference internal" href="function.html#nnabla.functions.convolution" title="nnabla.functions.convolution"><code class="xref py py-obj docutils literal notranslate"><span class="pre">convolution</span></code></a> for the output shape.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;conv&quot;</span></code>;</p>
<ul class="simple">
<li><p>W (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Filter weights. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,</span> <span class="pre">inmaps</span> <span class="pre">//</span> <span class="pre">group,</span> <span class="pre">*kernel)</span></code>)</p></li>
<li><p>b (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Bias vector. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">convolution</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.depthwise_convolution">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">depthwise_convolution</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">kernel</em>, <em class="sig-param">pad=None</em>, <em class="sig-param">stride=None</em>, <em class="sig-param">dilation=None</em>, <em class="sig-param">multiplier=1</em>, <em class="sig-param">w_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">base_axis=1</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#depthwise_convolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.depthwise_convolution" title="Permalink to this definition">¶</a></dt>
<dd><p>N-D Depthwise Convolution with a bias term.</p>
<p>Reference:</p>
<ul class="simple">
<li><ol class="upperalpha simple" start="6">
<li><p>Chollet: Chollet, Francois. “Xception: Deep Learning with Depthwise Separable Convolutions. <a class="reference external" href="https://arxiv.org/abs/1610.02357">https://arxiv.org/abs/1610.02357</a></p></li>
</ol>
</li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – N-D array.</p></li>
<li><p><strong>kernel</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</p></li>
<li><p><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Padding sizes for dimensions.</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Stride sizes for dimensions.</p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Dilation sizes for dimensions.</p></li>
<li><p><strong>multiplier</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Number of output feature maps per input feature map.</p></li>
<li><p><strong>w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for weight.  By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for bias. By default, it is initialized with zeros if <code class="xref any docutils literal notranslate"><span class="pre">with_bias</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>base_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Dimensions up to <code class="xref any docutils literal notranslate"><span class="pre">base_axis</span></code> are treated as the sample dimensions.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random generator for Initializer.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Specify whether to include the bias term.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-D array. See <a class="reference internal" href="function.html#nnabla.functions.depthwise_convolution" title="nnabla.functions.depthwise_convolution"><code class="xref py py-obj docutils literal notranslate"><span class="pre">depthwise_convolution</span></code></a> for the output shape.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;depthwise_conv&quot;</span></code>;</p>
<ul class="simple">
<li><p>W (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Filter weights. (shape: <code class="docutils literal notranslate"><span class="pre">(inmaps</span> <span class="pre">*</span> <span class="pre">multiplier,</span> <span class="pre">*kernel)</span></code>)</p></li>
<li><p>b (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Bias vector. (shape: <code class="docutils literal notranslate"><span class="pre">(inmaps</span> <span class="pre">*</span> <span class="pre">multiplier,)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">depthwise_convolution</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.deconvolution">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">deconvolution</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">outmaps</em>, <em class="sig-param">kernel</em>, <em class="sig-param">pad=None</em>, <em class="sig-param">stride=None</em>, <em class="sig-param">dilation=None</em>, <em class="sig-param">group=1</em>, <em class="sig-param">w_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">base_axis=1</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">apply_w=None</em>, <em class="sig-param">apply_b=None</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#deconvolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.deconvolution" title="Permalink to this definition">¶</a></dt>
<dd><p>Deconvolution layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – N-D array.</p></li>
<li><p><strong>outmaps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of deconvolution kernels (which is equal to the number of output channels). For example, to apply deconvolution on an input with 16 types of filters, specify 16.</p></li>
<li><p><strong>kernel</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Convolution kernel size. For example, to apply deconvolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</p></li>
<li><p><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Padding sizes for dimensions.</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Stride sizes for dimensions.</p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Dilation sizes for dimensions.</p></li>
<li><p><strong>group</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of groups of channels. This makes connections across channels sparser by grouping connections along map direction.</p></li>
<li><p><strong>w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for bias. By default, it is initialized with zeros if <code class="xref any docutils literal notranslate"><span class="pre">with_bias</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>base_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Dimensions up to <code class="xref any docutils literal notranslate"><span class="pre">base_axis</span></code> are treated as the sample dimensions.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random generator for Initializer.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Specify whether to include the bias term.</p></li>
<li><p><strong>apply_w</strong> (<em>function</em>) – Lambda, function, or callable object applied to the weights.</p></li>
<li><p><strong>apply_b</strong> (<em>function</em>) – Lambda, function, or callable object applied to the bias.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-D array. See <a class="reference internal" href="function.html#nnabla.functions.deconvolution" title="nnabla.functions.deconvolution"><code class="xref py py-obj docutils literal notranslate"><span class="pre">deconvolution</span></code></a> for the output shape.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;deconv&quot;</span></code>;</p>
<ul class="simple">
<li><p>W (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Filter weights. (shape: <code class="docutils literal notranslate"><span class="pre">(inmaps,</span> <span class="pre">outmaps</span> <span class="pre">//</span> <span class="pre">group,</span> <span class="pre">*kernel)</span></code>)</p></li>
<li><p>b (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Bias vector. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">deconvolution</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.depthwise_deconvolution">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">depthwise_deconvolution</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">kernel</em>, <em class="sig-param">pad=None</em>, <em class="sig-param">stride=None</em>, <em class="sig-param">dilation=None</em>, <em class="sig-param">divisor=1</em>, <em class="sig-param">w_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">base_axis=1</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#depthwise_deconvolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.depthwise_deconvolution" title="Permalink to this definition">¶</a></dt>
<dd><p>Depthwise deconvolution computes the transposed depthwise
convolution for one-dimensional and two-dimensional input data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – N-D array.</p></li>
<li><p><strong>kernel</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</p></li>
<li><p><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Padding sizes for dimensions.</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Stride sizes for dimensions.</p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Dilation sizes for dimensions.</p></li>
<li><p><strong>divisor</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Number of input feature maps per output feature map.</p></li>
<li><p><strong>w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for bias. By default, it is initialized with zeros if <code class="xref any docutils literal notranslate"><span class="pre">with_bias</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>base_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Dimensions up to <code class="xref any docutils literal notranslate"><span class="pre">base_axis</span></code> are treated as the sample dimensions.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random generator for Initializer.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Specify whether to include the bias term.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-D array. See <a class="reference internal" href="function.html#nnabla.functions.depthwise_deconvolution" title="nnabla.functions.depthwise_deconvolution"><code class="xref py py-obj docutils literal notranslate"><span class="pre">depthwise_deconvolution</span></code></a> for the output shape.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;depthwise_deconv&quot;</span></code>;</p>
<ul class="simple">
<li><p>W (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Filter weights. (shape: <code class="docutils literal notranslate"><span class="pre">(inmaps,)</span> <span class="pre">+</span> <span class="pre">kernel</span></code>)</p></li>
<li><p>b (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Bias vector. (shape: <code class="docutils literal notranslate"><span class="pre">(inmaps</span> <span class="pre">/</span> <span class="pre">divisor,)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">depthwise_deconvolution</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.batch_normalization">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">batch_normalization</code><span class="sig-paren">(</span><em class="sig-param">inp, axes=[1], decay_rate=0.9, eps=1e-05, batch_stat=True, output_stat=False, fix_parameters=False, param_init=None, no_scale=False, no_bias=False, name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#batch_normalization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.batch_normalization" title="Permalink to this definition">¶</a></dt>
<dd><p>Batch normalization layer.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{lcl}
\mu &amp;=&amp; \frac{1}{M} \sum x_i\\
\sigma^2 &amp;=&amp; \frac{1}{M} \sum \left(x_i - \mu\right)^2\\
\hat{x}_i &amp;=&amp; \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon }}\\
y_i &amp;= &amp; \hat{x}_i \gamma + \beta.
\end{array}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(x_i, y_i\)</span> are the inputs.
In testing, the mean and variance computed by moving average calculated during training are used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – N-D array of input.</p></li>
<li><p><strong>axes</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Mean and variance for each element in <code class="docutils literal notranslate"><span class="pre">axes</span></code> are calculated using
elements on the rest axes. For example, if an input is 4 dimensions,
and <code class="docutils literal notranslate"><span class="pre">axes</span></code> is <code class="docutils literal notranslate"><span class="pre">[1]</span></code>,  batch mean is calculated as
<code class="docutils literal notranslate"><span class="pre">np.mean(inp.d,</span> <span class="pre">axis=(0,</span> <span class="pre">2,</span> <span class="pre">3),</span> <span class="pre">keepdims=True)</span></code>
(using numpy expression as an example).</p></li>
<li><p><strong>decay_rate</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Decay rate of running mean and variance.</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Tiny value to avoid zero division by std.</p></li>
<li><p><strong>batch_stat</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Use mini-batch statistics rather than running ones.</p></li>
<li><p><strong>output_stat</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Output batch mean and variance.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the beta and gamma will not be updated.</p></li>
<li><p><strong>param_init</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a>) – Parameter initializers can be set with a dict. A key of the dict must
be <code class="docutils literal notranslate"><span class="pre">'beta'</span></code>, <code class="docutils literal notranslate"><span class="pre">'gamma'</span></code>, <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> or <code class="docutils literal notranslate"><span class="pre">'var'</span></code>.
A value of the dict must be an <code class="xref py py-obj docutils literal notranslate"><span class="pre">Initializer</span></code>
or a <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>.
E.g. <code class="docutils literal notranslate"><span class="pre">{'beta':</span> <span class="pre">ConstantInitializer(0),</span> <span class="pre">'gamma':</span> <span class="pre">np.ones(gamma_shape)</span> <span class="pre">*</span> <span class="pre">2}</span></code>.</p></li>
<li><p><strong>no_scale</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the scale term is omitted.</p></li>
<li><p><strong>no_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the bias term is omitted.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-D array.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Ioffe and Szegedy, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. <a class="reference external" href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a></p></li>
</ul>
<p>The shape of parameters has the same number of dimensions with the input
data, and the shapes in <code class="docutils literal notranslate"><span class="pre">axes</span></code> has the same dimensions with the input, while the rest has <code class="docutils literal notranslate"><span class="pre">1</span></code>.
If an input is 4-dim and <code class="docutils literal notranslate"><span class="pre">axes=[1]</span></code>, the parameter shape will be
<code class="docutils literal notranslate"><span class="pre">param_shape</span>&#160; <span class="pre">=</span> <span class="pre">np.mean(inp.d,</span> <span class="pre">axis=(0,</span> <span class="pre">2,</span> <span class="pre">3),</span> <span class="pre">keepdims=True).shape</span></code>
(using numpy expression as an example).</p>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;bn&quot;</span></code>;</p>
<ul class="simple">
<li><p>beta (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Trainable bias <span class="math notranslate nohighlight">\(\beta\)</span>. (shape: <code class="docutils literal notranslate"><span class="pre">&lt;see</span> <span class="pre">above&gt;</span></code>)</p></li>
<li><p>gamma (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Trainable scaling factor <span class="math notranslate nohighlight">\(\gamma\)</span>. (shape: <code class="docutils literal notranslate"><span class="pre">&lt;see</span> <span class="pre">above&gt;</span></code>)</p></li>
<li><p>mean (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Moving average of batch mean. (shape: <code class="docutils literal notranslate"><span class="pre">&lt;see</span> <span class="pre">above&gt;</span></code>)</p></li>
<li><p>var (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Moving average of batch variance. (shape: <code class="docutils literal notranslate"><span class="pre">&lt;see</span> <span class="pre">above&gt;</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">batch_normalization</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.fused_batch_normalization">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">fused_batch_normalization</code><span class="sig-paren">(</span><em class="sig-param">inp, z=None, axes=[1], decay_rate=0.9, eps=1e-05, batch_stat=True, nonlinearity='relu', output_stat=False, fix_parameters=False, param_init=None, no_scale=False, no_bias=False, name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#fused_batch_normalization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.fused_batch_normalization" title="Permalink to this definition">¶</a></dt>
<dd><p>Batch normalization layer fused with the following add2 operation of a
residual input and an nonlinear activation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – N-D array of input.</p></li>
<li><p><strong>z</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a><em>, </em><em>optional</em>) – A residual input. By specifying None, the activation function will
follow immediately after BN operation.</p></li>
<li><p><strong>axes</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Mean and variance for each element in <code class="docutils literal notranslate"><span class="pre">axes</span></code> are calculated using
elements on the rest axes. For example, if an input is 4 dimensions,
and <code class="docutils literal notranslate"><span class="pre">axes</span></code> is <code class="docutils literal notranslate"><span class="pre">[1]</span></code>,  batch mean is calculated as
<code class="docutils literal notranslate"><span class="pre">np.mean(inp.d,</span> <span class="pre">axis=(0,</span> <span class="pre">2,</span> <span class="pre">3),</span> <span class="pre">keepdims=True)</span></code>
(using numpy expression as an example).</p></li>
<li><p><strong>decay_rate</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Decay rate of running mean and variance.</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Tiny value to avoid zero division by std.</p></li>
<li><p><strong>batch_stat</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Use mini-batch statistics rather than running ones.</p></li>
<li><p><strong>nonlinearity</strong> (<em>string</em>) – Activation function. The default is ‘relu’.</p></li>
<li><p><strong>output_stat</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Output batch mean and variance.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the beta and gamma will not be updated.</p></li>
<li><p><strong>no_scale</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the scale term is omitted.</p></li>
<li><p><strong>no_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the bias term is omitted.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-D array.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;bn&quot;</span></code>;</p>
<ul class="simple">
<li><p>beta (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Trainable bias <span class="math notranslate nohighlight">\(\beta\)</span>. (shape: <code class="docutils literal notranslate"><span class="pre">&lt;see</span> <span class="pre">above&gt;</span></code>)</p></li>
<li><p>gamma (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Trainable scaling factor <span class="math notranslate nohighlight">\(\gamma\)</span>. (shape: <code class="docutils literal notranslate"><span class="pre">&lt;see</span> <span class="pre">above&gt;</span></code>)</p></li>
<li><p>mean (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Moving average of batch mean. (shape: <code class="docutils literal notranslate"><span class="pre">&lt;see</span> <span class="pre">above&gt;</span></code>)</p></li>
<li><p>var (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Moving average of batch variance. (shape: <code class="docutils literal notranslate"><span class="pre">&lt;see</span> <span class="pre">above&gt;</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">fused_batch_normalization</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.sync_batch_normalization">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">sync_batch_normalization</code><span class="sig-paren">(</span><em class="sig-param">inp, comm, group='world', axes=[1], decay_rate=0.9, eps=1e-05, batch_stat=True, output_stat=False, fix_parameters=False, param_init=None, no_scale=False, no_bias=False, name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#sync_batch_normalization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.sync_batch_normalization" title="Permalink to this definition">¶</a></dt>
<dd><p>Synchronized batch normalization layer.</p>
<p>For some tasks (e.g., semantic segmentation), batch size will be too small and BatchNormalization layer might not work well.
SyncBatchNorlization layer solves these problems by synchronizing batch stats (mean and var) between multiple processes.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{lcl}
\mu &amp;=&amp; \frac{1}{M} \sum x_i\\
\sigma^2 &amp;=&amp; \frac{1}{M} \left(\sum x_i - \mu\right)^2\\
\hat{x}_i &amp;=&amp; \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon }}\\
y_i &amp;= &amp; \hat{x}_i \gamma + \beta.
\end{array}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(x_i, y_i\)</span> are the inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – N-D array of input.</p></li>
<li><p><strong>comm</strong> (<a class="reference internal" href="communicator.html#nnabla.communicators.Communicator" title="nnabla.communicators.Communicator"><em>Communicator</em></a>) – The communicator</p></li>
<li><p><strong>group</strong> (<em>string</em>) – The name of the communicator group</p></li>
<li><p><strong>axes</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Mean and variance for each element in <code class="docutils literal notranslate"><span class="pre">axes</span></code> are calculated using
elements on the rest axes. For example, if an input is 4 dimensions,
and <code class="docutils literal notranslate"><span class="pre">axes</span></code> is <code class="docutils literal notranslate"><span class="pre">[1]</span></code>,  batch mean is calculated as
<code class="docutils literal notranslate"><span class="pre">np.mean(inp.d,</span> <span class="pre">axis=(0,</span> <span class="pre">2,</span> <span class="pre">3),</span> <span class="pre">keepdims=True)</span></code>
(using numpy expression as an example).</p></li>
<li><p><strong>decay_rate</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Decay rate of running mean and variance.</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Tiny value to avoid zero division by std.</p></li>
<li><p><strong>batch_stat</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Use mini-batch statistics rather than running ones.</p></li>
<li><p><strong>output_stat</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Output batch mean and variance.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the beta and gamma will not be updated.</p></li>
<li><p><strong>param_init</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a>) – Parameter initializers can be set with a dict. A key of the dict must
be <code class="docutils literal notranslate"><span class="pre">'beta'</span></code>, <code class="docutils literal notranslate"><span class="pre">'gamma'</span></code>, <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> or <code class="docutils literal notranslate"><span class="pre">'var'</span></code>.
A value of the dict must be an <code class="xref py py-obj docutils literal notranslate"><span class="pre">Initializer</span></code>
or a <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>.
E.g. <code class="docutils literal notranslate"><span class="pre">{'beta':</span> <span class="pre">ConstantInitializer(0),</span> <span class="pre">'gamma':</span> <span class="pre">np.ones(gamma_shape)</span> <span class="pre">*</span> <span class="pre">2}</span></code>.</p></li>
<li><p><strong>no_scale</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the scale term is omitted.</p></li>
<li><p><strong>no_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the bias term is omitted.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-D array.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Ioffe and Szegedy, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, <a class="reference external" href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a></p></li>
<li><p>Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, Amit Agrawal, Context Encoding for Semantic Segmentation, <a class="reference external" href="https://arxiv.org/abs/1803.08904">https://arxiv.org/abs/1803.08904</a></p></li>
<li><p>Implementing Synchronized Multi-GPU Batch Normalization <a class="reference external" href="https://hangzhang.org/PyTorch-Encoding/notes/syncbn.html">https://hangzhang.org/PyTorch-Encoding/notes/syncbn.html</a></p></li>
</ul>
<p>The shape of parameters has the same number of dimensions with the input
data, and the shapes in <code class="docutils literal notranslate"><span class="pre">axes</span></code> has the same dimensions with the input, while the rest has <code class="docutils literal notranslate"><span class="pre">1</span></code>.
If an input is 4-dim and <code class="docutils literal notranslate"><span class="pre">axes=[1]</span></code>, the parameter shape will be
<code class="docutils literal notranslate"><span class="pre">param_shape</span>&#160; <span class="pre">=</span> <span class="pre">np.mean(inp.d,</span> <span class="pre">axis=(0,</span> <span class="pre">2,</span> <span class="pre">3),</span> <span class="pre">keepdims=True).shape</span></code>
(using numpy expression as an example).</p>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;bn&quot;</span></code>;</p>
<ul class="simple">
<li><p>beta (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Trainable bias <span class="math notranslate nohighlight">\(\beta\)</span>. (shape: <code class="docutils literal notranslate"><span class="pre">&lt;see</span> <span class="pre">above&gt;</span></code>)</p></li>
<li><p>gamma (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Trainable scaling factor <span class="math notranslate nohighlight">\(\gamma\)</span>. (shape: <code class="docutils literal notranslate"><span class="pre">&lt;see</span> <span class="pre">above&gt;</span></code>)</p></li>
<li><p>mean (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Moving average of batch mean. (shape: <code class="docutils literal notranslate"><span class="pre">&lt;see</span> <span class="pre">above&gt;</span></code>)</p></li>
<li><p>var (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Moving average of batch variance. (shape: <code class="docutils literal notranslate"><span class="pre">&lt;see</span> <span class="pre">above&gt;</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">sync_batch_normalization</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.mean_subtraction">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">mean_subtraction</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">base_axis=1</em>, <em class="sig-param">update_running_mean=True</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#mean_subtraction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.mean_subtraction" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean subtraction layer.</p>
<p>It subtracts the mean of the elements of the input array,
and normalizes it to <span class="math notranslate nohighlight">\(0\)</span>. Preprocessing arrays with this function has the effect of improving accuracy
in various tasks such as image classification.</p>
<p>At training time, this function is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{lcl}
\mu &amp;=&amp; \frac{1}{M} \sum x_i \\
y_i &amp;=&amp; x_i - \mu
\end{array}\end{split}\]</div>
<p>At testing time, the mean values used are those that were computed during training by moving average.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The backward performs an approximated differentiation that takes into account only the latest mini-batch.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – N-D array of input.</p></li>
<li><p><strong>base_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Base axis of Mean Subtraction operation. Dimensions up to base_axis is treated as sample dimension.</p></li>
<li><p><strong>update_running_mean</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the running mean will not be updated.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – dummy parameter. This argument dose not affect anything.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-D array.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable">Variable</a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;mean_subtraction&quot;</span></code>;</p>
<ul class="simple">
<li><p>mean (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Moving average. (shape: <code class="docutils literal notranslate"><span class="pre">inp.shape[base_axis:]</span></code>)</p></li>
<li><p>t (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Minibatch counter used in forward pass. (shape: <code class="docutils literal notranslate"><span class="pre">(1,)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">mean_subtraction</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.layer_normalization">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">layer_normalization</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">batch_axis=0</em>, <em class="sig-param">eps=1e-05</em>, <em class="sig-param">output_stat=False</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">param_init=None</em>, <em class="sig-param">no_scale=False</em>, <em class="sig-param">no_bias=False</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#layer_normalization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.layer_normalization" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Layer Normalization over an input variable, which is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{eqnarray}
  \mu^l &amp;=&amp; \frac{1}{H} \sum_{i=1}^{H} x_i^l \\
  \sigma^l &amp;=&amp; \sqrt{\frac{1}{H} \sum_{i=1}^{H} \left(x_i^l - \mu^l\right)^2} \\
  y &amp;=&amp; \frac{x - \mu^l}{\sigma^l + \epsilon} \gamma + \beta
\end{eqnarray}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are input and output variable,
<span class="math notranslate nohighlight">\(\mu^l\)</span> and <span class="math notranslate nohighlight">\(\sigma^l\)</span> are the mean and std of each layer along batch axis,
and <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are trainable parameter.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unlike other normalizations,
which applies scalar scale and bias for each entire channel/plane,
Layer Normalization applies per-element scale and bias.</p>
</div>
<p class="rubric">References</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1607.06450">Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton, Layer Normalization.</a></p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – An input variable.</p></li>
<li><p><strong>batch_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>repeated int</em>) – Axes mean and variance are taken.</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Tiny value to avoid zero division by std.</p></li>
<li><p><strong>output_stat</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – It <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, calculated mean and variance are also returned.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the beta and gamma will not be updated.</p></li>
<li><p><strong>param_init</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a>) – Parameter initializers can be set with a dict. A key of the dict must
be <code class="docutils literal notranslate"><span class="pre">'gamma'</span></code>, <code class="docutils literal notranslate"><span class="pre">'beta'</span></code>.
A value of the dict must be an <code class="xref py py-obj docutils literal notranslate"><span class="pre">Initializer</span></code>
or a <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>.
E.g. <code class="docutils literal notranslate"><span class="pre">{'gamma':</span> <span class="pre">np.ones(...)</span> <span class="pre">*</span> <span class="pre">2,</span> <span class="pre">'beta':</span> <span class="pre">ConstantInitializer(0)}</span></code>.</p></li>
<li><p><strong>no_scale</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the scale term is omitted.</p></li>
<li><p><strong>no_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the bias term is omitted.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Normalized output variable.
* <a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Variable</span></code></a>: Mean (if <a href="#id2"><span class="problematic" id="id3">``</span></a>output_stat=True`).
* <a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Variable</span></code></a>: Std (if <a href="#id4"><span class="problematic" id="id5">``</span></a>output_stat=True`)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><ul class="simple">
<li><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Variable</span></code></a></p></li>
</ul>
</p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;layer_normalization&quot;</span></code>;</p>
<ul class="simple">
<li><p>beta (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Trainable bias <span class="math notranslate nohighlight">\(\beta\)</span>. (shape: <code class="docutils literal notranslate"><span class="pre">&lt;see</span> <span class="pre">above&gt;</span></code>)</p></li>
<li><p>gamma (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Trainable scaling factor <span class="math notranslate nohighlight">\(\gamma\)</span>. (shape: <code class="docutils literal notranslate"><span class="pre">&lt;see</span> <span class="pre">above&gt;</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">layer_normalization</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.instance_normalization">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">instance_normalization</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">channel_axis=1</em>, <em class="sig-param">batch_axis=0</em>, <em class="sig-param">eps=1e-05</em>, <em class="sig-param">output_stat=False</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">param_init=None</em>, <em class="sig-param">no_scale=False</em>, <em class="sig-param">no_bias=False</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#instance_normalization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.instance_normalization" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Instance Normalization over an input variable, which is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{eqnarray}
  \mu^i &amp;=&amp; \frac{1}{H} \sum_{i=1}^{H} x_i^i \\
  \sigma^i &amp;=&amp; \sqrt{\frac{1}{H} \sum_{i=1}^{H} \left(x_i^i - \mu^i\right)^2} \\
  y &amp;=&amp; \frac{x - \mu^i}{\sigma^ + \epsilon} \gamma + \beta
\end{eqnarray}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are input and output variable,
<span class="math notranslate nohighlight">\(\mu^i\)</span> and <span class="math notranslate nohighlight">\(\sigma^i\)</span> are the mean and std of each instance which is separately calculated for each batch and channel,
and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are adaptive gains and biases.</p>
<p>If the input shape is [B, C, H, W] (= channel_axis=1, batch_axis=0), the shape of calculated mean and std are [B, C, 1, 1]</p>
<p class="rubric">References</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1607.08022">Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky, Instance Normalization: The Missing Ingredient for Fast Stylization.</a></p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – An input variable.</p></li>
<li><p><strong>channel_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>repeated int</em>) – Channel axes.</p></li>
<li><p><strong>batch_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>repeated int</em>) – Batch axes.</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Tiny value to avoid zero division by std.</p></li>
<li><p><strong>output_stat</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – It <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the batch statistics of mean and variance.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the beta and gamma will not be updated.</p></li>
<li><p><strong>param_init</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a>) – Parameter initializers can be set with a dict. A key of the dict must
be <code class="docutils literal notranslate"><span class="pre">'gamma'</span></code>, <code class="docutils literal notranslate"><span class="pre">'beta'</span></code>.
A value of the dict must be an <code class="xref py py-obj docutils literal notranslate"><span class="pre">Initializer</span></code>
or a <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>.
E.g. <code class="docutils literal notranslate"><span class="pre">{'gamma':</span> <span class="pre">np.ones(...)</span> <span class="pre">*</span> <span class="pre">2,</span> <span class="pre">'beta':</span> <span class="pre">ConstantInitializer(0)}</span></code>.</p></li>
<li><p><strong>no_scale</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the scale term is omitted.</p></li>
<li><p><strong>no_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the bias term is omitted.</p></li>
<li><p><strong>Returns</strong> – <ul>
<li><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Variable</span></code></a>: Normalized output variable.</p></li>
<li><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Variable</span></code></a>: Mean (if <a href="#id6"><span class="problematic" id="id7">``</span></a>output_stat=True`)</p></li>
<li><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Variable</span></code></a>: Std (if <a href="#id8"><span class="problematic" id="id9">``</span></a>output_stat=True`)</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;instance_normalization&quot;</span></code>;</p>
<ul class="simple">
<li><p>beta (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Trainable bias <span class="math notranslate nohighlight">\(\beta\)</span>. (shape: <code class="docutils literal notranslate"><span class="pre">&lt;see</span> <span class="pre">above&gt;</span></code>)</p></li>
<li><p>gamma (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Trainable scaling factor <span class="math notranslate nohighlight">\(\gamma\)</span>. (shape: <code class="docutils literal notranslate"><span class="pre">&lt;see</span> <span class="pre">above&gt;</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">instance_normalization</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.group_normalization">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">group_normalization</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">num_groups</em>, <em class="sig-param">channel_axis=1</em>, <em class="sig-param">batch_axis=0</em>, <em class="sig-param">eps=1e-05</em>, <em class="sig-param">output_stat=False</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">param_init=None</em>, <em class="sig-param">no_scale=False</em>, <em class="sig-param">no_bias=False</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#group_normalization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.group_normalization" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Group Normalization over an input tensor, which is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{eqnarray}
  \mu^g &amp;=&amp; \frac{1}{H} \sum_{i=1}^{H} x_i^g \\
  \sigma^g &amp;=&amp; \sqrt{\frac{1}{H} \sum_{i=1}^{H} \left(x_i^g - \mu^g\right)^2} \\
  y &amp;=&amp; \frac{x - \mu^g}{\sigma^g + \epsilon} \gamma + \beta
\end{eqnarray}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are input and output variable,
<span class="math notranslate nohighlight">\(\mu^g\)</span> and <span class="math notranslate nohighlight">\(\sigma^g\)</span> are the mean and std of each group which contains <code class="xref any docutils literal notranslate"><span class="pre">num_channels</span> <span class="pre">/</span> <span class="pre">num_groups</span></code> channels,
and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are adaptive gains and biases.</p>
<p>The input channels, specified by <code class="xref py py-attr docutils literal notranslate"><span class="pre">channel_axis</span></code>, are separeted into <code class="xref py py-attr docutils literal notranslate"><span class="pre">num_groups</span></code> groups,
and the mean and std are calculated over the each group.
For example, if the input shape is [B, C, H, W] (= channel_axis=1, batch_axis=0),
an input variable is once reshaped to [B, num_groups, C / num_groups, H, W]
and standardize by its mean and std whose shapes are [B, num_groups, C / num_groups, 1, 1].
Before returning, an output variable is reshaped again to the original input shape (= [B, C, H, W] in the case above).</p>
<p class="rubric">References</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1803.08494">Yuxin Wu, Kaiming He, Group Normalization.</a></p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – An input variable.</p></li>
<li><p><strong>num_groups</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – A number of groups. The channel dim of ‘x’ must be integer multiple of <code class="xref any docutils literal notranslate"><span class="pre">num_groups</span></code>.</p></li>
<li><p><strong>channel_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Channel axis.</p></li>
<li><p><strong>batch_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>repeated int</em>) – Axes mean and variance are taken.</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Tiny value to avoid zero division by std.</p></li>
<li><p><strong>output_stat</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – It true, the batch statistics of mean and variance.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the beta and gamma will not be updated.</p></li>
<li><p><strong>param_init</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a>) – Parameter initializers can be set with a dict. A key of the dict must
be <code class="docutils literal notranslate"><span class="pre">'gamma'</span></code>, <code class="docutils literal notranslate"><span class="pre">'beta'</span></code>.
A value of the dict must be an <code class="xref py py-obj docutils literal notranslate"><span class="pre">Initializer</span></code>
or a <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>.
E.g. <code class="docutils literal notranslate"><span class="pre">{'gamma':</span> <span class="pre">np.ones(...)</span> <span class="pre">*</span> <span class="pre">2,</span> <span class="pre">'beta':</span> <span class="pre">ConstantInitializer(0)}</span></code>.</p></li>
<li><p><strong>no_scale</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the scale term is omitted.</p></li>
<li><p><strong>no_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the bias term is omitted.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Normalized output variable.
* <a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Variable</span></code></a>: Mean (if <a href="#id10"><span class="problematic" id="id11">``</span></a>output_stat=True`)
* <a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Variable</span></code></a>: Std (if <a href="#id12"><span class="problematic" id="id13">``</span></a>output_stat=True`)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><ul class="simple">
<li><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Variable</span></code></a></p></li>
</ul>
</p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;group_normalization&quot;</span></code>;</p>
<ul class="simple">
<li><p>beta (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Trainable bias <span class="math notranslate nohighlight">\(\beta\)</span>. (shape: <code class="docutils literal notranslate"><span class="pre">&lt;see</span> <span class="pre">above&gt;</span></code>)</p></li>
<li><p>gamma (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Trainable scaling factor <span class="math notranslate nohighlight">\(\gamma\)</span>. (shape: <code class="docutils literal notranslate"><span class="pre">&lt;see</span> <span class="pre">above&gt;</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">group_normalization</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.rnn">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">rnn</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">h</em>, <em class="sig-param">w0_init=None</em>, <em class="sig-param">w_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">num_layers=1</em>, <em class="sig-param">nonlinearity='tanh'</em>, <em class="sig-param">dropout=0.0</em>, <em class="sig-param">bidirectional=False</em>, <em class="sig-param">training=True</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#rnn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.rnn" title="Permalink to this definition">¶</a></dt>
<dd><p>N-Step RNN (recurrent neural networks).</p>
<p>N-Step RNN function implements Elman RNN with nonlineraity to input sequence.
N-Step RNN function is defined as following:</p>
<div class="math notranslate nohighlight">
\[h_t = \tanh(w_{ih}x_t+b_{ih}+w_{hh}h_{(t-1)}).\]</div>
<p>We use the following notations to describe the inputs and outputs below.
<span class="math notranslate nohighlight">\(T\)</span>: sequcne length, <span class="math notranslate nohighlight">\(B\)</span>: batch size, <span class="math notranslate nohighlight">\(I\)</span>: input size, <span class="math notranslate nohighlight">\(L\)</span>: number of layers, <span class="math notranslate nohighlight">\(D\)</span>: number of directions, can be either 1 or 2, <span class="math notranslate nohighlight">\(H\)</span>: hidden size.</p>
<p class="rubric">References</p>
<p>Jeffrey L. Elman. “Finding Structure in Time.”
Cognitive Science. 1990.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape <span class="math notranslate nohighlight">\((T, B, I)\)</span>.</p></li>
<li><p><strong>h</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape <span class="math notranslate nohighlight">\((L, D, B, H)\)</span>.</p></li>
<li><p><strong>w0_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>, optional) – Initializer for weight at the first layer. Shape is <span class="math notranslate nohighlight">\((D, H, I + H)\)</span>.</p></li>
<li><p><strong>w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>, optional) – Initializer for weights at the second layer and up. Shape is <span class="math notranslate nohighlight">\((L-1, D, H, D*H + H)\)</span>.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>, optional) – Initializer for bias. Shape is <span class="math notranslate nohighlight">\((L, D, H)\)</span>.</p></li>
<li><p><strong>num_layers</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><em>optional</em>) – Number of layers in the network. If set to 1, only the weights for the first layer will be invoked. Default is 1.</p></li>
<li><p><strong>nonlinearity</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>, </em><em>optional</em>) – Type of nonlinearity applied to input sequcne. Must be either tanh or relu. Default is tanh.</p></li>
<li><p><strong>dropout</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>, </em><em>optional</em>) – Dropout ratio applied to parameters. Default is 0.0.</p></li>
<li><p><strong>bidirectional</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, bidirectional computation will be performed in each layer. Default is False.</p></li>
<li><p><strong>training</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>optional</em>) – Backpropagation will be performed only when it is true. Default is True.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>optional</em>) – Specify whether to include the bias term.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output <span class="math notranslate nohighlight">\(y\)</span> with shape <span class="math notranslate nohighlight">\((T, B, D * H)\)</span>
~nnabla.Variable: Output <span class="math notranslate nohighlight">\(h_n\)</span> with shape <span class="math notranslate nohighlight">\((L, D, B, H)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable">Variable</a></p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">))</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">num_directions</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>
<span class="n">y</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
</pre></div>
</div>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;rnn&quot;</span></code>;</p>
<ul class="simple">
<li><p>weight_l0 (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Filter weights at 0-th layer. (shape: <code class="docutils literal notranslate"><span class="pre">(D,</span> <span class="pre">H,</span> <span class="pre">I</span> <span class="pre">+</span> <span class="pre">H)</span></code>)</p></li>
<li><p>weight (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Filter weights at 1-st layer and above. (shape: <code class="docutils literal notranslate"><span class="pre">(L-1,</span> <span class="pre">D,</span> <span class="pre">H,</span> <span class="pre">DH</span> <span class="pre">+</span> <span class="pre">H)</span></code>)</p></li>
<li><p>bias (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Biases. (shape: <code class="docutils literal notranslate"><span class="pre">(L,</span> <span class="pre">D,</span> <span class="pre">H)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.lstm">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">lstm</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">h</em>, <em class="sig-param">c</em>, <em class="sig-param">w0_init=None</em>, <em class="sig-param">w_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">num_layers=1</em>, <em class="sig-param">dropout=0.0</em>, <em class="sig-param">bidirectional=False</em>, <em class="sig-param">training=True</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#lstm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.lstm" title="Permalink to this definition">¶</a></dt>
<dd><p>LSTM (long short-term memory).</p>
<p>Long Short-Term Memory, or LSTM, is a building block for recurrent neural networks (RNN) layers.
LSTM unit consists of a cell and input, output, forget gates whose functions are defined as following:</p>
<div class="math notranslate nohighlight">
\[\begin{split}f_t&amp;&amp;=\sigma(W_fx_t+U_fh_{t-1}+b_f) \\
i_t&amp;&amp;=\sigma(W_ix_t+U_ih_{t-1}+b_i) \\
o_t&amp;&amp;=\sigma(W_ox_t+U_oh_{t-1}+b_o) \\
c_t&amp;&amp;=f_t\odot c_{t-1}+i_t\odot\tanh(W_cx_t+U_ch_{t-1}+b_c) \\
h_t&amp;&amp;=o_t\odot\tanh(c_t).\end{split}\]</div>
<p>We use the following notations to describe the inputs and outputs below.
<span class="math notranslate nohighlight">\(T\)</span>: sequcne length, <span class="math notranslate nohighlight">\(B\)</span>: batch size, <span class="math notranslate nohighlight">\(I\)</span>: input size, <span class="math notranslate nohighlight">\(L\)</span>: number of layers, <span class="math notranslate nohighlight">\(D\)</span>: number of directions, can be either 1 or 2, <span class="math notranslate nohighlight">\(H\)</span>: hidden size.</p>
<p class="rubric">References</p>
<p>S. Hochreiter, and J. Schmidhuber. “Long Short-Term Memory.”
Neural Computation. 1997.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape <span class="math notranslate nohighlight">\((T, B, I)\)</span>.</p></li>
<li><p><strong>h</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape <span class="math notranslate nohighlight">\((L, D, B, H)\)</span>.</p></li>
<li><p><strong>c</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape <span class="math notranslate nohighlight">\((L, D, B, H)\)</span> .</p></li>
<li><p><strong>w0_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>, optional) – Initializer for weight at the first layer. Shape is <span class="math notranslate nohighlight">\((D, 4, H, I + H)\)</span>.</p></li>
<li><p><strong>w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>, optional) – Initializer for weights at the second layer and up. Shape is <span class="math notranslate nohighlight">\((L-1, D, 4, H, D * H + H)\)</span>.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>, optional) – Initializer for bias. Shape is <span class="math notranslate nohighlight">\((L, D, 4, H)\)</span>.</p></li>
<li><p><strong>num_layers</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><em>optional</em>) – Number of layers in the network. If set to 1, only the weights for the first layer will be invoked. Default is 1.</p></li>
<li><p><strong>dropout</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>, </em><em>optional</em>) – Dropout ratio applied to parameters. Default is 0.0.</p></li>
<li><p><strong>bidirectional</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, bidirectional computation will be performed in each layer. Default is False.</p></li>
<li><p><strong>training</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>optional</em>) – Backpropagation will be performed only when it is true. Default is True.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>optional</em>) – Specify whether to include the bias term.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output <span class="math notranslate nohighlight">\(y\)</span> with shape <span class="math notranslate nohighlight">\((T, B, D * H)\)</span>
~nnabla.Variable: Output <span class="math notranslate nohighlight">\(h_n\)</span> with shape <span class="math notranslate nohighlight">\((L, D, B, H)\)</span>
~nnabla.Variable: Output <span class="math notranslate nohighlight">\(c_n\)</span> with shape <span class="math notranslate nohighlight">\((L, D, B, H)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable">Variable</a></p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">))</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">num_directions</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">num_directions</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>
<span class="n">y</span><span class="p">,</span> <span class="n">hn</span><span class="p">,</span> <span class="n">cn</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;lstm&quot;</span></code>;</p>
<ul class="simple">
<li><p>weight_l0 (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Filter weights at 0-th layer. (shape: <code class="docutils literal notranslate"><span class="pre">(D,</span> <span class="pre">4,</span> <span class="pre">H,</span> <span class="pre">I</span> <span class="pre">+</span> <span class="pre">H)</span></code>)</p></li>
<li><p>weight (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Filter weights at 1-st layer and above. (shape: <code class="docutils literal notranslate"><span class="pre">(L-1,</span> <span class="pre">D,</span> <span class="pre">4,</span> <span class="pre">H,</span> <span class="pre">DH</span> <span class="pre">+</span> <span class="pre">H)</span></code>)</p></li>
<li><p>bias (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Biases. (shape: <code class="docutils literal notranslate"><span class="pre">(L,</span> <span class="pre">D,</span> <span class="pre">4,</span> <span class="pre">H)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">lstm</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.gru">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">gru</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">h</em>, <em class="sig-param">w0_init=None</em>, <em class="sig-param">w_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">num_layers=1</em>, <em class="sig-param">dropout=0.0</em>, <em class="sig-param">bidirectional=False</em>, <em class="sig-param">training=True</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#gru"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.gru" title="Permalink to this definition">¶</a></dt>
<dd><p>GRU (gated recurrent units).</p>
<p>GRU is defined as following:</p>
<div class="math notranslate nohighlight">
\[\begin{split}r_t&amp;&amp;=\sigma(W_rx_t+U_rh_{t-1}+b_r) \\
z_t&amp;&amp;=\sigma(W_zx_t+U_zh_{t-1}+b_z) \\
n_t&amp;&amp;=\tanh(W_nx_t+b_{in}+r_n \odot (U_nh_{t-1}+b_{hn})) \\
h_t&amp;&amp;=(1-z_t) \odot n_t+z_t \odot h_{t-1}.\end{split}\]</div>
<p>We use the following notations to describe the inputs and outputs below.
<span class="math notranslate nohighlight">\(T\)</span>: sequcne length, <span class="math notranslate nohighlight">\(B\)</span>: batch size, <span class="math notranslate nohighlight">\(I\)</span>: input size, <span class="math notranslate nohighlight">\(L\)</span>: number of layers, <span class="math notranslate nohighlight">\(D\)</span>: number of directions, can be either 1 or 2, <span class="math notranslate nohighlight">\(H\)</span>: hidden size.</p>
<p class="rubric">References</p>
<p>K. Cho et al. “Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation.”
Empirical Methods in Natural Language Processing. 2014.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape <span class="math notranslate nohighlight">\((T, B, I)\)</span>.</p></li>
<li><p><strong>h</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape <span class="math notranslate nohighlight">\((L, D, B, H)\)</span>.</p></li>
<li><p><strong>w0_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>, optional) – Initializer for weight at the first layer. Shape is <span class="math notranslate nohighlight">\((D, 3, H, I + H)\)</span>.</p></li>
<li><p><strong>w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>, optional) – Initializer for weights at the second layer and up. Shape is <span class="math notranslate nohighlight">\((L-1, D, 3, H, D * H + H)\)</span>.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>, optional) – Initializer for bias. Shape is <span class="math notranslate nohighlight">\((L, D, 4, H)\)</span>.</p></li>
<li><p><strong>num_layers</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><em>optional</em>) – Number of layers in the network. If set to 1, only the weights for the first layer will be invoked. Default is 1.</p></li>
<li><p><strong>dropout</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>, </em><em>optional</em>) – Dropout ratio applied to parameters. Default is 0.0.</p></li>
<li><p><strong>bidirectional</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, bidirectional computation will be performed in each layer. Default is False.</p></li>
<li><p><strong>training</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>optional</em>) – Backpropagation will be performed only when it is true. Default is True.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>optional</em>) – Specify whether to include the bias term.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output <span class="math notranslate nohighlight">\(y\)</span> with shape <span class="math notranslate nohighlight">\((T, B, D * H)\)</span>
~nnabla.Variable: Output <span class="math notranslate nohighlight">\(h_n\)</span> with shape <span class="math notranslate nohighlight">\((L, D, B, H)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable">Variable</a></p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">))</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">num_directions</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>
<span class="n">y</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
</pre></div>
</div>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;gru&quot;</span></code>;</p>
<ul class="simple">
<li><p>weight_l0 (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Filter weights at 0-th layer. (shape: <code class="docutils literal notranslate"><span class="pre">(D,</span> <span class="pre">3,</span> <span class="pre">H,</span> <span class="pre">I</span> <span class="pre">+</span> <span class="pre">H)</span></code>)</p></li>
<li><p>weight (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Filter weights at 1-st layer and above. (shape: <code class="docutils literal notranslate"><span class="pre">(L-1,</span> <span class="pre">D,</span> <span class="pre">3,</span> <span class="pre">H,</span> <span class="pre">DH</span> <span class="pre">+</span> <span class="pre">H)</span></code>)</p></li>
<li><p>bias (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Biases. (shape: <code class="docutils literal notranslate"><span class="pre">(L,</span> <span class="pre">D,</span> <span class="pre">4,</span> <span class="pre">H)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">gru</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.embed">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">embed</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">n_inputs</em>, <em class="sig-param">n_features</em>, <em class="sig-param">initializer=None</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">apply_w=None</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#embed"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.embed" title="Permalink to this definition">¶</a></dt>
<dd><p>Embed.</p>
<p>Embed slices a matrix/tensor with indexing array/tensor. Weights are initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range of <span class="math notranslate nohighlight">\(-\sqrt{3}\)</span> and <span class="math notranslate nohighlight">\(\sqrt{3}\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – [Integer] Indices with shape <span class="math notranslate nohighlight">\((I_0, ..., I_N)\)</span></p></li>
<li><p><strong>n_inputs</strong> – number of possible inputs, words or vocabraries</p></li>
<li><p><strong>n_features</strong> – number of embedding features</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the embedding weight matrix
will not be updated.</p></li>
<li><p><strong>apply_w</strong> (<em>function</em>) – Lambda, function, or callable object applied to the weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output with shape <span class="math notranslate nohighlight">\((I_0, ..., I_N, W_1, ..., W_M)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable">Variable</a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;embed&quot;</span></code>;</p>
<ul class="simple">
<li><p>W (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Embedding matrix. (shape: <code class="docutils literal notranslate"><span class="pre">(n_inputs,</span> <span class="pre">n_features)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">embed</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.prelu">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">prelu</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">base_axis=1</em>, <em class="sig-param">shared=True</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">slope_init=None</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#prelu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.prelu" title="Permalink to this definition">¶</a></dt>
<dd><p>Parametrized Rectified Linear Unit function defined as</p>
<div class="math notranslate nohighlight">
\[y_i = \max(0, x_i) + w_i \min(0, x_i)\]</div>
<p>where negative slope <span class="math notranslate nohighlight">\(w\)</span> is learned and can vary across channels (an
axis specified with base_axis). Weights are initialized with <span class="math notranslate nohighlight">\(-1\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – N-D array as input</p></li>
<li><p><strong>base_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Dimensions up to base_axis is treated as sample dimension.</p></li>
<li><p><strong>shared</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Use shared weight value or not</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the negative slope values
will not be updated.</p></li>
<li><p><strong>slope_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer of negative slopes. By default, they are initialized with <code class="xref any docutils literal notranslate"><span class="pre">0.25</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-D array.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable">Variable</a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;prelu&quot;</span></code>;</p>
<ul class="simple">
<li><p>slope (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Negative slope. (shape: <code class="docutils literal notranslate"><span class="pre">tuple()</span> <span class="pre">if</span> <span class="pre">shared</span> <span class="pre">else</span> <span class="pre">(inp.shape[base_axis],)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">prelu</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.svd_affine">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">svd_affine</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">n_outmaps</em>, <em class="sig-param">r</em>, <em class="sig-param">base_axis=1</em>, <em class="sig-param">uv_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#svd_affine"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.svd_affine" title="Permalink to this definition">¶</a></dt>
<dd><p>SVD affine is a low rank approximation of the affine layer. It can
be seen as two consecutive affine layers with a bottleneck. It
computes:</p>
<div class="math notranslate nohighlight">
\[{\mathbf y} = {\mathbf U} {\mathbf V} {\mathbf x} + {\mathbf b}.\]</div>
<p>where <span class="math notranslate nohighlight">\({\mathbf x}, {\mathbf y}\)</span> are the inputs and
outputs respectively, and <span class="math notranslate nohighlight">\({\mathbf U}, {\mathbf V},
{\mathbf b}\)</span> are constants.</p>
<p>The weights <span class="math notranslate nohighlight">\({\mathbf U}\)</span> and <span class="math notranslate nohighlight">\({\mathbf V}\)</span> are
approximated with singular value decomposition (SVD) of the
original weight matrix <span class="math notranslate nohighlight">\({\mathbf W}\)</span> and by selecting the
<span class="math notranslate nohighlight">\({R}\)</span> dominant singular values and the corresponding
singular vectors. Therefore the low rank <span class="math notranslate nohighlight">\({R}\)</span> is the size
of the bottleneck.</p>
<p>If <code class="xref any docutils literal notranslate"><span class="pre">uv_init</span></code> is a numpy array, <span class="math notranslate nohighlight">\({\mathbf U}\)</span> and
<span class="math notranslate nohighlight">\({\mathbf V}\)</span> are computed such that <code class="xref any docutils literal notranslate"><span class="pre">uv_init</span></code> is
approximated by <span class="math notranslate nohighlight">\({\mathbf{UV}}\)</span>. If <code class="xref any docutils literal notranslate"><span class="pre">uv_init</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#None" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">None</span></code></a> or
an initializer, the product of <span class="math notranslate nohighlight">\({\mathbf U}\)</span> and
<span class="math notranslate nohighlight">\({\mathbf V}\)</span> approximates the random initialization.</p>
<p>If <span class="math notranslate nohighlight">\({\mathbf U}\)</span> and <span class="math notranslate nohighlight">\({\mathbf V}\)</span> exist in the context,
they take precedence over <code class="xref any docutils literal notranslate"><span class="pre">uv_init</span></code>.</p>
<p>Suppose the weight of the affine is of <span class="math notranslate nohighlight">\({I \times O}\)</span> and
the compression rate you want to specify is <span class="math notranslate nohighlight">\({CR}\)</span>, then you
set <span class="math notranslate nohighlight">\({R}\)</span> as</p>
<div class="math notranslate nohighlight">
\[R = \left\lfloor \frac{(1 - CR)OI}{O + I} \right\rfloor.\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape (<span class="math notranslate nohighlight">\(M_0
\times \ldots \times M_{B-1} \times D_B \times \ldots
\times D_N\)</span>). Dimensions before and after base_axis are
flattened as if it is a matrix.</p></li>
<li><p><strong>n_outmaps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><em>tuple</em></a>) – Number of output neurons per data.</p></li>
<li><p><strong>r</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – rank of the factorized layer (size of the bottleneck)</p></li>
<li><p><strong>base_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Dimensions up to <code class="xref any docutils literal notranslate"><span class="pre">base_axis</span></code> are treated as
the sample dimensions.</p></li>
<li><p><strong>uv_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for bias. By default, it is initialized with zeros if <code class="xref any docutils literal notranslate"><span class="pre">with_bias</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights
and biases will not be updated.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random generator for Initializer.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Specify whether to include the bias term.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><span class="math notranslate nohighlight">\((B + 1)\)</span>-D array.
(<span class="math notranslate nohighlight">\(M_0 \times \ldots \times M_{B-1} \times L\)</span>)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable">Variable</a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;svd_affine&quot;</span></code>;</p>
<ul class="simple">
<li><p>U (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : <span class="math notranslate nohighlight">\({\mathbf U}\)</span>. (shape: <code class="docutils literal notranslate"><span class="pre">(inmaps,</span> <span class="pre">r)</span></code>)</p></li>
<li><p>V (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : <span class="math notranslate nohighlight">\({\mathbf V}\)</span>. (shape: <code class="docutils literal notranslate"><span class="pre">(r,</span> <span class="pre">outmaps)</span></code>)</p></li>
<li><p>b (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Bias vector. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">svd_affine</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.svd_convolution">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">svd_convolution</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">outmaps</em>, <em class="sig-param">kernel</em>, <em class="sig-param">r</em>, <em class="sig-param">pad=None</em>, <em class="sig-param">stride=None</em>, <em class="sig-param">dilation=None</em>, <em class="sig-param">uv_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">base_axis=1</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#svd_convolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.svd_convolution" title="Permalink to this definition">¶</a></dt>
<dd><p>SVD convolution is a low rank approximation of the convolution
layer. It can be seen as a depth wise convolution followed by a
1x1 convolution.</p>
<p>The flattened kernels for the i-th input map are expressed by
their low rank approximation. The kernels for the i-th input
<span class="math notranslate nohighlight">\({\mathbf W_i}\)</span> are approximated with the singular value
decomposition (SVD) and by selecting the <span class="math notranslate nohighlight">\({R}\)</span> dominant
singular values and the corresponding singular vectors.</p>
<div class="math notranslate nohighlight">
\[{\mathbf W_{:,i,:}} ~ {\mathbf U_i} {\mathbf V_i}.\]</div>
<p><span class="math notranslate nohighlight">\({\mathbf U}\)</span> contains the weights of the depthwise
convolution with multiplier <span class="math notranslate nohighlight">\({R}\)</span> and <span class="math notranslate nohighlight">\({\mathbf V}\)</span>
contains the weights of the 1x1 convolution.</p>
<p>If <code class="xref any docutils literal notranslate"><span class="pre">uv_init</span></code> is a numpy array, <span class="math notranslate nohighlight">\({\mathbf U}\)</span> and
<span class="math notranslate nohighlight">\({\mathbf V}\)</span> are computed such that <code class="xref any docutils literal notranslate"><span class="pre">uv_init</span></code> is
approximated by <span class="math notranslate nohighlight">\({\mathbf{UV}}\)</span>. If <code class="xref any docutils literal notranslate"><span class="pre">uv_init</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#None" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">None</span></code></a> or
an initializer, the product of <span class="math notranslate nohighlight">\({\mathbf U}\)</span> and
<span class="math notranslate nohighlight">\({\mathbf V}\)</span> approximates the random initialization.</p>
<p>If <span class="math notranslate nohighlight">\({\mathbf U}\)</span> and <span class="math notranslate nohighlight">\({\mathbf V}\)</span> exist in the
context, they take precedence over <code class="xref any docutils literal notranslate"><span class="pre">uv_init</span></code>.</p>
<p>Suppose the kernel tensor of the convolution is of <span class="math notranslate nohighlight">\({O \times I \times K \times K}\)</span> and
the compression rate you want to specify is <span class="math notranslate nohighlight">\({CR}\)</span>, then you
set <span class="math notranslate nohighlight">\({R}\)</span> as</p>
<div class="math notranslate nohighlight">
\[R = \left\lfloor \frac{(1 - CR)OIK^2}{I(O + K^2)} \right\rfloor.\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – N-D array.</p></li>
<li><p><strong>outmaps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of convolution kernels (which is equal
to the number of output channels). For example, to apply
convolution on an input with 16 types of filters, specify
16.</p></li>
<li><p><strong>kernel</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><em>tuple</em></a>) – Convolution kernel size. For example,
to apply convolution on an image with a 3 (height) by 5
(width) two-dimensional kernel, specify (3, 5).</p></li>
<li><p><strong>r</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Rank of the factorized layer.</p></li>
<li><p><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><em>tuple</em></a>) – Padding sizes (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">int</span></code></a>) for dimensions.</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><em>tuple</em></a>) – Stride sizes (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">int</span></code></a>) for dimensions.</p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><em>tuple</em></a>) – Dilation sizes (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">int</span></code></a>) for dimensions.</p></li>
<li><p><strong>uv_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for bias. By default, it is initialized with zeros if <code class="xref any docutils literal notranslate"><span class="pre">with_bias</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>base_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Dimensions up to <code class="xref any docutils literal notranslate"><span class="pre">base_axis</span></code> are treated as the
sample dimensions.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and
biases will not be updated.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random generator for Initializer.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Specify whether to include the bias term.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><span class="math notranslate nohighlight">\((B + 1)\)</span>-D array.
(<span class="math notranslate nohighlight">\(M_0 \times \ldots \times M_{B-1} \times L\)</span>)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;svd_conv&quot;</span></code>;</p>
<ul class="simple">
<li><p>U (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Decomposed filter weights <span class="math notranslate nohighlight">\({\mathbf U}\)</span>. (shape: <code class="docutils literal notranslate"><span class="pre">(inmaps</span> <span class="pre">*</span> <span class="pre">r,</span> <span class="pre">*kernel)</span></code>)</p></li>
<li><p>V (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Decomposed filter weights <span class="math notranslate nohighlight">\({\mathbf V}\)</span>. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,</span> <span class="pre">inmaps</span> <span class="pre">*</span> <span class="pre">r,</span> <span class="pre">1,</span> <span class="pre">...)</span></code>)</p></li>
<li><p>b (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Bias vector. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">svd_convolution</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.cpd3_convolution">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">cpd3_convolution</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">outmaps</em>, <em class="sig-param">kernel</em>, <em class="sig-param">r</em>, <em class="sig-param">pad=None</em>, <em class="sig-param">stride=None</em>, <em class="sig-param">dilation=None</em>, <em class="sig-param">oik_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">base_axis=1</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">max_iter=500</em>, <em class="sig-param">stopping_criterion=1e-05</em>, <em class="sig-param">lambda_reg=0.0</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#cpd3_convolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.cpd3_convolution" title="Permalink to this definition">¶</a></dt>
<dd><p>CP convolution is a low rank approximation of a convolution layer. A 3D tensor containing the parameter is built by collapsing the N-D kernels into 1D, then the tensor is decomposed into three matrices. The decomposed layer can be seen as linear combinations of the input feature maps to <span class="math notranslate nohighlight">\({R}\)</span> feature maps followed by a depthwise convolution and followed by linear combinations of the feature maps to compute the output feature maps.</p>
<p>The CP decomposition allows to approximate the kernel tensor by <span class="math notranslate nohighlight">\({R}\)</span> rank-1 tensors of the form:</p>
<div class="math notranslate nohighlight">
\[\sum_{r=1}^{R} \lambda_r {\mathbf{o}^{(r)} \otimes \mathbf{i}^{(r)} \otimes \mathbf{k}^{(r)}},\]</div>
<p>where <span class="math notranslate nohighlight">\({\lambda}_r\)</span> is the normalization coefficient and <span class="math notranslate nohighlight">\({\otimes}\)</span> is the outer product.</p>
<p>If <code class="xref any docutils literal notranslate"><span class="pre">oik_init</span></code> is a numpy array, U and V are computed so that uv_init can be approximates from UV
If <code class="xref any docutils literal notranslate"><span class="pre">oik_init</span></code> is None or an initializer, the product of U and V approximate the randomly initialized array</p>
<p>If <code class="xref any docutils literal notranslate"><span class="pre">O</span></code>, <code class="xref any docutils literal notranslate"><span class="pre">I</span></code> and <code class="xref any docutils literal notranslate"><span class="pre">K</span></code> exist in context, they are used to initialize the layer and oik_init is not used.</p>
<p>Suppose the kernel tensor of the affine is of <span class="math notranslate nohighlight">\({I \times O}\)</span> and
the compression rate you want to specify is <span class="math notranslate nohighlight">\({CR}\)</span>, then you
set <span class="math notranslate nohighlight">\({R}\)</span> as</p>
<div class="math notranslate nohighlight">
\[R = \left\lfloor \frac{(1 - CR)OIK^2}{O + I + K^2} \right\rfloor.\]</div>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Lebedev, Vadim, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and Victor Lempitsky,  “Speeding-up convolutional neural networks using fine-tuned cp-decomposition.”, arXiv preprint arXiv:1412.6553 (2014).</p></li>
<li><p>Marcella Astrid, Seung-Ik Lee, “CP-decomposition with Tensor Power Method for Convolutional Neural Networks Compression”, BigComp 2017.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – N-D array.</p></li>
<li><p><strong>outmaps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of convolution kernels (which is equal to the number of output channels). For example, to apply convolution on an input with 16 types of filters, specify 16.</p></li>
<li><p><strong>kernel</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</p></li>
<li><p><strong>r</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – rank of the factorized layer</p></li>
<li><p><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Padding sizes for dimensions.</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Stride sizes for dimensions.</p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Dilation sizes for dimensions.</p></li>
<li><p><strong>oik_init</strong> (numpy array or <a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a>) – Initializer for weight. Initializer for weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for bias. It is initialized with zeros if <code class="xref any docutils literal notranslate"><span class="pre">with_bias</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>base_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Dimensions up to <code class="xref any docutils literal notranslate"><span class="pre">base_axis</span></code> are treated as the sample dimensions.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random generator for Initializer.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Specify whether to include the bias term.</p></li>
<li><p><strong>max_iter</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Max iteration of the ALS.</p></li>
<li><p><strong>stopping_criterion</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Threshold for stopping the ALS.
If the value is negative, the convergence check is ignored;
in other words, it may reduce the computation time.</p></li>
<li><p><strong>lambda_reg</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – regularization parameter for the ALS. Larger
lambda_reg means larger regularization.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><span class="math notranslate nohighlight">\((B + 1)\)</span>-D array. (<span class="math notranslate nohighlight">\(M_0 \times \ldots \times M_{B-1} \times L\)</span>)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;cpd3_conv&quot;</span></code>;</p>
<ul class="simple">
<li><p>I (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Decomposed filter weights <span class="math notranslate nohighlight">\({\mathbf I}\)</span>. (shape: <code class="docutils literal notranslate"><span class="pre">(r,</span> <span class="pre">inmaps,</span> <span class="pre">1,</span> <span class="pre">...)</span></code>)</p></li>
<li><p>K (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Decomposed filter weights <span class="math notranslate nohighlight">\({\mathbf K}\)</span>. (shape: <code class="docutils literal notranslate"><span class="pre">(r,</span> <span class="pre">*kernel)</span></code>)</p></li>
<li><p>O (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Decomposed filter weights <span class="math notranslate nohighlight">\({\mathbf O}\)</span>. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,</span> <span class="pre">r,</span> <span class="pre">1,</span> <span class="pre">...)</span></code>)</p></li>
<li><p>b (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Bias vector. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">cpd3_convolution</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.binary_connect_affine">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">binary_connect_affine</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">n_outmaps</em>, <em class="sig-param">base_axis=1</em>, <em class="sig-param">quantize_zero_to=1.0</em>, <em class="sig-param">w_init=None</em>, <em class="sig-param">wb_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#binary_connect_affine"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.binary_connect_affine" title="Permalink to this definition">¶</a></dt>
<dd><p>Binary Connect Affine, multiplier-less inner-product.</p>
<p>Binary Connect Affine is an affine function,
except the definition of the inner product is modified.
The input-output relation of this function is as follows:</p>
<div class="math notranslate nohighlight">
\[y_i = \sum_{i} sign(w_i) x_i.\]</div>
<p>Therefore <span class="math notranslate nohighlight">\(sign(w_i)\)</span> is either <span class="math notranslate nohighlight">\(1\)</span> or <span class="math notranslate nohighlight">\(-1\)</span> and the inner product
simplifies to addition.</p>
<p>This function should be used together with Batch Normalization.</p>
<p class="rubric">References</p>
<p>M. Courbariaux, Y. Bengio, and J.-P. David. “BinaryConnect:
Training Deep Neural Networks with binary weights during propagations.”
Advances in Neural Information Processing Systems. 2015.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>1) if you would like to share weights between some layers, please
make sure to share the standard, floating value weights (<code class="xref any docutils literal notranslate"><span class="pre">weight</span></code>)
and not the binarized weights (<code class="xref any docutils literal notranslate"><span class="pre">binary_weight</span></code>)</p>
<p>2) The weights and the binary weights become synced only after <code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code> is called,
and not after a call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code>.
To access the parameters of the network, remember to call <code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code> once before doing so, otherwise the
float weights and the binary weights will not be in sync.</p>
<p>3) Quantized values are stored as floating point number for <code class="xref any docutils literal notranslate"><span class="pre">binary_weight</span></code>,
since this function is only for simulation purposes.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape (<span class="math notranslate nohighlight">\(M_0 \times \ldots \times M_{B-1} \times D_B \times \ldots \times D_N\)</span>). Dimensions before and after base_axis are flattened as if it is a matrix.</p></li>
<li><p><strong>n_outmaps</strong> (int or <a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Number of output neurons per data.</p></li>
<li><p><strong>base_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Dimensions up to <code class="xref any docutils literal notranslate"><span class="pre">base_axis</span></code> are treated as the sample dimensions.</p></li>
<li><p><strong>quantize_zero_to</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Input value at zero is quantized to this value.</p></li>
<li><p><strong>w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>wb_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for binary weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for bias. By default, it is initialized with zeros if <code class="xref any docutils literal notranslate"><span class="pre">with_bias</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random generator for Initializer.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;bicon_affine&quot;</span></code>;</p>
<ul class="simple">
<li><p>W (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Weight matrix in floating type. (shape: <code class="docutils literal notranslate"><span class="pre">(inmaps,</span> <span class="pre">outmaps)</span></code>)</p></li>
<li><p>Wb (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Binarized weights. (shape: <code class="docutils literal notranslate"><span class="pre">(inmaps,</span> <span class="pre">outmaps)</span></code>)</p></li>
<li><p>b (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Bias vector. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">binary_connect_affine</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.binary_connect_convolution">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">binary_connect_convolution</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">outmaps</em>, <em class="sig-param">kernel</em>, <em class="sig-param">pad=None</em>, <em class="sig-param">stride=None</em>, <em class="sig-param">dilation=None</em>, <em class="sig-param">group=1</em>, <em class="sig-param">quantize_zero_to=1.0</em>, <em class="sig-param">w_init=None</em>, <em class="sig-param">wb_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">base_axis=1</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#binary_connect_convolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.binary_connect_convolution" title="Permalink to this definition">¶</a></dt>
<dd><p>Binary Connect Convolution, multiplier-less inner-product.</p>
<p>Binary Connect Convolution is the convolution function,
except the definition of the inner product is modified.
The input-output relation of this function is as follows:</p>
<div class="math notranslate nohighlight">
\[y_{n, a, b} = \sum_{m} \sum_{i} \sum_{j} sign(w_{n, m, i, j}) x_{m, a + i, b + j}.\]</div>
<p>Therefore <span class="math notranslate nohighlight">\(sign(w_i)\)</span> is either <span class="math notranslate nohighlight">\(1\)</span> or <span class="math notranslate nohighlight">\(-1\)</span> and the inner product
simplifies to addition.</p>
<p>This function should be used together with BatchNormalization.</p>
<p class="rubric">References</p>
<p>M. Courbariaux, Y. Bengio, and J.-P. David. “BinaryConnect:
Training Deep Neural Networks with binary weights during propagations.”
Advances in Neural Information Processing Systems. 2015.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>1) if you would like to share weights between some layers, please
make sure to share the standard, floating value weights (<code class="xref any docutils literal notranslate"><span class="pre">weight</span></code>)
and not the binarized weights (<code class="xref any docutils literal notranslate"><span class="pre">binary_weight</span></code>)</p>
<p>2) The weights and the binary weights become synced only after <code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code> is called,
and not after a call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code>.
To access the parameters of the network, remember to call <code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code> once before doing so, otherwise the
float weights and the binary weights will not be in sync.</p>
<p>3) Quantized values are stored as floating point number for <code class="xref any docutils literal notranslate"><span class="pre">binary_weight</span></code>,
since this function is only for simulation purposes.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – N-D array.</p></li>
<li><p><strong>outmaps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of convolution kernels (which is equal to the number of output channels). For example, to apply convolution on an input with 16 types of filters, specify 16.</p></li>
<li><p><strong>kernel</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</p></li>
<li><p><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Padding sizes for dimensions.</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Stride sizes for dimensions.</p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Dilation sizes for dimensions.</p></li>
<li><p><strong>group</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of groups of channels. This makes connections across channels sparser by grouping connections along map direction.</p></li>
<li><p><strong>quantize_zero_to</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Input value at zero is quantized to this value.</p></li>
<li><p><strong>w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>wb_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for binary weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for bias. By default, it is initialized with zeros if <code class="xref any docutils literal notranslate"><span class="pre">with_bias</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>base_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Dimensions up to <code class="xref any docutils literal notranslate"><span class="pre">base_axis</span></code> are treated as the sample dimensions.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random generator for Initializer.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Specify whether to include the bias term.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;bicon_conv&quot;</span></code>;</p>
<ul class="simple">
<li><p>W (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Filter weights in float. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,</span> <span class="pre">inmaps,</span> <span class="pre">*kernel)</span></code>)</p></li>
<li><p>Wb (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Binarized filter weights. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,</span> <span class="pre">inmaps,</span> <span class="pre">*kernel)</span></code>)</p></li>
<li><p>b (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Bias vector. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">binary_connect_convolution</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.binary_weight_affine">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">binary_weight_affine</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">n_outmaps</em>, <em class="sig-param">base_axis=1</em>, <em class="sig-param">quantize_zero_to=1.0</em>, <em class="sig-param">w_init=None</em>, <em class="sig-param">wb_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#binary_weight_affine"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.binary_weight_affine" title="Permalink to this definition">¶</a></dt>
<dd><p>Binary Weight Affine, multiplier-less inner-product with a scale factor.</p>
<p>Binary Weight Affine is the affine function, but the inner product
in this function is the following,</p>
<div class="math notranslate nohighlight">
\[y_j = \frac{1}{\|\mathbf{w}_j\|_{\ell_1}} \sum_{i} sign(w_{ji}) x_i\]</div>
<p>Therefore <span class="math notranslate nohighlight">\(sign(w_{ji})\)</span> is either <span class="math notranslate nohighlight">\(1\)</span> or <span class="math notranslate nohighlight">\(-1\)</span> and the inner product
simplifies to addition followed by scaling factor <span class="math notranslate nohighlight">\(\alpha = \frac{1}{\|\mathbf{w}_j\|_{\ell_1}}\)</span>.
The number of :<span class="math notranslate nohighlight">\(\alpha\)</span> is the outmaps of the affine function.</p>
<p class="rubric">References</p>
<p>Rastegari, Mohammad, et al. “XNOR-Net: ImageNet Classification Using
Binary Convolutional Neural Networks.” arXiv preprint
arXiv:1603.05279 (2016).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>1) if you would like to share weights between some layers, please
make sure to share the standard, floating value weights (<code class="xref any docutils literal notranslate"><span class="pre">weight</span></code>)
and not the binarized weights (<code class="xref any docutils literal notranslate"><span class="pre">binary_weight</span></code>)</p>
<p>2) The weights and the binary weights become synced only after <code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code> is called,
and not after a call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code>.
To access the parameters of the network, remember to call <code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code> once before doing so, otherwise the
float weights and the binary weights will not be in sync.</p>
<p>3) Quantized values are stored as floating point number for <code class="xref any docutils literal notranslate"><span class="pre">binary_weight</span></code>,
since this function is only for simulation purposes.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape (<span class="math notranslate nohighlight">\(M_0 \times \ldots \times M_{B-1} \times D_B \times \ldots \times D_N\)</span>). Dimensions before and after base_axis are flattened as if it was a matrix.</p></li>
<li><p><strong>n_outmaps</strong> (int or <a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Number of output neurons per data.</p></li>
<li><p><strong>base_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Dimensions up to <code class="xref any docutils literal notranslate"><span class="pre">base_axis</span></code> are treated as the sample dimensions.</p></li>
<li><p><strong>quantize_zero_to</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Input value at zero is quantized to this value.</p></li>
<li><p><strong>w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for the weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>wb_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for the binary weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for the bias. By defalut, it is initialized with zeros if <code class="xref any docutils literal notranslate"><span class="pre">with_bias</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weight and bias will not be updated.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random generator for Initializer.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Specify whether to include the bias term.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;bwn_affine&quot;</span></code>;</p>
<ul class="simple">
<li><p>W (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Weight matrix in floating type. (shape: <code class="docutils literal notranslate"><span class="pre">(inmaps,</span> <span class="pre">outmaps)</span></code>)</p></li>
<li><p>Wb (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Binarized weights. (shape: <code class="docutils literal notranslate"><span class="pre">(inmaps,</span> <span class="pre">outmaps)</span></code>)</p></li>
<li><p>alpha (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Scaling factor <span class="math notranslate nohighlight">\(\alpha\)</span>. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
<li><p>b (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Bias vector. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">binary_weight_affine</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.binary_weight_convolution">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">binary_weight_convolution</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">outmaps</em>, <em class="sig-param">kernel</em>, <em class="sig-param">pad=None</em>, <em class="sig-param">stride=None</em>, <em class="sig-param">dilation=None</em>, <em class="sig-param">group=1</em>, <em class="sig-param">quantize_zero_to=1.0</em>, <em class="sig-param">w_init=None</em>, <em class="sig-param">wb_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">base_axis=1</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#binary_weight_convolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.binary_weight_convolution" title="Permalink to this definition">¶</a></dt>
<dd><p>Binary Weight Convolution, multiplier-less inner-product with a scale factor.</p>
<p>Binary Weight Convolution is the convolution function, but the
inner product in this function is the following,</p>
<div class="math notranslate nohighlight">
\[y_{n, a, b} = \frac{1}{\|\mathbf{w}_n\|_{\ell_1}} \sum_{m} \sum_{i} \sum_{j} sign(w_{n, m, i, j}) x_{m, a + i, b + j}.\]</div>
<p>Therefore <span class="math notranslate nohighlight">\(sign(w_{n, m, i, j})\)</span>  is either <span class="math notranslate nohighlight">\(1\)</span> or <span class="math notranslate nohighlight">\(-1\)</span> and the inner product
simplifies to addition followed by scaling factor <span class="math notranslate nohighlight">\(\alpha = \frac{1}{\|\mathbf{w}_n\|_{\ell_1}}\)</span>.
The number of <span class="math notranslate nohighlight">\(n\)</span> is the number of outmaps of the convolution
function.</p>
<p class="rubric">References</p>
<p>Rastegari, Mohammad, et al. “XNOR-Net: ImageNet Classification Using
Binary Convolutional Neural Networks.” arXiv preprint
arXiv:1603.05279 (2016).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>1) if you would like to share weights between some layers, please
make sure to share the standard, floating value weights (<code class="xref any docutils literal notranslate"><span class="pre">weight</span></code>)
and not the binarized weights (<code class="xref any docutils literal notranslate"><span class="pre">binary_weight</span></code>)</p>
<p>2) The weights and the binary weights become synced only after <code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code> is called,
and not after a call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code>.
To access the parameters of the network, remember to call <code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code> once before doing so, otherwise the
float weights and the binary weights will not be in sync.</p>
<p>3) Quantized values are stored as floating point number for <code class="xref any docutils literal notranslate"><span class="pre">binary_weight</span></code>,
since this function is only for simulation purposes.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – N-D array.</p></li>
<li><p><strong>outmaps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of convolution kernels (which is equal to the number of output channels). For example, to apply convolution on an input with 16 types of filters, specify 16.</p></li>
<li><p><strong>kernel</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</p></li>
<li><p><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Padding sizes for dimensions.</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Stride sizes for dimensions.</p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Dilation sizes for dimensions.</p></li>
<li><p><strong>group</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of groups of channels. This makes connections across channels sparser by grouping connections along map direction.</p></li>
<li><p><strong>quantize_zero_to</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Input value at zero is quantized to this value.</p></li>
<li><p><strong>w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>wb_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for binary weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for bias. By default, it is initialized with zeros if <code class="xref any docutils literal notranslate"><span class="pre">with_bias</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>base_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Dimensions up to <code class="xref any docutils literal notranslate"><span class="pre">base_axis</span></code> are treated as the sample dimensions.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random generator for Initializer.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Specify whether to include the bias term.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;bwn_conv&quot;</span></code>;</p>
<ul class="simple">
<li><p>W (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Filter weights in float. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,</span> <span class="pre">inmaps,</span> <span class="pre">*kernel)</span></code>)</p></li>
<li><p>Wb (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Binarized filter weights. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,</span> <span class="pre">inmaps,</span> <span class="pre">*kernel)</span></code>)</p></li>
<li><p>alpha (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Scaling factor <span class="math notranslate nohighlight">\(\alpha\)</span>. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
<li><p>b (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Bias vector. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">binary_weight_convolution</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.inq_affine">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">inq_affine</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">n_outmaps</em>, <em class="sig-param">base_axis=1</em>, <em class="sig-param">num_bits=4</em>, <em class="sig-param">inq_iterations=()</em>, <em class="sig-param">selection_algorithm='random'</em>, <em class="sig-param">seed=-1</em>, <em class="sig-param">w_init=None</em>, <em class="sig-param">i_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#inq_affine"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.inq_affine" title="Permalink to this definition">¶</a></dt>
<dd><p>Incremental Network Quantization Affine Layer</p>
<p>During training, the weights are sequentially quantized to power-of-two
values, which allows the training of a multiplierless network.</p>
<p>Using <code class="xref any docutils literal notranslate"><span class="pre">inq_iterations</span></code>, one can specify after how many forward passes
half of the learnable weights are fixed and quantized to powers-of-two.
After reaching the last value in <code class="xref any docutils literal notranslate"><span class="pre">inq_iterations</span></code>, all weights are fixed.</p>
<p>For more details, please refer to the reference.</p>
<p>Reference:
Zhou A, Yao A, Guo Y, Xu L, Chen Y. Incremental network quantization:
Towards lossless CNNs with low-precision weights.
&lt;<a class="reference external" href="https://arxiv.org/abs/1702.03044">https://arxiv.org/abs/1702.03044</a>&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape (<span class="math notranslate nohighlight">\(M_0 \times \ldots \times M_{B-1} \times D_B \times \ldots \times D_N\)</span>). Dimensions before and after base_axis are flattened as if it was a matrix.</p></li>
<li><p><strong>n_outmaps</strong> (int or <a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Number of output neurons per data.</p></li>
<li><p><strong>base_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Dimensions up to <code class="xref any docutils literal notranslate"><span class="pre">base_axis</span></code> are treated as the sample dimensions.</p></li>
<li><p><strong>quantize_zero_to</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Input value at zero is quantized to this value.</p></li>
<li><p><strong>num_bits</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of bits per weight. Value has to be larger than 1 as one bit is already used to code the value “0”</p></li>
<li><p><strong>inq_iterations</strong> (<em>tuple of int</em>) – Tuple of iteration numbers at which we fix half of the weights.</p></li>
<li><p><strong>selection_algorithm</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – Chooses algorithm that is used to decide which weights are fixed. (“largest_abs” … fix weights with largest absolute value, “random” … fix weights randomly)</p></li>
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Random seed for INQ algorithm</p></li>
<li><p><strong>w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>i_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for indicators (0 … learnable, 1 … fixed). By default, it is initialized with zeros.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for bias. By default, it is initialized with zeros if <code class="xref any docutils literal notranslate"><span class="pre">with_bias</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weight and bias will not be updated.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random generator for Initializer.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Specify whether to include the bias term.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;inq_affine&quot;</span></code>;</p>
<ul class="simple">
<li><p>W (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Weight matrix in floating type. (shape: <code class="docutils literal notranslate"><span class="pre">(inmaps,</span> <span class="pre">outmaps)</span></code>)</p></li>
<li><p>I (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Binary indicator matrix of fixed weights. (shape: <code class="docutils literal notranslate"><span class="pre">(inmaps,</span> <span class="pre">outmaps)</span></code>)</p></li>
<li><p>b (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Bias vector. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">inq_affine</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.inq_convolution">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">inq_convolution</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">outmaps</em>, <em class="sig-param">kernel</em>, <em class="sig-param">pad=None</em>, <em class="sig-param">stride=None</em>, <em class="sig-param">dilation=None</em>, <em class="sig-param">group=1</em>, <em class="sig-param">num_bits=4</em>, <em class="sig-param">inq_iterations=()</em>, <em class="sig-param">selection_algorithm='random'</em>, <em class="sig-param">seed=-1</em>, <em class="sig-param">w_init=None</em>, <em class="sig-param">i_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">base_axis=1</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#inq_convolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.inq_convolution" title="Permalink to this definition">¶</a></dt>
<dd><p>Incremental Network Quantization Convolution Layer</p>
<p>During training, the weights are sequentially quantized to power-of-two
values, which allows the training of a multiplierless network.</p>
<p>Using <code class="xref any docutils literal notranslate"><span class="pre">inq_iterations</span></code>, one can specify after how many forward passes
half of the learnable weights are fixed and quantized to powers-of-two.
After reaching the last value in <code class="xref any docutils literal notranslate"><span class="pre">inq_iterations</span></code>, all weights are fixed.</p>
<p>For more details, please refer to the reference.</p>
<p>Reference:
Zhou A, Yao A, Guo Y, Xu L, Chen Y. Incremental network quantization:
Towards lossless CNNs with low-precision weights.
&lt;<a class="reference external" href="https://arxiv.org/abs/1702.03044">https://arxiv.org/abs/1702.03044</a>&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape (<span class="math notranslate nohighlight">\(M_0 \times \ldots \times M_{B-1} \times D_B \times \ldots \times D_N\)</span>). Dimensions before and after base_axis are flattened as if it was a matrix.</p></li>
<li><p><strong>n_outmaps</strong> (int or <a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Number of output neurons per data.</p></li>
<li><p><strong>base_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Dimensions up to <code class="xref any docutils literal notranslate"><span class="pre">base_axis</span></code> are treated as the sample dimensions.</p></li>
<li><p><strong>num_bits</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of bits per weight. Value has to be larger than 1 as one bit is already used to code the value “0”</p></li>
<li><p><strong>inq_iterations</strong> (<em>tuple of int</em>) – Tuple of iteration numbers at which we fix half of the weights.</p></li>
<li><p><strong>selection_algorithm</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – Chooses algorithm that is used to decide which weights are fixed. (“largest_abs” … fix weights with largest absolute value, “random” … fix weights randomly)</p></li>
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Random seed for INQ algorithm</p></li>
<li><p><strong>w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for the weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>i_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for the indicators (0 … learnable, 1 … fixed). By default, it is initialized with zeros.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for the bias. By default, it is initialized with zeros if <code class="xref any docutils literal notranslate"><span class="pre">with_bias</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weight and bias will not be updated.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random generator for Initializer.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Specify whether to include the bias term.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;inq_conv&quot;</span></code>;</p>
<ul class="simple">
<li><p>W (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Filter weights in float. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,</span> <span class="pre">inmaps,</span> <span class="pre">*kernel)</span></code>)</p></li>
<li><p>I (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Binary indicator matrix of fixed weights. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,</span> <span class="pre">inmaps,</span> <span class="pre">*kernel)</span></code>)</p></li>
<li><p>b (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Bias vector. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">inq_convolution</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.fixed_point_quantized_affine">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">fixed_point_quantized_affine</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">n_outmaps</em>, <em class="sig-param">base_axis=1</em>, <em class="sig-param">w_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">quantize_w=True</em>, <em class="sig-param">sign_w=True</em>, <em class="sig-param">n_w=8</em>, <em class="sig-param">delta_w=0.0625</em>, <em class="sig-param">ste_fine_grained_w=True</em>, <em class="sig-param">quantize_b=True</em>, <em class="sig-param">sign_b=True</em>, <em class="sig-param">n_b=8</em>, <em class="sig-param">delta_b=0.0625</em>, <em class="sig-param">ste_fine_grained_b=True</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#fixed_point_quantized_affine"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.fixed_point_quantized_affine" title="Permalink to this definition">¶</a></dt>
<dd><p>Fixed-Point Quantized Affine.</p>
<p>Fixed-Point Quantized Affine is the affine function,
except the definition of the inner product is modified.
The input-output relation of this function is as follows:</p>
<div class="math notranslate nohighlight">
\[y_j = \sum_{i} Q(w_{ji}) x_i,\]</div>
<p>where <span class="math notranslate nohighlight">\(Q(w_{ji})\)</span> is the fixed-point quantization function.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>1) if you would like to share weights between some layers, please
make sure to share the standard, floating value weights (<code class="xref any docutils literal notranslate"><span class="pre">weight</span></code>)
and not the quantized weights (<code class="xref any docutils literal notranslate"><span class="pre">quantized</span> <span class="pre">weight</span></code>)</p>
<p>2) The weights and the quantized weights become synced only after <code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code> is called,
and not after a call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code>.
To access the parameters of the network, remember to call <code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code> once before doing so, otherwise the
float weights and the quantized weights will not be in sync.</p>
<p>3) CPU and GPU implementations now use float value for <code class="xref any docutils literal notranslate"><span class="pre">quantized</span> <span class="pre">weight</span></code>,
since this function is only for simulation purposes.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape (<span class="math notranslate nohighlight">\(M_0 \times \ldots \times M_{B-1} \times D_B \times \ldots \times D_N\)</span>). Dimensions before and after base_axis are flattened as if it is a matrix.</p></li>
<li><p><strong>n_outmaps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a> or <a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Number of output neurons per data.</p></li>
<li><p><strong>base_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Dimensions up to <code class="xref any docutils literal notranslate"><span class="pre">base_axis</span></code> are treated as the sample dimensions.</p></li>
<li><p><strong>w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for bias. By default, it is initialized with zeros if <code class="xref any docutils literal notranslate"><span class="pre">with_bias</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random generator for Initializer.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Specify whether to include the bias term.</p></li>
<li><p><strong>quantize_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Quantize weights if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>sign_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Use signed quantization if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>n_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Bit width used for weight.</p></li>
<li><p><strong>delta_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Step size for weight.</p></li>
<li><p><strong>ste_fine_grained_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – STE is fine-grained if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>quantize_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Quantize bias if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>n_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Bit width used for bias.</p></li>
<li><p><strong>delta_w</strong> – Step size for bias.</p></li>
<li><p><strong>ste_fine_grained_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – STE is fine-grained if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><span class="math notranslate nohighlight">\((B + 1)\)</span>-D array. (<span class="math notranslate nohighlight">\(M_0 \times \ldots \times M_{B-1} \times L\)</span>)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;fp_quantized_affine&quot;</span></code>;</p>
<ul class="simple">
<li><p>W (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Weight matrix in float. (shape: <code class="docutils literal notranslate"><span class="pre">(inmaps,</span> <span class="pre">outmaps)</span></code>)</p></li>
<li><p>b (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Bias vector in float. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
<li><p>W_q (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Quantized weights. (shape: <code class="docutils literal notranslate"><span class="pre">(inmaps,</span> <span class="pre">outmaps)</span></code>)</p></li>
<li><p>b_q (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Quantized biases. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">fixed_point_quantized_affine</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.fixed_point_quantized_convolution">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">fixed_point_quantized_convolution</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">outmaps</em>, <em class="sig-param">kernel</em>, <em class="sig-param">pad=None</em>, <em class="sig-param">stride=None</em>, <em class="sig-param">dilation=None</em>, <em class="sig-param">group=1</em>, <em class="sig-param">channel_last=False</em>, <em class="sig-param">w_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">base_axis=1</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">quantize_w=True</em>, <em class="sig-param">sign_w=True</em>, <em class="sig-param">n_w=8</em>, <em class="sig-param">delta_w=0.0625</em>, <em class="sig-param">ste_fine_grained_w=True</em>, <em class="sig-param">quantize_b=True</em>, <em class="sig-param">sign_b=True</em>, <em class="sig-param">n_b=8</em>, <em class="sig-param">delta_b=0.0625</em>, <em class="sig-param">ste_fine_grained_b=True</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#fixed_point_quantized_convolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.fixed_point_quantized_convolution" title="Permalink to this definition">¶</a></dt>
<dd><p>Fixed-Point Quantized Convolution.</p>
<p>Fixed-Point Quantized Convolution is the convolution function,
except the definition of the inner product is modified.
The input-output relation of this function is as follows:</p>
<div class="math notranslate nohighlight">
\[y_{n, a, b} = \sum_{m} \sum_{i} \sum_{j} Q(w_{n, m, i, j}) x_{m, a + i, b + j},\]</div>
<p>where <span class="math notranslate nohighlight">\(Q(w_{n, m, i, j})\)</span> is the fixed-point quantization function.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>1) if you would like to share weights between some layers, please
make sure to share the standard, floating value weights (<code class="xref any docutils literal notranslate"><span class="pre">weight</span></code>)
and not the quantized weights (<code class="xref any docutils literal notranslate"><span class="pre">quantized</span> <span class="pre">weight</span></code>)</p>
<p>2) The weights and the quantized weights become synced only after <code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code> is called,
and not after a call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code>.
To access the parameters of the network, remember to call <code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code> once before doing so, otherwise the
float weights and the quantized weights will not be in sync.</p>
<p>3) CPU and GPU implementations now use float value for <code class="xref any docutils literal notranslate"><span class="pre">quantized</span> <span class="pre">weight</span></code>,
since this function is only for simulation purposes.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – N-D array.</p></li>
<li><p><strong>outmaps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of convolution kernels (which is equal to the number of output channels). For example, to apply convolution on an input with 16 types of filters, specify 16.</p></li>
<li><p><strong>kernel</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</p></li>
<li><p><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Padding sizes for dimensions.</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Stride sizes for dimensions.</p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Dilation sizes for dimensions.</p></li>
<li><p><strong>group</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of groups of channels. This makes connections across channels more sparse by grouping connections along map direction.</p></li>
<li><p><strong>w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for bias. By default, it is initialized with zeros if <code class="xref any docutils literal notranslate"><span class="pre">with_bias</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>base_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Dimensions up to <code class="xref any docutils literal notranslate"><span class="pre">base_axis</span></code> are treated as the sample dimensions.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random generator for Initializer.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Specify whether to include the bias term.</p></li>
<li><p><strong>quantize_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Quantize weights if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>quantize_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Quantize bias if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>sign_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Use signed quantization if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>n_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Bit width used for weight.</p></li>
<li><p><strong>delta_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Step size for weight.</p></li>
<li><p><strong>ste_fine_grained_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – STE is fine-grained if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>quantize_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Quantize bias if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>n_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Bit width used for bias.</p></li>
<li><p><strong>delta_w</strong> – Step size for bias.</p></li>
<li><p><strong>ste_fine_grained_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – STE is fine-grained if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-D array.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;fp_quantized_conv&quot;</span></code>;</p>
<ul class="simple">
<li><p>W (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Filter weights in float. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,</span> <span class="pre">inmaps</span> <span class="pre">//</span> <span class="pre">group,</span> <span class="pre">*kernel)</span></code>)</p></li>
<li><p>b (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Bias vector in float. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
<li><p>W_q (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Quantized weights. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,</span> <span class="pre">inmaps</span> <span class="pre">//</span> <span class="pre">group,</span> <span class="pre">*kernel)</span></code>)</p></li>
<li><p>b_q (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Quantized biases. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">fixed_point_quantized_convolution</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.min_max_quantized_affine">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">min_max_quantized_affine</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">n_outmaps</em>, <em class="sig-param">base_axis=1</em>, <em class="sig-param">w_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">quantize_w=True</em>, <em class="sig-param">ql_min_w=0</em>, <em class="sig-param">ql_max_w=255</em>, <em class="sig-param">w_min_max=False</em>, <em class="sig-param">qr_min_w_init=None</em>, <em class="sig-param">qr_max_w_init=None</em>, <em class="sig-param">ste_fine_grained_w=True</em>, <em class="sig-param">quantize_b=True</em>, <em class="sig-param">ql_min_b=0</em>, <em class="sig-param">ql_max_b=255</em>, <em class="sig-param">b_min_max=False</em>, <em class="sig-param">qr_min_b_init=None</em>, <em class="sig-param">qr_max_b_init=None</em>, <em class="sig-param">ste_fine_grained_b=True</em>, <em class="sig-param">eps=0.01</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#min_max_quantized_affine"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.min_max_quantized_affine" title="Permalink to this definition">¶</a></dt>
<dd><p>Min-max Quantized Affine.</p>
<p>Min-max Quantized Affine is the affine function,
except the definition of the inner product is modified.
The input-output relation of this function is as follows:</p>
<div class="math notranslate nohighlight">
\[y_j = \sum_{i} Q(w_{ji}) x_i,\]</div>
<p>where <span class="math notranslate nohighlight">\(Q(w_{ji})\)</span> is the min-max quantization function.</p>
<p>In the min_max_quantized affine, the exponential moving average is not used. the min and max quantization
ranges are either the min-max of weights and bias or trained.</p>
<p>Notice that the min and max values of inputs are always used instead of the exponential moving average.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>1) if you would like to share weights between some layers, please
make sure to share the standard, floating value weights (<code class="xref any docutils literal notranslate"><span class="pre">weight</span></code>)
and not the quantized weights (<code class="xref any docutils literal notranslate"><span class="pre">quantized</span> <span class="pre">weight</span></code>)</p>
<p>2) The weights and the quantized weights become synced only after <code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code> is called,
and not after a call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code>.
To access the parameters of the network, remember to call <code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code> once before doing so, otherwise the
float weights and the quantized weights will not be in sync.</p>
<p>3) CPU and GPU implementations now use float value for <code class="xref any docutils literal notranslate"><span class="pre">quantized</span> <span class="pre">weight</span></code>,
since this function is only for simulation purposes.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape (<span class="math notranslate nohighlight">\(M_0 \times \ldots \times M_{B-1} \times D_B \times \ldots \times D_N\)</span>). Dimensions before and after base_axis are flattened as if it is a matrix.</p></li>
<li><p><strong>n_outmaps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a> or <a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Number of output neurons per data.</p></li>
<li><p><strong>base_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Dimensions up to <code class="xref any docutils literal notranslate"><span class="pre">base_axis</span></code> are treated as the sample dimensions.</p></li>
<li><p><strong>w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for bias. By default, it is initialized with zeros if <code class="xref any docutils literal notranslate"><span class="pre">with_bias</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random generator for Initializer.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Specify whether to include the bias term.</p></li>
<li><p><strong>quantize_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Quantize weights if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>ql_min_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>, or </em><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Minimum quantization level for weights. Default is 0.</p></li>
<li><p><strong>ql_max_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>, or </em><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Maximum quantization level for weights. Default is 255.</p></li>
<li><p><strong>w_min_max</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Use the min and max of weights to compute quantization ranges. Default is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#False" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">False</span></code></a>.</p></li>
<li><p><strong>qr_min_w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for the minimum quantization range, qr_min. Default is <a class="reference internal" href="#nnabla.initializer.ConstantInitializer" title="nnabla.initializer.ConstantInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.ConstantInitializer</span></code></a> (-2.0).</p></li>
<li><p><strong>qr_max_w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for the minimum quantization range, qr_min. Default is <a class="reference internal" href="#nnabla.initializer.ConstantInitializer" title="nnabla.initializer.ConstantInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.ConstantInitializer</span></code></a> (2.0).</p></li>
<li><p><strong>ste_fine_grained_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If true, STE is not 1, the {0, 1}-mask computed from the min-max is applied to the gradient in the backward; otherwise, STE is 1.</p></li>
<li><p><strong>quantize_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Quantize bias if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>ql_min_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>, or </em><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Minimum quantization level for bias. Default is 0.</p></li>
<li><p><strong>ql_max_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>, or </em><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Maximum quantization level for bias. Default is 255.</p></li>
<li><p><strong>b_min_max</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Use the min and max of bias to compute quantization ranges. Default is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#False" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">False</span></code></a>.</p></li>
<li><p><strong>qr_min_b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for the minimum quantization range, qr_min. Default is <a class="reference internal" href="#nnabla.initializer.ConstantInitializer" title="nnabla.initializer.ConstantInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.ConstantInitializer</span></code></a> (-6.0).</p></li>
<li><p><strong>qr_max_b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for the minimum quantization range, qr_min. Default is <a class="reference internal" href="#nnabla.initializer.ConstantInitializer" title="nnabla.initializer.ConstantInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.ConstantInitializer</span></code></a> (6.0).</p></li>
<li><p><strong>ste_fine_grained_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If true, STE is not 1, the {0, 1}-mask computed from the min-max is applied to the gradient in the backward; otherwise, STE is 1.</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Epsilon, or small value to ensure <span class="math notranslate nohighlight">\(qr_{max} - qr_{min}\)</span> must be greater
than the epsilon for both weights and bias.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><span class="math notranslate nohighlight">\((B + 1)\)</span>-D array. (<span class="math notranslate nohighlight">\(M_0 \times \ldots \times M_{B-1} \times L\)</span>)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;min_max_quantized_affine&quot;</span></code>;</p>
<ul class="simple">
<li><p>W (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Weight matrix in float. (shape: <code class="docutils literal notranslate"><span class="pre">(inmaps,</span> <span class="pre">outmaps)</span></code>)</p></li>
<li><p>b (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Bias vector in float. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
<li><p>W_q (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Quantized weights. (shape: <code class="docutils literal notranslate"><span class="pre">(inmaps,</span> <span class="pre">outmaps)</span></code>)</p></li>
<li><p>b_q (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Quantized biases. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
<li><p>qr_min (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Minimum quantization range. Minimum values of inputs or trainable range.. (shape: <code class="docutils literal notranslate"><span class="pre">ql_min.shape</span></code>)</p></li>
<li><p>qr_max (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Maximum quantization range. Maximum values of inputs or trainable range.. (shape: <code class="docutils literal notranslate"><span class="pre">ql_max.shape</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">min_max_quantized_affine</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.min_max_quantized_convolution">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">min_max_quantized_convolution</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">outmaps</em>, <em class="sig-param">kernel</em>, <em class="sig-param">pad=None</em>, <em class="sig-param">stride=None</em>, <em class="sig-param">dilation=None</em>, <em class="sig-param">group=1</em>, <em class="sig-param">channel_last=False</em>, <em class="sig-param">w_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">base_axis=1</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">quantize_w=True</em>, <em class="sig-param">ql_min_w=0</em>, <em class="sig-param">ql_max_w=255</em>, <em class="sig-param">w_min_max=False</em>, <em class="sig-param">qr_min_w_init=None</em>, <em class="sig-param">qr_max_w_init=None</em>, <em class="sig-param">ste_fine_grained_w=True</em>, <em class="sig-param">quantize_b=True</em>, <em class="sig-param">ql_min_b=0</em>, <em class="sig-param">ql_max_b=255</em>, <em class="sig-param">b_min_max=False</em>, <em class="sig-param">qr_min_b_init=None</em>, <em class="sig-param">qr_max_b_init=None</em>, <em class="sig-param">ste_fine_grained_b=True</em>, <em class="sig-param">eps=0.01</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#min_max_quantized_convolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.min_max_quantized_convolution" title="Permalink to this definition">¶</a></dt>
<dd><p>Min-max Quantized Convolution.</p>
<p>Min-max Quantized Convolution is the convolution function,
except the definition of the inner product is modified.
The input-output relation of this function is as follows:</p>
<div class="math notranslate nohighlight">
\[y_{n, a, b} = \sum_{m} \sum_{i} \sum_{j} Q(w_{n, m, i, j}) x_{m, a + i, b + j},\]</div>
<p>where <span class="math notranslate nohighlight">\(Q(w_{n, m, i, j})\)</span> is the min-max quantization function.</p>
<p>In the min_max_quantized convolution, the exponential moving average is not used.
the min and max quantization ranges are either the min-max of weights and bias or trained.</p>
<p>Notice that the min and max values of inputs are always used instead of the exponential moving average.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>1) if you would like to share weights between some layers, please
make sure to share the standard, floating value weights (<code class="xref any docutils literal notranslate"><span class="pre">weight</span></code>)
and not the quantized weights (<code class="xref any docutils literal notranslate"><span class="pre">quantized</span> <span class="pre">weight</span></code>)</p>
<p>2) The weights and the quantized weights become synced only after <code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code> is called,
and not after a call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code>.
To access the parameters of the network, remember to call <code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code> once before doing so, otherwise the
float weights and the quantized weights will not be in sync.</p>
<p>3) CPU and GPU implementations now use float value for <code class="xref any docutils literal notranslate"><span class="pre">quantized</span> <span class="pre">weight</span></code>,
since this function is only for simulation purposes.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – N-D array.</p></li>
<li><p><strong>outmaps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of convolution kernels (which is equal to the number of output channels). For example, to apply convolution on an input with 16 types of filters, specify 16.</p></li>
<li><p><strong>kernel</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</p></li>
<li><p><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Padding sizes for dimensions.</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Stride sizes for dimensions.</p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Dilation sizes for dimensions.</p></li>
<li><p><strong>group</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of groups of channels. This makes connections across channels more sparse by grouping connections along map direction.</p></li>
<li><p><strong>channel_last</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If True, the last dimension is considered as channel dimension, a.k.a. NHWC order.</p></li>
<li><p><strong>w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for bias. By default, it is initialized with zeros if <code class="xref any docutils literal notranslate"><span class="pre">with_bias</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>base_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Dimensions up to <code class="xref any docutils literal notranslate"><span class="pre">base_axis</span></code> are treated as the sample dimensions.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random generator for Initializer.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Specify whether to include the bias term.</p></li>
<li><p><strong>quantize_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Quantize weights if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>ql_min_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>, or </em><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Minimum quantization level for weights. Default is 0.</p></li>
<li><p><strong>ql_max_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>, or </em><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Maximum quantization level for weights. Default is 255.</p></li>
<li><p><strong>w_min_max</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Use the min and max of weights to compute quantization ranges. Default is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#False" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">False</span></code></a>.</p></li>
<li><p><strong>qr_min_w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for the minimum quantization range, qr_min. Default is <a class="reference internal" href="#nnabla.initializer.ConstantInitializer" title="nnabla.initializer.ConstantInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.ConstantInitializer</span></code></a> (-2.0).</p></li>
<li><p><strong>qr_max_w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for the maximum quantization rage, qr_max Default is <a class="reference internal" href="#nnabla.initializer.ConstantInitializer" title="nnabla.initializer.ConstantInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.ConstantInitializer</span></code></a> (2.0).</p></li>
<li><p><strong>ste_fine_grained_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If true, STE is not 1, the {0, 1}-mask computed from the min-max is applied to the gradient in the backward; otherwise, STE is 1.</p></li>
<li><p><strong>quantize_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Quantize bias if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>ql_min_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>, or </em><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Minimum quantization level for bias. Default is 0.</p></li>
<li><p><strong>ql_max_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>, or </em><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Maximum quantization level for bias. Default is 255.</p></li>
<li><p><strong>b_min_max</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Use the min and max of bias to compute quantization ranges. Default is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#False" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">False</span></code></a>.</p></li>
<li><p><strong>qr_min_b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for the minimum quantization range, qr_min. Default is <a class="reference internal" href="#nnabla.initializer.ConstantInitializer" title="nnabla.initializer.ConstantInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.ConstantInitializer</span></code></a> (-6.0).</p></li>
<li><p><strong>qr_max_b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for the maximum quantization rage, qr_max Default is <a class="reference internal" href="#nnabla.initializer.ConstantInitializer" title="nnabla.initializer.ConstantInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.ConstantInitializer</span></code></a> (6.0).</p></li>
<li><p><strong>ste_fine_grained_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If true, STE is not 1, the {0, 1}-mask computed from the min-max is applied to the gradient in the backward; otherwise, STE is 1.</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Epsilon, or small value to ensure <span class="math notranslate nohighlight">\(qr_{max} - qr_{min}\)</span> must be greater
than the epsilon for both weights and bias.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-D array.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;min_max_quantized_conv&quot;</span></code>;</p>
<ul class="simple">
<li><p>W (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Filter weights in float. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,</span> <span class="pre">inmaps</span> <span class="pre">//</span> <span class="pre">group,</span> <span class="pre">*kernel)</span></code>)</p></li>
<li><p>b (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Bias vector in float. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
<li><p>W_q (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Quantized weights. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,</span> <span class="pre">inmaps</span> <span class="pre">//</span> <span class="pre">group,</span> <span class="pre">*kernel)</span></code>)</p></li>
<li><p>b_q (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Quantized biases. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
<li><p>qr_min (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Minimum quantization range. Minimum values of inputs or trainable range.. (shape: <code class="docutils literal notranslate"><span class="pre">ql_min.shape</span></code>)</p></li>
<li><p>qr_max (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Maximum quantization range. Maximum values of inputs or trainable range.. (shape: <code class="docutils literal notranslate"><span class="pre">ql_max.shape</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">min_max_quantized_convolution</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.pow2_quantized_affine">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">pow2_quantized_affine</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">n_outmaps</em>, <em class="sig-param">base_axis=1</em>, <em class="sig-param">w_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">quantize_w=True</em>, <em class="sig-param">sign_w=True</em>, <em class="sig-param">with_zero_w=False</em>, <em class="sig-param">n_w=8</em>, <em class="sig-param">m_w=2</em>, <em class="sig-param">ste_fine_grained_w=True</em>, <em class="sig-param">quantize_b=True</em>, <em class="sig-param">sign_b=True</em>, <em class="sig-param">with_zero_b=False</em>, <em class="sig-param">n_b=8</em>, <em class="sig-param">m_b=2</em>, <em class="sig-param">ste_fine_grained_b=True</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#pow2_quantized_affine"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.pow2_quantized_affine" title="Permalink to this definition">¶</a></dt>
<dd><p>Pow2 Quantized Affine.</p>
<p>Pow2 Quantized Affine is the affine function,
except the definition of the inner product is modified.
The input-output relation of this function is as follows:</p>
<div class="math notranslate nohighlight">
\[y_j = \sum_{i} Q(w_{ji}) x_i,\]</div>
<p>where <span class="math notranslate nohighlight">\(Q(w_{ji})\)</span> is the power-of-2 quantization function.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>1) if you would like to share weights between some layers, please
make sure to share the standard, floating value weights (<code class="xref any docutils literal notranslate"><span class="pre">weight</span></code>)
and not the quantized weights (<code class="xref any docutils literal notranslate"><span class="pre">quantized</span> <span class="pre">weight</span></code>)</p>
<p>2) The weights and the quantized weights become synced only after <code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code> is called,
and not after a call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code>.
To access the parameters of the network, remember to call <code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code> once before doing so, otherwise the
float weights and the quantized weights will not be in sync.</p>
<p>3) Quantized values are stored as floating point number for <code class="xref any docutils literal notranslate"><span class="pre">quantized</span> <span class="pre">weight</span></code>,
since this function is only for simulation purposes.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape (<span class="math notranslate nohighlight">\(M_0 \times \ldots \times M_{B-1} \times D_B \times \ldots \times D_N\)</span>). Dimensions before and after base_axis are flattened as if it is a matrix.</p></li>
<li><p><strong>n_outmaps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a> or <a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Number of output neurons per data.</p></li>
<li><p><strong>base_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Dimensions up to <code class="xref any docutils literal notranslate"><span class="pre">base_axis</span></code> are treated as the sample dimensions.</p></li>
<li><p><strong>w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for bias. By default, it is initialized with zeros if <code class="xref any docutils literal notranslate"><span class="pre">with_bias</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random generator for Initializer.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Specify whether to include the bias term.</p></li>
<li><p><strong>quantize_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Quantize weights if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>sign_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Use signed quantization if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>with_zero_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Indicate using zero as a quantized value. Default is false.</p></li>
<li><p><strong>n_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Bit width used for weight.</p></li>
<li><p><strong>m_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – <span class="math notranslate nohighlight">\(2^m\)</span> is upper bound and <span class="math notranslate nohighlight">\(-2^m\)</span> is lower bound for weights. Default is 2.</p></li>
<li><p><strong>ste_fine_grained_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – STE is fine-grained if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>quantize_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Quantize bias if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>with_zero_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Indicate using zero as a quantized value. Default is false.</p></li>
<li><p><strong>n_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Bit width used for bias.</p></li>
<li><p><strong>m_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – <span class="math notranslate nohighlight">\(2^m\)</span> is upper bound and <span class="math notranslate nohighlight">\(-2^m\)</span> is lower bound for bias. Default is 2.</p></li>
<li><p><strong>ste_fine_grained_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – STE is fine-grained if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><span class="math notranslate nohighlight">\((B + 1)\)</span>-D array. (<span class="math notranslate nohighlight">\(M_0 \times \ldots \times M_{B-1} \times L\)</span>)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;pow2_quantized_affine&quot;</span></code>;</p>
<ul class="simple">
<li><p>W (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Weight matrix in float. (shape: <code class="docutils literal notranslate"><span class="pre">(inmaps,</span> <span class="pre">outmaps)</span></code>)</p></li>
<li><p>b (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Bias vector in float. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
<li><p>W_q (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Quantized weights. (shape: <code class="docutils literal notranslate"><span class="pre">(inmaps,</span> <span class="pre">outmaps)</span></code>)</p></li>
<li><p>b_q (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Quantized biases. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">pow2_quantized_affine</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.pow2_quantized_convolution">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">pow2_quantized_convolution</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">outmaps</em>, <em class="sig-param">kernel</em>, <em class="sig-param">pad=None</em>, <em class="sig-param">stride=None</em>, <em class="sig-param">dilation=None</em>, <em class="sig-param">group=1</em>, <em class="sig-param">w_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">base_axis=1</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">quantize_w=True</em>, <em class="sig-param">with_zero_w=False</em>, <em class="sig-param">sign_w=True</em>, <em class="sig-param">n_w=8</em>, <em class="sig-param">m_w=2</em>, <em class="sig-param">ste_fine_grained_w=True</em>, <em class="sig-param">quantize_b=True</em>, <em class="sig-param">with_zero_b=False</em>, <em class="sig-param">sign_b=True</em>, <em class="sig-param">n_b=8</em>, <em class="sig-param">m_b=2</em>, <em class="sig-param">ste_fine_grained_b=True</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#pow2_quantized_convolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.pow2_quantized_convolution" title="Permalink to this definition">¶</a></dt>
<dd><p>Pow2 Quantized Convolution.</p>
<p>Pow2 Quantized Convolution is the convolution function,
except the definition of the inner product is modified.
The input-output relation of this function is as follows:</p>
<div class="math notranslate nohighlight">
\[y_{n, a, b} = \sum_{m} \sum_{i} \sum_{j} Q(w_{n, m, i, j}) x_{m, a + i, b + j},\]</div>
<p>where <span class="math notranslate nohighlight">\(Q(w_{n, m, i, j})\)</span> is the power-of-2 quantization function.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>1) if you would like to share weights between some layers, please
make sure to share the standard, floating value weights (<code class="xref any docutils literal notranslate"><span class="pre">weight</span></code>)
and not the quantized weights (<code class="xref any docutils literal notranslate"><span class="pre">quantized</span> <span class="pre">weight</span></code>)</p>
<p>2) The weights and the quantized weights become synced only after <code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code> is called,
and not after a call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code>.
To access the parameters of the network, remember to call <code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code> once before doing so, otherwise the
float weights and the quantized weights will not be in sync.</p>
<p>3) Quantized values are stored as floating point number for <code class="xref any docutils literal notranslate"><span class="pre">quantized</span> <span class="pre">weight</span></code>,
since this function is only for simulation purposes.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – N-D array.</p></li>
<li><p><strong>outmaps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of convolution kernels (which is equal to the number of output channels). For example, to apply convolution on an input with 16 types of filters, specify 16.</p></li>
<li><p><strong>kernel</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</p></li>
<li><p><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Padding sizes for dimensions.</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Stride sizes for dimensions.</p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Dilation sizes for dimensions.</p></li>
<li><p><strong>group</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of groups of channels. This makes connections across channels more sparse by grouping connections along map direction.</p></li>
<li><p><strong>w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for bias. By default, it is initialized with zeros if <code class="xref any docutils literal notranslate"><span class="pre">with_bias</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>base_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Dimensions up to <code class="xref any docutils literal notranslate"><span class="pre">base_axis</span></code> are treated as the sample dimensions.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random generator for Initializer.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Specify whether to include the bias term.</p></li>
<li><p><strong>quantize_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Quantize weights if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>sign_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Use signed quantization if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>n_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Bit width used for weight.</p></li>
<li><p><strong>m_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – <span class="math notranslate nohighlight">\(2^m\)</span> is upper bound and <span class="math notranslate nohighlight">\(-2^m\)</span> is lower bound for weights. Default is 2.</p></li>
<li><p><strong>ste_fine_grained_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – STE is fine-grained if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>quantize_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Quantize bias if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>sign_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Use signed quantization if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>n_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Bit width used for bias.</p></li>
<li><p><strong>m_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – <span class="math notranslate nohighlight">\(2^m\)</span> is upper bound and <span class="math notranslate nohighlight">\(-2^m\)</span> is lower bound for bias. Default is 2.</p></li>
<li><p><strong>ste_fine_grained_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – STE is fine-grained if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-D array.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;pow2_quantized_conv&quot;</span></code>;</p>
<ul class="simple">
<li><p>W (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Filter weights in float. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,</span> <span class="pre">inmaps</span> <span class="pre">//</span> <span class="pre">group,</span> <span class="pre">*kernel)</span></code>)</p></li>
<li><p>b (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Bias vector in float. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
<li><p>W_q (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Quantized weights. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,</span> <span class="pre">inmaps</span> <span class="pre">//</span> <span class="pre">group,</span> <span class="pre">*kernel)</span></code>)</p></li>
<li><p>b_q (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Quantized biases. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">pow2_quantized_convolution</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.pruned_affine">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">pruned_affine</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">n_outmaps</em>, <em class="sig-param">base_axis=1</em>, <em class="sig-param">w_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">prune_w=True</em>, <em class="sig-param">rate_w=0.9</em>, <em class="sig-param">prune_b=True</em>, <em class="sig-param">rate_b=0.9</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#pruned_affine"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.pruned_affine" title="Permalink to this definition">¶</a></dt>
<dd><p>Pruned Affine.</p>
<p>Pruned Affine is the affine function,
except the definition of the inner product is modified.
The input-output relation of this function is as follows:</p>
<div class="math notranslate nohighlight">
\[y_j = \sum_{i} Q(w_{ji}) x_i,\]</div>
<p>where <span class="math notranslate nohighlight">\(Q(w_{ji})\)</span> is the pruning function, i.e., <code class="xref any docutils literal notranslate"><span class="pre">F.prune</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>1) if you would like to share weights between some layers, please
make sure to share the standard, floating value weights (<code class="xref any docutils literal notranslate"><span class="pre">weight</span></code>)
and not the quantized weights (<code class="xref any docutils literal notranslate"><span class="pre">quantized</span> <span class="pre">weight</span></code>)</p>
<p>2) The weights and the quantized weights become synced only after <code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code> is called,
and not after a call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code>.
To access the parameters of the network, remember to call <code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code> once before doing so, otherwise the
float weights and the quantized weights will not be in sync.</p>
<p>3) CPU and GPU implementations now use float value for <code class="xref any docutils literal notranslate"><span class="pre">quantized</span> <span class="pre">weight</span></code>,
since this function is only for simulation purposes.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape (<span class="math notranslate nohighlight">\(M_0 \times \ldots \times M_{B-1} \times D_B \times \ldots \times D_N\)</span>). Dimensions before and after base_axis are flattened as if it is a matrix.</p></li>
<li><p><strong>n_outmaps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a> or <a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Number of output neurons per data.</p></li>
<li><p><strong>base_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Dimensions up to <code class="xref any docutils literal notranslate"><span class="pre">base_axis</span></code> are treated as the sample dimensions.</p></li>
<li><p><strong>w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for weight.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for bias.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random generator for Initializer.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Specify whether to include the bias term.</p></li>
<li><p><strong>prune_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Quantize weights if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>rate_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Pruning rate for weights.</p></li>
<li><p><strong>prune_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Quantize bias if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>rate_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Pruning rate for bias.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><span class="math notranslate nohighlight">\((B + 1)\)</span>-D array. (<span class="math notranslate nohighlight">\(M_0 \times \ldots \times M_{B-1} \times L\)</span>)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;pruned_affine&quot;</span></code>;</p>
<ul class="simple">
<li><p>W (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Weight matrix in float. (shape: <code class="docutils literal notranslate"><span class="pre">(inmaps,</span> <span class="pre">outmaps)</span></code>)</p></li>
<li><p>b (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Bias vector in float. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
<li><p>W_q (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Qunatized weights. (shape: <code class="docutils literal notranslate"><span class="pre">(inmaps,</span> <span class="pre">outmaps)</span></code>)</p></li>
<li><p>b_q (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Quantized biases. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">pruned_affine</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.pruned_convolution">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">pruned_convolution</code><span class="sig-paren">(</span><em class="sig-param">inp</em>, <em class="sig-param">outmaps</em>, <em class="sig-param">kernel</em>, <em class="sig-param">pad=None</em>, <em class="sig-param">stride=None</em>, <em class="sig-param">dilation=None</em>, <em class="sig-param">group=1</em>, <em class="sig-param">channel_last=False</em>, <em class="sig-param">w_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">base_axis=1</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">prune_w=True</em>, <em class="sig-param">rate_w=0.9</em>, <em class="sig-param">prune_b=True</em>, <em class="sig-param">rate_b=0.9</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#pruned_convolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.pruned_convolution" title="Permalink to this definition">¶</a></dt>
<dd><p>Pruned Convolution.</p>
<p>Pruned Convolution is the convolution function,
except the definition of the inner product is modified.
The input-output relation of this function is as follows:</p>
<div class="math notranslate nohighlight">
\[y_{n, a, b} = \sum_{m} \sum_{i} \sum_{j} Q(w_{n, m, i, j}) x_{m, a + i, b + j},\]</div>
<p>where <span class="math notranslate nohighlight">\(Q(w_{ji})\)</span> is the pruning function, i.e., <code class="xref any docutils literal notranslate"><span class="pre">F.prune</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>1) if you would like to share weights between some layers, please
make sure to share the standard, floating value weights (<code class="xref any docutils literal notranslate"><span class="pre">weight</span></code>)
and not the quantized weights (<code class="xref any docutils literal notranslate"><span class="pre">quantized</span> <span class="pre">weight</span></code>)</p>
<p>2) The weights and the quantized weights become synced only after <code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code> is called,
and not after a call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code>.
To access the parameters of the network, remember to call <code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code> once before doing so, otherwise the
float weights and the quantized weights will not be in sync.</p>
<p>3) CPU and GPU implementations now use float value for <code class="xref any docutils literal notranslate"><span class="pre">quantized</span> <span class="pre">weight</span></code>,
since this function is only for simulation purposes.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – N-D array.</p></li>
<li><p><strong>outmaps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of convolution kernels (which is equal to the number of output channels). For example, to apply convolution on an input with 16 types of filters, specify 16.</p></li>
<li><p><strong>kernel</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</p></li>
<li><p><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Padding sizes for dimensions.</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Stride sizes for dimensions.</p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Dilation sizes for dimensions.</p></li>
<li><p><strong>group</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of groups of channels. This makes connections across channels more sparse by grouping connections along map direction.</p></li>
<li><p><strong>w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for weight.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for bias.</p></li>
<li><p><strong>base_axis</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Dimensions up to <code class="xref any docutils literal notranslate"><span class="pre">base_axis</span></code> are treated as the sample dimensions.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random generator for Initializer.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Specify whether to include the bias term.</p></li>
<li><p><strong>prune_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Quantize weights if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>rate_w</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Pruning rate for weights.</p></li>
<li><p><strong>prune_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Quantize bias if <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>rate_b</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Pruning rate for bias.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>N-D array.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;pruned_conv&quot;</span></code>;</p>
<ul class="simple">
<li><p>W (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Filter weights in float. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,</span> <span class="pre">inmaps</span> <span class="pre">//</span> <span class="pre">group,</span> <span class="pre">*kernel)</span></code>)</p></li>
<li><p>b (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Bias vector in float. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
<li><p>W_q (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Qunatized weights. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,</span> <span class="pre">inmaps</span> <span class="pre">//</span> <span class="pre">group,</span> <span class="pre">*kernel)</span></code>)</p></li>
<li><p>b_q (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Quantized biases. (shape: <code class="docutils literal notranslate"><span class="pre">(outmaps,)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">pruned_convolution</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.min_max_quantize">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">min_max_quantize</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">ql_min=0</em>, <em class="sig-param">ql_max=255</em>, <em class="sig-param">decay=0.999</em>, <em class="sig-param">x_min_max=False</em>, <em class="sig-param">ema=False</em>, <em class="sig-param">ste_fine_grained=True</em>, <em class="sig-param">eps=0.01</em>, <em class="sig-param">qr_min_init=None</em>, <em class="sig-param">qr_max_init=None</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">outputs=None</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#min_max_quantize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.min_max_quantize" title="Permalink to this definition">¶</a></dt>
<dd><p>Min-max quantization.</p>
<p>This function uniformly quantizes values in the range of min and max quantization levels.</p>
<p>Min-max quantization is defined as the following equation</p>
<div class="math notranslate nohighlight">
\[y = round \left(\frac{\min(\max(x, m), M) - m}{scale} \right) \times scale + m,\]</div>
<p>where the <span class="math notranslate nohighlight">\(scale\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[scale = \frac{M - m}{M_q - m_q},\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\begin{split}m_q = ql_{min}, \\
M_q = ql_{max}, \\
m = qr_{min}, \\
M = qr_{max}.\end{split}\]</div>
<p>In the backward pass when using <code class="xref any docutils literal notranslate"><span class="pre">ste_fine_grained</span></code> as false,</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{\partial q_i}{\partial x_i} = 1.\]</div>
</div></blockquote>
<p>In the backward pass when using <code class="xref any docutils literal notranslate"><span class="pre">ste_fine_grained</span></code> as true,</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}  \frac{\partial q_i}{\partial x_i}= \left\{
\begin{array}{ll}
  0 &amp; if \ \ \ x_i &gt; M \\
  1 &amp; if \ \ m \le x_i \le M \\
  0 &amp; if \ \ x_i &lt; m \\
\end{array} \right..\end{split}\]</div>
</div></blockquote>
<p><span class="math notranslate nohighlight">\(qr_{min}\)</span> and <span class="math notranslate nohighlight">\(qr_{max}\)</span> are treaded as follows.</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref any docutils literal notranslate"><span class="pre">x_min_max</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a> and <code class="xref any docutils literal notranslate"><span class="pre">ema</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>:
Exponential moving average are computed for each <span class="math notranslate nohighlight">\(min(x)\)</span> and <span class="math notranslate nohighlight">\(max(x)\)</span>
then stored in <span class="math notranslate nohighlight">\(qr_{min}\)</span> and <span class="math notranslate nohighlight">\(qr_{max}\)</span>.</p></li>
<li><p><code class="xref any docutils literal notranslate"><span class="pre">x_min_max</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a> and <code class="xref any docutils literal notranslate"><span class="pre">ema</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#False" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">False</span></code></a>:
<span class="math notranslate nohighlight">\(min(x)\)</span> and <span class="math notranslate nohighlight">\(max(x)\)</span> are computed then stored in <span class="math notranslate nohighlight">\(qr_{min}\)</span> and <span class="math notranslate nohighlight">\(qr_{max}\)</span>.</p></li>
<li><p><code class="xref any docutils literal notranslate"><span class="pre">x_min_max</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#False" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">False</span></code></a> and <code class="xref any docutils literal notranslate"><span class="pre">ema</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>:
Exponential moving average stored in <span class="math notranslate nohighlight">\(qr_{min}\)</span> and <span class="math notranslate nohighlight">\(qr_{max}\)</span> are used.</p></li>
<li><p><code class="xref any docutils literal notranslate"><span class="pre">x_min_max</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#False" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">False</span></code></a> and <code class="xref any docutils literal notranslate"><span class="pre">ema</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#False" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">False</span></code></a>
Gradients of <span class="math notranslate nohighlight">\(qr_{min}\)</span> and <span class="math notranslate nohighlight">\(qr_{max}\)</span> are computed in the backward pass.</p></li>
</ul>
</div></blockquote>
<p>More precisely, in inference of the min-max quantization, one has to consider <em>zero-point (zp)</em>
which corresponds
to the real value 0, and its data type is an integer. <em>zero-point</em> is defined as</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}  &amp;&amp; zp_f = ql_{min} -\frac{qr_{min}}{scale}, \\
  &amp;&amp; zp = \left\{
\begin{array}{ll}
  ql_{max} &amp; if \ \ \ zp_f &gt;= ql_{max} \\
  round(zp_f) &amp; if \ \ otherwise \\
  ql_{min}  &amp; if \ \ zp_f &lt;= ql_{min} \\
\end{array} \right..\end{split}\]</div>
</div></blockquote>
<p>Accordingly, in order to simulate quantization effect of <em>zero-point</em>,
during both forward and backward pass, <span class="math notranslate nohighlight">\(qr_{min}\)</span> and <span class="math notranslate nohighlight">\(qr_{max}\)</span> are adjusted as follows,</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}qr_{min}^{adj} = ql_{min} - zp * scale, \\
qr_{max}^{adj} = ql_{max} - zp * scale.\end{split}\]</div>
</div></blockquote>
<p>These operations are often called <em>nudge</em>.</p>
<p>Finally, in the formulas of the min-max quantization, <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(M\)</span> are replaced by
<span class="math notranslate nohighlight">\(qr_{min}^{adj}\)</span> and <span class="math notranslate nohighlight">\(qr_{max}^{adj}\)</span> respectively.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array.</p></li>
<li><p><strong>ql_min</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>, or </em><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Minimum quantization level. Default is 0.</p></li>
<li><p><strong>ql_max</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>, or </em><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Maximum quantization level. Default is 255.</p></li>
<li><p><strong>decay</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – The decay rate for the exponential moving average.</p></li>
<li><p><strong>x_min_max</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Use the min and max of x to compute quantization ranges. Default is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#False" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">False</span></code></a>.</p></li>
<li><p><strong>ema</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – Use the exponential moving average for the min and max quantization ranges.
Default is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#False" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">False</span></code></a>.</p></li>
<li><p><strong>ste_fine_grained</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – If true, STE is not 1, the {0, 1}-mask computed from the min-max is applied to the gradient in the backward; otherwise, STE is 1.</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – Epsilon, or small value to ensure <span class="math notranslate nohighlight">\(qr_{max} - qr_{min}\)</span> must be greater
than the epsilon for both weights and bias.</p></li>
<li><p><strong>qr_min_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for the minimum quantization range, qr_min. Default is <a class="reference internal" href="#nnabla.initializer.ConstantInitializer" title="nnabla.initializer.ConstantInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.ConstantInitializer</span></code></a> (-6.0).</p></li>
<li><p><strong>qr_max_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>) – Initializer for the maximum quantization rage, qr_max Default is <a class="reference internal" href="#nnabla.initializer.ConstantInitializer" title="nnabla.initializer.ConstantInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.ConstantInitializer</span></code></a> (6.0).</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<p>Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko, “Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference”, <a class="reference external" href="https://arxiv.org/abs/1712.05877">https://arxiv.org/abs/1712.05877</a></p>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;min_max_quantize&quot;</span></code>;</p>
<ul class="simple">
<li><p>qr_min (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Minimum quantization range, the exponential movining average of min values of inputs initialized with -6.0 if ema is True. (shape: <code class="docutils literal notranslate"><span class="pre">ql_min.shape</span></code>)</p></li>
<li><p>qr_max (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Maximum quantization range, the exponential movining average of max values of inputs initialized with 6.0 if ema is True. (shape: <code class="docutils literal notranslate"><span class="pre">ql_max.shape</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">min_max_quantize</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.lstm_cell">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">lstm_cell</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">h</em>, <em class="sig-param">c</em>, <em class="sig-param">state_size</em>, <em class="sig-param">w_init=None</em>, <em class="sig-param">b_init=None</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#lstm_cell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.lstm_cell" title="Permalink to this definition">¶</a></dt>
<dd><p>Long Short-Term Memory.</p>
<p>Long Short-Term Memory, or LSTM, is a building block for recurrent neural networks (RNN) layers.
LSTM unit consists of a cell and input, output, forget gates whose functions are defined as following:</p>
<div class="math notranslate nohighlight">
\[\begin{split}f_t&amp;&amp;=\sigma(W_fx_t+U_fh_{t-1}+b_f) \\
i_t&amp;&amp;=\sigma(W_ix_t+U_ih_{t-1}+b_i) \\
o_t&amp;&amp;=\sigma(W_ox_t+U_oh_{t-1}+b_o) \\
c_t&amp;&amp;=f_t\odot c_{t-1}+i_t\odot\tanh(W_cx_t+U_ch_{t-1}+b_c) \\
h_t&amp;&amp;=o_t\odot\tanh(c_t).\end{split}\]</div>
<p class="rubric">References</p>
<p>S. Hochreiter, and J. Schmidhuber. “Long Short-Term Memory.”
Neural Computation. 1997.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape (batch_size, input_size).</p></li>
<li><p><strong>h</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape (batch_size, state_size).</p></li>
<li><p><strong>c</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape (batch_size, state_size).</p></li>
<li><p><strong>state_size</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Internal state size is set to <code class="xref any docutils literal notranslate"><span class="pre">state_size</span></code>.</p></li>
<li><p><strong>w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>, optional) – Initializer for weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>, optional) – Initializer for bias. By default, it is initialized with zeros if <code class="xref any docutils literal notranslate"><span class="pre">with_bias</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;lstm&quot;</span></code>;</p>
<ul class="simple">
<li><p>affine/W (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Stacked weight matrixes of LSTM block. (shape: <code class="docutils literal notranslate"><span class="pre">(inmaps,</span> <span class="pre">4,</span> <span class="pre">state_size)</span></code>)</p></li>
<li><p>affine/b (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Stacked bias vectors of LSTM block. (shape: <code class="docutils literal notranslate"><span class="pre">(4,</span> <span class="pre">state_size,)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">lstm_cell</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="class">
<dt id="nnabla.parametric_functions.LSTMCell">
<em class="property">class </em><code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">LSTMCell</code><span class="sig-paren">(</span><em class="sig-param">batch_size</em>, <em class="sig-param">state_size</em>, <em class="sig-param">h=None</em>, <em class="sig-param">c=None</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#LSTMCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.LSTMCell" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="nnabla.parametric_functions.LSTMCell.__call__">
<code class="sig-name descname">__call__</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">w_init</em>, <em class="sig-param">b_init</em>, <em class="sig-param">fix_parameters</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#LSTMCell.__call__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.LSTMCell.__call__" title="Permalink to this definition">¶</a></dt>
<dd><p>Updates h and c by calling lstm function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape (batch_size, input_size).</p></li>
<li><p><strong>w_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>, optional) – Initializer for weight. By default, it is initialized with <a class="reference internal" href="#nnabla.initializer.UniformInitializer" title="nnabla.initializer.UniformInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.UniformInitializer</span></code></a> within the range determined by <a class="reference internal" href="#nnabla.initializer.calc_uniform_lim_glorot" title="nnabla.initializer.calc_uniform_lim_glorot"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.calc_uniform_lim_glorot</span></code></a>.</p></li>
<li><p><strong>b_init</strong> (<a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a> or <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>, optional) – Initializer for bias. By default, it is initialized with zeros if <code class="xref any docutils literal notranslate"><span class="pre">with_bias</span></code> is <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.spectral_norm">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">spectral_norm</code><span class="sig-paren">(</span><em class="sig-param">w</em>, <em class="sig-param">dim=0</em>, <em class="sig-param">itr=1</em>, <em class="sig-param">eps=1e-12</em>, <em class="sig-param">test=False</em>, <em class="sig-param">u_init=None</em>, <em class="sig-param">fix_parameters=True</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#spectral_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.spectral_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Spectral Normalization.</p>
<div class="math notranslate nohighlight">
\[W_{sn} = \frac{W}{\sigma(W)}.\]</div>
<p>where <span class="math notranslate nohighlight">\(W\)</span> is the input matrix, and the <span class="math notranslate nohighlight">\(\sigma(W)\)</span> is the spectral norm of <span class="math notranslate nohighlight">\(W\)</span>. The spectral norm is approximately computed by the power iteration.</p>
<p class="rubric">References</p>
<p>Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida,
“Spectral Normalization for Generative Adversarial Networks”,
International Conference on Learning Representations. 2018.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>W</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape. This is normally network parameter.</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">int</span></code></a>) – Output dimension. Default is 0. If the dimension is not 0, then the specified dimension becomes the most-left dimension by transposing.</p></li>
<li><p><strong>itr</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">int</span></code></a>) – Number of iterations. Default is 1.</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">float</span></code></a>) – Epsilon for the normalization. Default is 1e-12.</p></li>
<li><p><strong>test</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">bool</span></code></a>) – Use test mode. Default is False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Spectrally normalized <span class="math notranslate nohighlight">\(W_{sn}\)</span> with the same shape as <span class="math notranslate nohighlight">\(W\)</span>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable">Variable</a></p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nnabla</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">nnabla.parametric_functions</span> <span class="k">as</span> <span class="nn">PF</span>

<span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span>

<span class="c1"># Spectrally normalized convolution</span>
<span class="n">apply_w</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">PF</span><span class="o">.</span><span class="n">spectral_norm</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="o">.</span><span class="n">from_numpy_array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">apply_w</span><span class="o">=</span><span class="n">apply_w</span><span class="p">)</span>

<span class="c1"># Spectrally normalized affine</span>
<span class="n">apply_w</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">PF</span><span class="o">.</span><span class="n">spectral_norm</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="o">.</span><span class="n">from_numpy_array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">))</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">apply_w</span><span class="o">=</span><span class="n">apply_w</span><span class="p">)</span>

<span class="c1"># Spectrally normalized embed</span>
<span class="n">apply_w</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">PF</span><span class="o">.</span><span class="n">spectral_norm</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="o">.</span><span class="n">from_numpy_array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">))</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">apply_w</span><span class="o">=</span><span class="n">apply_w</span><span class="p">)</span>
</pre></div>
</div>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;spectral-norm&quot;</span></code>;</p>
<ul class="simple">
<li><p>W_sn (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : Spectral Normalized Weight matrix. (shape: <code class="docutils literal notranslate"><span class="pre">w.shape</span></code>)</p></li>
<li><p>u (<code class="docutils literal notranslate"><span class="pre">need_grad=False</span></code>) : singular vector. (shape: <code class="docutils literal notranslate"><span class="pre">(w.shape[dim],</span> <span class="pre">)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">spectral_norm</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.weight_normalization">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">weight_normalization</code><span class="sig-paren">(</span><em class="sig-param">v</em>, <em class="sig-param">dim=0</em>, <em class="sig-param">eps=1e-12</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#weight_normalization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.weight_normalization" title="Permalink to this definition">¶</a></dt>
<dd><p>Weight Normalization.</p>
<div class="math notranslate nohighlight">
\[\mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|}\]</div>
<p>where <span class="math notranslate nohighlight">\(v\)</span> is the input matrix,
and <span class="math notranslate nohighlight">\(g\)</span> is learnable multiplication factors each of which is applied to each output map at <code class="xref any docutils literal notranslate"><span class="pre">dim</span></code>.
This function is in general used as callback passed to apply_w for PF.convolution, PF.affine and so on.
According to the author`s original implementation (<a class="reference external" href="https://github.com/TimSalimans/weight_norm">https://github.com/TimSalimans/weight_norm</a>), <span class="math notranslate nohighlight">\(v\)</span> should be initialized by <span class="math notranslate nohighlight">\(N(0, 0.05)\)</span>.
To meet this condition, initializer should be passed to convolution which Weight Normalization is applied, like an example below.</p>
<p class="rubric">References</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1602.07868">Tim Salimans, Diederik P. Kingma, Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks.</a></p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>W</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape. This is normally network parameter.</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">int</span></code></a>) – Output dimension. Default is 0.
If the dimension is not 0, then the specified dimension becomes the most-left dimension by transposing.</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">float</span></code></a>) – Epsilon for the normalization. Default is 1e-12.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><span class="math notranslate nohighlight">\(W_{sn}\)</span> with the same shape as <span class="math notranslate nohighlight">\(W\)</span>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable">Variable</a></p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nnabla</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">nnabla.parametric_functions</span> <span class="k">as</span> <span class="nn">PF</span>
<span class="kn">import</span> <span class="nn">nnabla.initializer</span> <span class="k">as</span> <span class="nn">I</span>

<span class="c1"># h is nn.Variable.</span>

<span class="c1"># convolution</span>
<span class="c1"># according to the original implementation, w should be initialized by N(0, 0.05).</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">apply_w</span><span class="o">=</span><span class="n">PF</span><span class="o">.</span><span class="n">weight_normalization</span><span class="p">,</span> <span class="n">w_init</span><span class="o">=</span><span class="n">I</span><span class="o">.</span><span class="n">NormalInitializer</span><span class="p">(</span><span class="mf">0.05</span><span class="p">))</span>

<span class="c1"># affine</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">apply_w</span><span class="o">=</span><span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">PF</span><span class="o">.</span><span class="n">weight_normalization</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">w_init</span><span class="o">=</span><span class="n">I</span><span class="o">.</span><span class="n">NormalInitializer</span><span class="p">(</span><span class="mf">0.05</span><span class="p">))</span>
</pre></div>
</div>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;wn&quot;</span></code>;</p>
<ul class="simple">
<li><p>g (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : Weight Normalization adaptive scale scalar.. (shape: <code class="docutils literal notranslate"><span class="pre">w.shape[dim]</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">weight_normalization</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.multi_head_attention">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">multi_head_attention</code><span class="sig-paren">(</span><em class="sig-param">query</em>, <em class="sig-param">key</em>, <em class="sig-param">value</em>, <em class="sig-param">num_heads=12</em>, <em class="sig-param">dropout=0.0</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">with_bias=True</em>, <em class="sig-param">add_attn_bias=False</em>, <em class="sig-param">additive_mask=None</em>, <em class="sig-param">key_padding_mask=None</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">param_init=None</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#multi_head_attention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.multi_head_attention" title="Permalink to this definition">¶</a></dt>
<dd><p>MultiHeadAttention.</p>
<p>Computes multi-headed attention with query, key, and value.
We use the following notations to describe the inputs and outputs below.
<span class="math notranslate nohighlight">\(L_T\)</span>: target sequence length, <span class="math notranslate nohighlight">\(L_S\)</span>: source sequence length, <span class="math notranslate nohighlight">\(B\)</span>: batch size, <span class="math notranslate nohighlight">\(E\)</span>: embedding dimension.</p>
<p class="rubric">References</p>
<p>A. Vaswani et al. “Attention is All You Need.”
NIPS. 2017.
&lt;<a class="reference external" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf</a>&gt;</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">tgt_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">))</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">src_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">kdim</span><span class="p">))</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">src_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">vdim</span><span class="p">))</span>

<span class="n">out</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">multi_head_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">forward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>query</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape <span class="math notranslate nohighlight">\((L_T, B, E)\)</span>.</p></li>
<li><p><strong>key</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape <span class="math notranslate nohighlight">\((L_S, B, E_k)\)</span>.</p></li>
<li><p><strong>value</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input N-D array with shape <span class="math notranslate nohighlight">\((L_S, B, E_v)\)</span>.</p></li>
<li><p><strong>num_heads</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><em>optional</em>) – Number of attention heads. Note that embedding dimensoin E must be divisible by the number of heads. Default is 12 which is conventional.</p></li>
<li><p><strong>dropout</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>, </em><em>optional</em>) – Dropout ratio applied to parameters. Default is 0.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em><em>, </em><em>optional</em>) – Random generator for Initializer. Default is None.</p></li>
<li><p><strong>with_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>optional</em>) – Specify whether to include the bias parameters. Default is True.</p></li>
<li><p><strong>add_attn_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>optional</em>) – Specify whether to add attention bias parameters for key and value. Default is False.</p></li>
<li><p><strong>additive_mask</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a><em>, </em><em>optional</em>) – Input N-D array with shape <span class="math notranslate nohighlight">\((L_T, L_S)\)</span>. Values will be added to the attention layer to prevent attention to certain positions.</p></li>
<li><p><strong>key_padding_mask</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a><em>, </em><em>optional</em>) – Input N-D array with shape <span class="math notranslate nohighlight">\((B, L_S)\)</span>. Specified padding elements will be ignored by the attention layer. Values must be either 1 or 0.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>optional</em>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated. Default is False.</p></li>
<li><p><strong>param_init</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a><em>, </em><em>optional</em>) – Parameter initializers can be set with a dict. Possible keys of the dict include q_weight, k_weight, v_weight, q_bias, k_bias, v_bias, out_weight, out_bias, attn_bias_k, attn_bias_v.
A value of the dict must be an <code class="xref py py-obj docutils literal notranslate"><span class="pre">Initializer</span></code>
or a <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a>.
E.g. <code class="docutils literal notranslate"><span class="pre">{'q_bias':</span> <span class="pre">ConstantInitializer(0)}</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output <span class="math notranslate nohighlight">\(y\)</span> with shape <span class="math notranslate nohighlight">\((L_T, B, E)\)</span>
~nnabla.Variable: Output <span class="math notranslate nohighlight">\(h_n\)</span> with shape <span class="math notranslate nohighlight">\((B, L_T, L_S)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable">Variable</a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;multi_head_attention&quot;</span></code>;</p>
<ul class="simple">
<li><p>q_weight (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : weights for query. (shape: <code class="docutils literal notranslate"><span class="pre">(E,</span> <span class="pre">E)</span></code>)</p></li>
<li><p>k_weight (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : weights for key. (shape: <code class="docutils literal notranslate"><span class="pre">(E_k,</span> <span class="pre">E)</span></code>)</p></li>
<li><p>v_weight (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : weights for value. (shape: <code class="docutils literal notranslate"><span class="pre">(E_v,</span> <span class="pre">E)</span></code>)</p></li>
<li><p>out_weight (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : weigths for out projection. (shape: <code class="docutils literal notranslate"><span class="pre">(E,</span> <span class="pre">E)</span></code>)</p></li>
<li><p>q_bias (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : bias for query. (shape: <code class="docutils literal notranslate"><span class="pre">(E,</span> <span class="pre">)</span></code>)</p></li>
<li><p>k_bias (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : bias for key. (shape: <code class="docutils literal notranslate"><span class="pre">(E,</span> <span class="pre">)</span></code>)</p></li>
<li><p>v_bias (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : bais for value. (shape: <code class="docutils literal notranslate"><span class="pre">(E,</span> <span class="pre">)</span></code>)</p></li>
<li><p>out_bias (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : bias for out projection. (shape: <code class="docutils literal notranslate"><span class="pre">(E,</span> <span class="pre">)</span></code>)</p></li>
<li><p>attn_bias_k (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : attnetion bias for k. (shape: <code class="docutils literal notranslate"><span class="pre">(E,</span> <span class="pre">1)</span></code>)</p></li>
<li><p>attn_bias_v (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : attnetion bias for v. (shape: <code class="docutils literal notranslate"><span class="pre">(E,</span> <span class="pre">1)</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">multi_head_attention</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.transformer">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">transformer</code><span class="sig-paren">(</span><em class="sig-param">src</em>, <em class="sig-param">tgt</em>, <em class="sig-param">embed_dim=512</em>, <em class="sig-param">num_heads=8</em>, <em class="sig-param">num_encoder_layers=6</em>, <em class="sig-param">num_decoder_layers=6</em>, <em class="sig-param">dim_feedforward=2048</em>, <em class="sig-param">dropout=0.1</em>, <em class="sig-param">activation=None</em>, <em class="sig-param">src_additive_mask=None</em>, <em class="sig-param">tgt_additive_mask=None</em>, <em class="sig-param">memory_additive_mask=None</em>, <em class="sig-param">src_key_padding_mask=None</em>, <em class="sig-param">tgt_key_padding_mask=None</em>, <em class="sig-param">memory_key_padding_mask=None</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">add_attn_bias=False</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#transformer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.transformer" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer.</p>
<p>We use the following notations to describe the inputs and outputs below.
<span class="math notranslate nohighlight">\(L_T\)</span>: target sequence length, <span class="math notranslate nohighlight">\(L_S\)</span>: source sequence length, <span class="math notranslate nohighlight">\(B\)</span>: batch size, <span class="math notranslate nohighlight">\(E\)</span>: embedding dimension.</p>
<p class="rubric">References</p>
<p>A. Vaswani et al. “Attention is All You Need.”
NIPS. 2017.
&lt;<a class="reference external" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf</a>&gt;</p>
<p>Examples:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">src</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">src_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">),</span><span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tgt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">tgt_len</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">),</span><span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">transformer</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">num_encoder_layers</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">forward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input source sequence to the encoder with shape:math:<code class="xref any docutils literal notranslate"><span class="pre">(L_S,</span> <span class="pre">B,</span> <span class="pre">E)</span></code>.</p></li>
<li><p><strong>tgt</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input target sequence to the decoder with shape <span class="math notranslate nohighlight">\((L_T, B, E)\)</span>.</p></li>
<li><p><strong>embed_dim</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><em>optional</em>) – Embedding dimension to be used. Default is 512.</p></li>
<li><p><strong>num_heads</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><em>optional</em>) – Number of attention heads. Default is 12.</p></li>
<li><p><strong>num_encoder_layers</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><em>optional</em>) – Number of encoder layers to stack. Default is 6.</p></li>
<li><p><strong>num_decoder_layers</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><em>optional</em>) – Number of decoder layers to stack. Default is 6.</p></li>
<li><p><strong>dim_feedforward</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><em>optional</em>) – Dimension of the feedforward network model. Default is 2048.</p></li>
<li><p><strong>dropout</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>, </em><em>optional</em>) – Dropout ratio applied. Default is 0.1.</p></li>
<li><p><strong>activation</strong> (<em>function</em><em>, </em><em>optional</em>) – Non-linear activation function to be used. Default is None, which is set as F.relu in the code.</p></li>
<li><p><strong>src_additive_mask</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a><em>, </em><em>optional</em>) – Additive mask for the src sequence (optional). <span class="math notranslate nohighlight">\((L_S, L_S)\)</span>.</p></li>
<li><p><strong>tgt_additive_mask</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a><em>, </em><em>optional</em>) – Additive mask for the tgt sequence (optional).:math:<code class="xref any docutils literal notranslate"><span class="pre">(L_T,</span> <span class="pre">L_T)</span></code>.</p></li>
<li><p><strong>memory_additive_mask</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a><em>, </em><em>optional</em>) – Additive mask for the encoder output (optional). <span class="math notranslate nohighlight">\((L_T, L_S)\)</span>.</p></li>
<li><p><strong>src_key_padding_mask</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a><em>, </em><em>optional</em>) – Key padding mask for src keys per batch (optional).:math:<code class="xref any docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">L_S)</span></code>. Specified padding elements will be ignored by the attention layer. Values must be either 1 or 0.</p></li>
<li><p><strong>tgt_key_padding_mask</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a><em>, </em><em>optional</em>) – Key padding mask for tgt keys per batch (optional).:math:<code class="xref any docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">L_T)</span></code>. Specified padding elements will be ignored by the attention layer. Values must be either 1 or 0.</p></li>
<li><p><strong>memory_key_padding_mask</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a><em>, </em><em>optional</em>) – Key padding mask for memory keys per batch (optional).:math:<code class="xref any docutils literal notranslate"><span class="pre">(B,</span> <span class="pre">L_S)</span></code>. Specified padding elements will be ignored by the attention layer. Values must be either 1 or 0.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em><em>, </em><em>optional</em>) – Random generator for Initializer. Default is None.</p></li>
<li><p><strong>add_attn_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>optional</em>) – Specify whether to add attention bias parameters for key and value. Default is False.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>optional</em>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated. Default is False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output <span class="math notranslate nohighlight">\(y\)</span> with shape <span class="math notranslate nohighlight">\((L_T, B, E)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable">Variable</a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;transformer&quot;</span></code>;</p>
<ul class="simple">
<li><p>encoder{layer#} (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : parameters for the n’th encoder layer. (shape: <code class="docutils literal notranslate"><span class="pre">Refer</span> <span class="pre">to</span> <span class="pre">transformer_encode</span> <span class="pre">for</span> <span class="pre">details</span></code>)</p></li>
<li><p>decoder{layer#} (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : parameters for the n’th decoder layer. (shape: <code class="docutils literal notranslate"><span class="pre">Refer</span> <span class="pre">to</span> <span class="pre">transformer_decode</span> <span class="pre">for</span> <span class="pre">details</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.transformer_encode">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">transformer_encode</code><span class="sig-paren">(</span><em class="sig-param">src</em>, <em class="sig-param">embed_dim</em>, <em class="sig-param">num_heads</em>, <em class="sig-param">dim_feedforward=2048</em>, <em class="sig-param">dropout=0.1</em>, <em class="sig-param">activation=None</em>, <em class="sig-param">src_additive_mask=None</em>, <em class="sig-param">src_key_padding_mask=None</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">add_attn_bias=False</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#transformer_encode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.transformer_encode" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer Encoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input sequnce to the encoder layer with shape <span class="math notranslate nohighlight">\((L_S, B, E)\)</span>.</p></li>
<li><p><strong>embed_dim</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of embedding dimension.</p></li>
<li><p><strong>num_heads</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of attention heads.</p></li>
<li><p><strong>dim_feedforward</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><em>optional</em>) – Dimension of the feedforward network model. Default is 2048.</p></li>
<li><p><strong>dropout</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>, </em><em>optional</em>) – Dropout ratio. Default is 0.1.</p></li>
<li><p><strong>activation</strong> (<em>function</em><em>, </em><em>optional</em>) – Non-linear activation function to be used. Default is None, which is set as F.relu in the code.</p></li>
<li><p><strong>src_additive_mask</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a><em>, </em><em>optional</em>) – Additive mask for the source sequence with shape <span class="math notranslate nohighlight">\((L_S, L_S)\)</span></p></li>
<li><p><strong>src_key_padding_mask</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a><em>, </em><em>optional</em>) – Padding mask for the source sequence with shape <span class="math notranslate nohighlight">\((B, L_S)\)</span>. Specified padding elements will be ignored by the attention layer. Values must be either 1 or 0.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em><em>, </em><em>optional</em>) – Random generator for Initializer. Defalut is None.</p></li>
<li><p><strong>add_attn_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>optional</em>) – Specify whether to add attention bias parameters for key and value. Default is False.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>optional</em>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated. Default is False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output <span class="math notranslate nohighlight">\(y\)</span> with shape <span class="math notranslate nohighlight">\((L_S, B, E)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable">Variable</a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;transformer_encode&quot;</span></code>;</p>
<ul class="simple">
<li><p>src_self_attn (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : self-attention parameters for source sequence. (shape: <code class="docutils literal notranslate"><span class="pre">Refer</span> <span class="pre">to</span> <span class="pre">multi_head_attention</span> <span class="pre">for</span> <span class="pre">details</span></code>)</p></li>
<li><p>enc_affine1 (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : first affine used in encoder. (shape: <code class="docutils literal notranslate"><span class="pre">Refer</span> <span class="pre">to</span> <span class="pre">affine</span> <span class="pre">for</span> <span class="pre">details</span></code>)</p></li>
<li><p>enc_affine2 (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : second affine used in encoder. (shape: <code class="docutils literal notranslate"><span class="pre">Refer</span> <span class="pre">to</span> <span class="pre">affine</span> <span class="pre">for</span> <span class="pre">details</span></code>)</p></li>
<li><p>enc_layer_norm1 (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : fist layer normalization used in encoder. (shape: <code class="docutils literal notranslate"><span class="pre">Refer</span> <span class="pre">to</span> <span class="pre">layer_normalization</span> <span class="pre">for</span> <span class="pre">details</span></code>)</p></li>
<li><p>enc_layer_norm2 (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : second layer normalization used in encoder. (shape: <code class="docutils literal notranslate"><span class="pre">Refer</span> <span class="pre">to</span> <span class="pre">layer_normalization</span> <span class="pre">for</span> <span class="pre">details</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">transformer_encode</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="function">
<dt id="nnabla.parametric_functions.transformer_decode">
<code class="sig-prename descclassname">nnabla.parametric_functions.</code><code class="sig-name descname">transformer_decode</code><span class="sig-paren">(</span><em class="sig-param">tgt</em>, <em class="sig-param">memory</em>, <em class="sig-param">embed_dim</em>, <em class="sig-param">num_heads</em>, <em class="sig-param">dim_feedforward=2048</em>, <em class="sig-param">dropout=0.1</em>, <em class="sig-param">activation=None</em>, <em class="sig-param">tgt_additive_mask=None</em>, <em class="sig-param">memory_additive_mask=None</em>, <em class="sig-param">tgt_key_padding_mask=None</em>, <em class="sig-param">memory_key_padding_mask=None</em>, <em class="sig-param">rng=None</em>, <em class="sig-param">add_attn_bias=False</em>, <em class="sig-param">fix_parameters=False</em>, <em class="sig-param">name=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/parametric_functions.html#transformer_decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.parametric_functions.transformer_decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer Decoder.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tgt</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Input sequnce to the decoder layer with shape <span class="math notranslate nohighlight">\((L_T, B, E)\)</span>.</p></li>
<li><p><strong>memory</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a>) – Output sequnce from the last layer of the encoder with shape <span class="math notranslate nohighlight">\((L_T, B, E)\)</span>.</p></li>
<li><p><strong>embed_dim</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of embedding dimension.</p></li>
<li><p><strong>num_heads</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Number of attention heads.</p></li>
<li><p><strong>dim_feedforward</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><em>optional</em>) – Dimension of the feedforward network model. Default is 2048.</p></li>
<li><p><strong>dropout</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em>, </em><em>optional</em>) – Dropout ratio. Default is 0.1.</p></li>
<li><p><strong>activation</strong> (<em>function</em><em>, </em><em>optional</em>) – Non-linear activation function to be used. Default is None, which is set as F.relu in the code.</p></li>
<li><p><strong>tgt_additive_mask</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a><em>, </em><em>optional</em>) – Additive mask for the target sequence with shape <span class="math notranslate nohighlight">\((L_T, L_T)\)</span>.</p></li>
<li><p><strong>memory_additive_mask</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a><em>, </em><em>optional</em>) – Additive mask for the memory sequcne with shape <span class="math notranslate nohighlight">\((L_T, L_S)\)</span>.</p></li>
<li><p><strong>tgt_key_padding_mask</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a><em>, </em><em>optional</em>) – Padding mask for the target sequence with shape <span class="math notranslate nohighlight">\((B, L_T)\)</span>. Specified padding elements will be ignored by the attention layer. Values must be either 1 or 0.</p></li>
<li><p><strong>memory_key_padding_mask</strong> (<a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable"><em>Variable</em></a><em>, </em><em>optional</em>) – Padding mask for the mask sequence with shape <span class="math notranslate nohighlight">\((B, L_S)\)</span>. Specified padding elements will be ignored by the attention layer. Values must be either 1 or 0.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random generator for Initializer. Default is None.</p></li>
<li><p><strong>add_attn_bias</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>optional</em>) – Specify whether to add attention bias parameters for key and value. Default is False.</p></li>
<li><p><strong>fix_parameters</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – When set to <a class="reference external" href="https://docs.python.org/3.6/library/constants.html#True" title="(in Python v3.6)"><code class="xref any docutils literal notranslate"><span class="pre">True</span></code></a>, the weights and biases will not be updated. Default is False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output <span class="math notranslate nohighlight">\(y\)</span> with shape <span class="math notranslate nohighlight">\((L_T, B, E)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="variable.html#nnabla.Variable" title="nnabla.Variable">Variable</a></p>
</dd>
</dl>
<dl class="simple">
<dt>Parameters to be registered</dt><dd><p>The following variables are registered in a parameter scope <code class="docutils literal notranslate"><span class="pre">&quot;transformer_decode&quot;</span></code>;</p>
<ul class="simple">
<li><p>tgt_self_attn (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : self-attention parameters for target sequence. (shape: <code class="docutils literal notranslate"><span class="pre">Refer</span> <span class="pre">to</span> <span class="pre">multi_head_attention</span> <span class="pre">for</span> <span class="pre">details</span></code>)</p></li>
<li><p>tgt_memory_attn (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : attention parameters for target sequence with output from encoder as key. (shape: <code class="docutils literal notranslate"><span class="pre">Refer</span> <span class="pre">to</span> <span class="pre">multi_head_attention</span> <span class="pre">for</span> <span class="pre">details</span></code>)</p></li>
<li><p>dec_affine1 (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : first affine used in decoder. (shape: <code class="docutils literal notranslate"><span class="pre">Refer</span> <span class="pre">to</span> <span class="pre">affine</span> <span class="pre">for</span> <span class="pre">details</span></code>)</p></li>
<li><p>dec_affine2 (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : second affine used in decoder. (shape: <code class="docutils literal notranslate"><span class="pre">Refer</span> <span class="pre">to</span> <span class="pre">affine</span> <span class="pre">for</span> <span class="pre">details</span></code>)</p></li>
<li><p>dec_layer_norm1 (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : fist layer normalization used in decoder. (shape: <code class="docutils literal notranslate"><span class="pre">Refer</span> <span class="pre">to</span> <span class="pre">layer_normalization</span> <span class="pre">for</span> <span class="pre">details</span></code>)</p></li>
<li><p>dec_layer_norm2 (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : second layer normalization used in decoder. (shape: <code class="docutils literal notranslate"><span class="pre">Refer</span> <span class="pre">to</span> <span class="pre">layer_normalization</span> <span class="pre">for</span> <span class="pre">details</span></code>)</p></li>
<li><p>dec_layer_norm3 (<code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code>) : third layer normalization used in decoder. (shape: <code class="docutils literal notranslate"><span class="pre">Refer</span> <span class="pre">to</span> <span class="pre">layer_normalization</span> <span class="pre">for</span> <span class="pre">details</span></code>)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">name</span></code> option is passed, the parameters become wrapped inside the parameter scope
with the specified name, yielding the same results as the following code.
This can be used to simplify the code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">parametric_scope</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">transformer_decode</span><span class="p">(</span><span class="o">&lt;</span><span class="n">args</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

</div>
<div class="section" id="parameter-initializer">
<h2>Parameter Initializer<a class="headerlink" href="#parameter-initializer" title="Permalink to this headline">¶</a></h2>
<p>Some of the parametric functions optionally takes parameter initializer
listed below.</p>
<span class="target" id="module-nnabla.initializer"></span><dl class="class">
<dt id="nnabla.initializer.BaseInitializer">
<em class="property">class </em><code class="sig-prename descclassname">nnabla.initializer.</code><code class="sig-name descname">BaseInitializer</code><a class="reference internal" href="../../_modules/nnabla/initializer.html#BaseInitializer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.initializer.BaseInitializer" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class of the parameter initializer.</p>
<dl class="method">
<dt id="nnabla.initializer.BaseInitializer.__call__">
<code class="sig-name descname">__call__</code><span class="sig-paren">(</span><em class="sig-param">shape</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/initializer.html#BaseInitializer.__call__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.initializer.BaseInitializer.__call__" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates an array with an initializer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>shape</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a> with the shape created.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Array.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></a></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Subclasses of <a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseInitializer</span></code></a> must override this method.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nnabla.initializer.ConstantInitializer">
<em class="property">class </em><code class="sig-prename descclassname">nnabla.initializer.</code><code class="sig-name descname">ConstantInitializer</code><span class="sig-paren">(</span><em class="sig-param">value=0</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/initializer.html#ConstantInitializer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.initializer.ConstantInitializer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-class docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a></p>
<p>Generates a constant valued array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>value</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – A constant value.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nnabla</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">nnabla.parametric_functions</span> <span class="k">as</span> <span class="nn">PF</span>
<span class="kn">import</span> <span class="nn">nnabla.initializer</span> <span class="k">as</span> <span class="nn">I</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">60</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">])</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">ConstantInitializer</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">ConstantInitializer</span><span class="p">()</span> <span class="c1"># this generates constant valued array of default value 0</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">w_init</span><span class="o">=</span><span class="n">w</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;conv&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="nnabla.initializer.NormalInitializer">
<em class="property">class </em><code class="sig-prename descclassname">nnabla.initializer.</code><code class="sig-name descname">NormalInitializer</code><span class="sig-paren">(</span><em class="sig-param">sigma=1.0</em>, <em class="sig-param">rng=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/initializer.html#NormalInitializer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.initializer.NormalInitializer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-class docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a></p>
<p>Generates a random array from a specified normal distribution.</p>
<div class="math notranslate nohighlight">
\[\mathbf x \sim {\cal N} (\mathbf 0 | \sigma^2 \mathbf I)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sigma</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – <span class="math notranslate nohighlight">\(\sigma\)</span>.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random number generator.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nnabla</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">nnabla.parametric_functions</span> <span class="k">as</span> <span class="nn">PF</span>
<span class="kn">import</span> <span class="nn">nnabla.initializer</span> <span class="k">as</span> <span class="nn">I</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">60</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">])</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">NormalInitializer</span><span class="p">(</span><span class="mf">5e-5</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">NormalInitializer</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">w_init</span><span class="o">=</span><span class="n">w</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;conv&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="nnabla.initializer.UniformInitializer">
<em class="property">class </em><code class="sig-prename descclassname">nnabla.initializer.</code><code class="sig-name descname">UniformInitializer</code><span class="sig-paren">(</span><em class="sig-param">lim=(-1</em>, <em class="sig-param">1)</em>, <em class="sig-param">rng=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/initializer.html#UniformInitializer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.initializer.UniformInitializer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-class docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a></p>
<p>Generates a random array from a specified uniform distribution.</p>
<div class="math notranslate nohighlight">
\[\mathbf x \sim {\cal U} (a, b)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lim</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code></a>) – A tuple of two floats, <span class="math notranslate nohighlight">\((a, b)\)</span>.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random number generator.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nnabla</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">nnabla.parametric_functions</span> <span class="k">as</span> <span class="nn">PF</span>
<span class="kn">import</span> <span class="nn">nnabla.initializer</span> <span class="k">as</span> <span class="nn">I</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">60</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">])</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">UniformInitializer</span><span class="p">()</span> <span class="c1"># this generates uniform distribution within the default range of (-1,1)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">UniformInitializer</span><span class="p">((</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">))</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">w_init</span><span class="o">=</span><span class="n">w</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;conv&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="nnabla.initializer.UniformIntInitializer">
<em class="property">class </em><code class="sig-prename descclassname">nnabla.initializer.</code><code class="sig-name descname">UniformIntInitializer</code><span class="sig-paren">(</span><em class="sig-param">lim=(0</em>, <em class="sig-param">10)</em>, <em class="sig-param">rng=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/initializer.html#UniformIntInitializer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.initializer.UniformIntInitializer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-class docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a></p>
<p>Generates a random array from a specified integer uniform distribution.</p>
<div class="math notranslate nohighlight">
\[\mathbf x \sim {\cal U} ([a, b))\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lim</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – A tuple of two ints, <span class="math notranslate nohighlight">\([a, b)\)</span>.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random number generator.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nnabla</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">nnabla.parametric_functions</span> <span class="k">as</span> <span class="nn">PF</span>
<span class="kn">import</span> <span class="nn">nnabla.initializer</span> <span class="k">as</span> <span class="nn">I</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">60</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">])</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">UniformIntInitializer</span><span class="p">()</span> <span class="c1"># this generates uniform integer distribution within the default range of (0,10)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">UniformIntInitializer</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">w_init</span><span class="o">=</span><span class="n">w</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;conv&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="nnabla.initializer.RangeInitializer">
<em class="property">class </em><code class="sig-prename descclassname">nnabla.initializer.</code><code class="sig-name descname">RangeInitializer</code><span class="sig-paren">(</span><em class="sig-param">start=0</em>, <em class="sig-param">step=1</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/initializer.html#RangeInitializer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.initializer.RangeInitializer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-class docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a></p>
<p>Generates an array with sequence of numbers.</p>
<div class="math notranslate nohighlight">
\[\mathbf x[i] = start + step * i\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – A start value.</p></li>
<li><p><strong>step</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – A step value.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nnabla</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">nnabla.initializer</span> <span class="k">as</span> <span class="nn">I</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">100</span><span class="p">])</span>
<span class="n">x</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">RangeInitializer</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="nnabla.initializer.OrthogonalInitializer">
<em class="property">class </em><code class="sig-prename descclassname">nnabla.initializer.</code><code class="sig-name descname">OrthogonalInitializer</code><span class="sig-paren">(</span><em class="sig-param">gain=1.0</em>, <em class="sig-param">rng=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/initializer.html#OrthogonalInitializer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.initializer.OrthogonalInitializer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nnabla.initializer.BaseInitializer" title="nnabla.initializer.BaseInitializer"><code class="xref py py-class docutils literal notranslate"><span class="pre">nnabla.initializer.BaseInitializer</span></code></a></p>
<p>Generates an orthogonal matrix weights proposed by Saxe et al.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gain</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) – scaling factor which should be decided depending on a type of units.</p></li>
<li><p><strong>rng</strong> (<em>numpy.random.RandomState</em>) – Random number generator.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">nnabla</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">nnabla.parametric_functions</span> <span class="k">as</span> <span class="nn">PF</span>
<span class="kn">import</span> <span class="nn">nnabla.initializer</span> <span class="k">as</span> <span class="nn">I</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">60</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">])</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">OrthogonalInitializer</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span><span class="p">))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">ConstantInitializer</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">w_init</span><span class="o">=</span><span class="n">w</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;conv&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">References</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1312.6120">Saxe, et al. Exact solutions to the nonlinear dynamics of
learning in deep linear neural networks.</a></p></li>
</ul>
</dd></dl>

<dl class="function">
<dt id="nnabla.initializer.calc_normal_std_he_forward">
<code class="sig-prename descclassname">nnabla.initializer.</code><code class="sig-name descname">calc_normal_std_he_forward</code><span class="sig-paren">(</span><em class="sig-param">inmaps</em>, <em class="sig-param">outmaps</em>, <em class="sig-param">kernel=(1</em>, <em class="sig-param">1)</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/initializer.html#calc_normal_std_he_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.initializer.calc_normal_std_he_forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the standard deviation proposed by He et al.</p>
<div class="math notranslate nohighlight">
\[\sigma = \sqrt{\frac{2}{NK}}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inmaps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Map size of an input Variable, <span class="math notranslate nohighlight">\(N\)</span>.</p></li>
<li><p><strong>outmaps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Map size of an output Variable, <span class="math notranslate nohighlight">\(M\)</span>.</p></li>
<li><p><strong>kernel</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Convolution kernel spatial shape.
In above definition, <span class="math notranslate nohighlight">\(K\)</span> is the product of shape dimensions.
In Affine, the default value should be used.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nnabla</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">nnabla.parametric_functions</span> <span class="k">as</span> <span class="nn">PF</span>
<span class="kn">import</span> <span class="nn">nnabla.initializer</span> <span class="k">as</span> <span class="nn">I</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">60</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">])</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">calc_normal_std_he_forward</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">64</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">NormalInitializer</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">w_init</span><span class="o">=</span><span class="n">w</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;conv&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">References</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1502.01852">He, et al. Delving Deep into Rectifiers: Surpassing Human-Level
Performance on ImageNet Classification.</a></p></li>
</ul>
</dd></dl>

<dl class="function">
<dt id="nnabla.initializer.calc_normal_std_he_backward">
<code class="sig-prename descclassname">nnabla.initializer.</code><code class="sig-name descname">calc_normal_std_he_backward</code><span class="sig-paren">(</span><em class="sig-param">inmaps</em>, <em class="sig-param">outmaps</em>, <em class="sig-param">kernel=(1</em>, <em class="sig-param">1)</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/initializer.html#calc_normal_std_he_backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.initializer.calc_normal_std_he_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the standard deviation of He et al. (backward case).</p>
<div class="math notranslate nohighlight">
\[\sigma = \sqrt{\frac{2}{MK}}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inmaps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Map size of an input Variable, <span class="math notranslate nohighlight">\(N\)</span>.</p></li>
<li><p><strong>outmaps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Map size of an output Variable, <span class="math notranslate nohighlight">\(M\)</span>.</p></li>
<li><p><strong>kernel</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Convolution kernel spatial shape.
In above definition, <span class="math notranslate nohighlight">\(K\)</span> is the product of shape dimensions.
In Affine, the default value should be used.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nnabla</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">nnabla.parametric_functions</span> <span class="k">as</span> <span class="nn">PF</span>
<span class="kn">import</span> <span class="nn">nnabla.initializer</span> <span class="k">as</span> <span class="nn">I</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">60</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">])</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">calc_normal_std_he_backward</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">64</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">NormalInitializer</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">w_init</span><span class="o">=</span><span class="n">w</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;conv&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">References</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1502.01852">He, et al. Delving Deep into Rectifiers: Surpassing Human-Level
Performance on ImageNet Classification.</a></p></li>
</ul>
</dd></dl>

<dl class="function">
<dt id="nnabla.initializer.calc_normal_std_glorot">
<code class="sig-prename descclassname">nnabla.initializer.</code><code class="sig-name descname">calc_normal_std_glorot</code><span class="sig-paren">(</span><em class="sig-param">inmaps</em>, <em class="sig-param">outmaps</em>, <em class="sig-param">kernel=(1</em>, <em class="sig-param">1)</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/initializer.html#calc_normal_std_glorot"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.initializer.calc_normal_std_glorot" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the standard deviation proposed by Glorot et al.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We have updated the definition as following from v.1.3. It may affect the
behavior of existing scripts that rely on the default initialization.</p>
</div>
<div class="math notranslate nohighlight">
\[\sigma = \sqrt{\frac{2}{K(N + M)}}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inmaps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Map size of an input Variable, <span class="math notranslate nohighlight">\(N\)</span>.</p></li>
<li><p><strong>outmaps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Map size of an output Variable, <span class="math notranslate nohighlight">\(M\)</span>.</p></li>
<li><p><strong>kernel</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Convolution kernel spatial shape.
In above definition, <span class="math notranslate nohighlight">\(K\)</span> is the product of shape dimensions.
In Affine, the default value should be used.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nnabla</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">nnabla.parametric_functions</span> <span class="k">as</span> <span class="nn">PF</span>
<span class="kn">import</span> <span class="nn">nnabla.initializer</span> <span class="k">as</span> <span class="nn">I</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">60</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">])</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">calc_normal_std_glorot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">64</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">NormalInitializer</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">w_init</span><span class="o">=</span><span class="n">w</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;conv&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">References</p>
<ul class="simple">
<li><p><a class="reference external" href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">Glorot and Bengio. Understanding the difficulty of training deep
feedforward neural networks</a></p></li>
</ul>
</dd></dl>

<dl class="function">
<dt id="nnabla.initializer.calc_uniform_lim_glorot">
<code class="sig-prename descclassname">nnabla.initializer.</code><code class="sig-name descname">calc_uniform_lim_glorot</code><span class="sig-paren">(</span><em class="sig-param">inmaps</em>, <em class="sig-param">outmaps</em>, <em class="sig-param">kernel=(1</em>, <em class="sig-param">1)</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/nnabla/initializer.html#calc_uniform_lim_glorot"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nnabla.initializer.calc_uniform_lim_glorot" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the lower bound and the upper bound of the uniform distribution proposed by Glorot et al.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We have updated the definition as following from v.1.3. It may affect the
behavior of existing scripts that rely on the default initialization.</p>
</div>
<div class="math notranslate nohighlight">
\[\begin{split}b &amp;= \sqrt{\frac{6}{K(N + M)}}\\
a &amp;= -b\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inmaps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Map size of an input Variable, <span class="math notranslate nohighlight">\(N\)</span>.</p></li>
<li><p><strong>outmaps</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – Map size of an output Variable, <span class="math notranslate nohighlight">\(M\)</span>.</p></li>
<li><p><strong>kernel</strong> (<a class="reference external" href="https://docs.python.org/3.6/library/stdtypes.html#tuple" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code></a> of <a class="reference external" href="https://docs.python.org/3.6/library/functions.html#int" title="(in Python v3.6)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a>) – Convolution kernel spatial shape.
In above definition, <span class="math notranslate nohighlight">\(K\)</span> is the product of shape dimensions.
In Affine, the default value should be used.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nnabla</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">nnabla.parametric_functions</span> <span class="k">as</span> <span class="nn">PF</span>
<span class="kn">import</span> <span class="nn">nnabla.initializer</span> <span class="k">as</span> <span class="nn">I</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">60</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">])</span>
<span class="n">lb</span><span class="p">,</span><span class="n">ub</span><span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">64</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">UniformInitializer</span><span class="p">((</span><span class="n">lb</span><span class="p">,</span><span class="n">ub</span><span class="p">))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">I</span><span class="o">.</span><span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">w_init</span><span class="o">=</span><span class="n">w</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;conv&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">References</p>
<ul class="simple">
<li><p><a class="reference external" href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">Glorot and Bengio. Understanding the difficulty of training deep
feedforward neural networks</a></p></li>
</ul>
</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="grad.html" class="btn btn-neutral float-right" title="Grad" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="function.html" class="btn btn-neutral float-left" title="Functions" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Sony Corporation

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>