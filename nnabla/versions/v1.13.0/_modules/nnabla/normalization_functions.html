

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>nnabla.normalization_functions &mdash; Neural Network Libraries 1.13.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> Neural Network Libraries
          

          
          </a>

          
            
            
              <div class="version">
                1.13.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../python.html">Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cpp.html">C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data_exchange_file_format.html">Data exchange file format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../format.html">Data Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python/file_format_converter/file_format_converter.html">File format converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../support_status.html">Support Status</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">Contributing Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../license.html">License</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Neural Network Libraries</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>nnabla.normalization_functions</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for nnabla.normalization_functions</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) 2017 Sony Corporation. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">.function_bases</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="kn">import</span> <span class="n">reduce</span> <span class="k">as</span> <span class="n">rd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">nnabla</span> <span class="k">as</span> <span class="nn">nn</span>


<span class="k">def</span> <span class="nf">_check_axis</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">axis</span> <span class="o">&gt;=</span> <span class="n">ndim</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;axis must be in the range of [0, ndim). axis : </span><span class="si">{}</span><span class="s2">, ndim: </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_force_list</span><span class="p">(</span><span class="n">axis</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;__iter__&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">axis</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">axis</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">_check_batch_axis_and_force_list</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">batch_axis</span><span class="p">):</span>
    <span class="n">batch_axis</span> <span class="o">=</span> <span class="n">_force_list</span><span class="p">(</span><span class="n">batch_axis</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">_axis</span> <span class="ow">in</span> <span class="n">batch_axis</span><span class="p">:</span>
        <span class="n">_check_axis</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">_axis</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">batch_axis</span>


<span class="k">def</span> <span class="nf">_get_axes_excluding</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">_force_list</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">_create_bn_dummy_vars</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">):</span>
    <span class="n">in_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">_force_list</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span>
    <span class="n">adaptive_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
        <span class="n">in_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">axes</span> <span class="k">else</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">in_shape</span><span class="p">)))</span>

    <span class="c1"># create dummy adaptive variables</span>
    <span class="n">assert_flag</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">beta</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">assert_flag</span> <span class="o">|=</span> <span class="n">beta</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">adaptive_shape</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">constant</span><span class="p">(</span><span class="n">val</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">adaptive_shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">gamma</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">assert_flag</span> <span class="o">|=</span> <span class="n">gamma</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">adaptive_shape</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="n">constant</span><span class="p">(</span><span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">adaptive_shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">assert_flag</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;The shapes of beta and gamma must be </span><span class="si">{}</span><span class="s2">.&quot;</span>
            <span class="s2">&quot; If you want to use a tensor with other shape, use arithmetic operators like&quot;</span>
            <span class="s2">&quot; `tensor_normalization(x, axes, beta=None, gamma=None) * gamma + beta`.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">adaptive_shape</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">mean</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># _mean is never used and there is no need to initialize.</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">adaptive_shape</span><span class="p">,</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">variance</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">variance</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">adaptive_shape</span><span class="p">,</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># same above.</span>

    <span class="k">return</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span>


<span class="k">def</span> <span class="nf">_init_beta_gamma</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="p">,</span> <span class="n">param_init</span><span class="p">,</span> <span class="n">no_bias</span><span class="p">,</span> <span class="n">no_scale</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">nnabla.parameter</span> <span class="kn">import</span> <span class="n">get_parameter_or_create</span>
    <span class="kn">from</span> <span class="nn">nnabla.initializer</span> <span class="kn">import</span> <span class="n">ConstantInitializer</span>

    <span class="k">if</span> <span class="n">no_bias</span><span class="p">:</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">beta_init</span> <span class="o">=</span> <span class="n">param_init</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">beta_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">no_scale</span><span class="p">:</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">gamma_init</span> <span class="o">=</span> <span class="n">param_init</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;gamma&#39;</span><span class="p">,</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;gamma&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">gamma_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span>


<span class="k">def</span> <span class="nf">_apply_affine</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">scale</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">*=</span> <span class="n">scale</span>

    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">+=</span> <span class="n">bias</span>

    <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">BatchNormalizationInOutAdapter</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ndim</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span>
        <span class="n">outer_axes</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">))</span><span class="o">.</span><span class="n">difference</span><span class="p">(</span><span class="n">axes</span><span class="p">))</span>
        <span class="n">inner_axes</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">=</span> <span class="n">ndim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outer_axes</span> <span class="o">=</span> <span class="n">outer_axes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transpose_axes</span> <span class="o">=</span> <span class="n">outer_axes</span> <span class="o">+</span> <span class="n">inner_axes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transposed_shape</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inv_transpose_axes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">transpose_axes</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">transposed</span> <span class="o">=</span> <span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_axes</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">transposed_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">transposed_shape</span> <span class="o">==</span> <span class="n">transposed</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;Wrong shape input given.&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transposed_shape</span> <span class="o">=</span> <span class="n">transposed</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">reduced_inner_size</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">transposed</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outer_axes</span><span class="p">):])</span>
        <span class="n">outer_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">transposed</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outer_axes</span><span class="p">)])</span>
        <span class="k">return</span> <span class="n">reshape</span><span class="p">(</span><span class="n">transposed</span><span class="p">,</span> <span class="n">outer_shape</span> <span class="o">+</span> <span class="p">[</span><span class="n">reduced_inner_size</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">inv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">transposed</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transposed_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">transpose</span><span class="p">(</span><span class="n">transposed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inv_transpose_axes</span><span class="p">)</span>


<div class="viewcode-block" id="batch_normalization"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.batch_normalization">[docs]</a><span class="k">def</span> <span class="nf">batch_normalization</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">batch_stat</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_stat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Batch normalization.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{eqnarray}</span>
<span class="sd">          \mu &amp;=&amp; \frac{1}{M} \sum x_i \\</span>
<span class="sd">          \sigma^2 &amp;=&amp; \frac{1}{M} \sum \left(x_i - \mu\right)^2 \\</span>
<span class="sd">          \hat{x}_i &amp;=&amp; \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} \\</span>
<span class="sd">          y_i &amp;=&amp; \hat{x}_i \gamma + \beta.</span>
<span class="sd">        \end{eqnarray}</span>


<span class="sd">    At testing time, the mean and variance values used are those that were computed during training by moving average.</span>

<span class="sd">    References:</span>

<span class="sd">        * `Ioffe and Szegedy, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1502.03167&gt;`_</span>

<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array of input.</span>
<span class="sd">        beta(~nnabla.Variable or None): N-D array of beta which is learned. If None, the bias term is omitted.</span>
<span class="sd">        gamma(~nnabla.Variable or None): N-D array of gamma which is learned. If None, the scale term is omitted.</span>
<span class="sd">        mean(~nnabla.Variable or None):</span>
<span class="sd">            N-D array of running mean (modified during forward execution).</span>
<span class="sd">            If None, dummy variable is created and running mean is not updated.</span>
<span class="sd">            mean=None with batch_stat=False is prohibited.</span>
<span class="sd">        variance(~nnabla.Variable or None):</span>
<span class="sd">            N-D array of running variance (modified during forward execution).</span>
<span class="sd">            If None, dummy variable is created and running variance is not updated.</span>
<span class="sd">            variance=None with batch_stat=False is prohibited.</span>
<span class="sd">        axes(list of int or int): Mean and variance are calculated along these axes.</span>
<span class="sd">        decay_rate(float): Decay rate of running mean and variance.</span>
<span class="sd">        eps(float): Tiny value to avoid zero division by std.</span>
<span class="sd">        batch_stat(bool):</span>
<span class="sd">            Use mini-batch statistics rather than running ones.</span>
<span class="sd">            If False, mean and variance must be `~nnabla.Variable`. (None is prohibited.)</span>
<span class="sd">        output_stat(bool): It true, the batch statistics of mean and variance,</span>
<span class="sd">            will be returned as Variables. They are also differentiable.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns batch normalization output as :obj:`~nnabla.Variable`.</span>
<span class="sd">        If ``output_stat=True``, it also returns the mean and variance</span>
<span class="sd">        of the mini-batch</span>

<span class="sd">        * :obj:`~nnabla.Variable`: Output of the batch normalization</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Mean (if ``output_stat=True`)</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Variance (if ``output_stat=True`)</span>

<span class="sd">    See Also:</span>
<span class="sd">        ``nnabla.function_bases.batch_normalization``.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">.function_bases</span> <span class="kn">import</span> <span class="n">batch_normalization</span> <span class="k">as</span> <span class="n">batch_normalization_base</span>
    <span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">3</span> <span class="k">if</span> <span class="n">output_stat</span> <span class="k">else</span> <span class="mi">1</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">_force_list</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span>

    <span class="k">assert</span> <span class="n">batch_stat</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="n">output_stat</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">batch_stat</span> <span class="ow">and</span> <span class="p">(</span><span class="n">mean</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">variance</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;If batch_stat is False, mean and variable must not be None.&quot;</span><span class="p">)</span>

    <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span> <span class="o">=</span> <span class="n">_create_bn_dummy_vars</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">batch_stat</span> <span class="ow">and</span> <span class="p">(</span><span class="n">mean</span><span class="o">.</span><span class="n">parent</span> <span class="ow">or</span> <span class="n">variance</span><span class="o">.</span><span class="n">parent</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;if batch_stat is True, mean and variable must not have a parent function.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">batch_normalization_base</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span>
                                        <span class="n">axes</span><span class="o">=</span><span class="n">axes</span><span class="p">,</span>
                                        <span class="n">decay_rate</span><span class="o">=</span><span class="n">decay_rate</span><span class="p">,</span>
                                        <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
                                        <span class="n">batch_stat</span><span class="o">=</span><span class="n">batch_stat</span><span class="p">,</span>
                                        <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">)</span>

    <span class="n">in_adapter</span> <span class="o">=</span> <span class="n">BatchNormalizationInOutAdapter</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
    <span class="n">param_adapter</span> <span class="o">=</span> <span class="n">BatchNormalizationInOutAdapter</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
    <span class="n">inp</span> <span class="o">=</span> <span class="n">in_adapter</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">param_adapter</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="n">param_adapter</span><span class="p">(</span><span class="n">gamma</span><span class="p">)</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">param_adapter</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">param_adapter</span><span class="p">(</span><span class="n">variance</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">n_outputs</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">batch_normalization_base</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span>
                                       <span class="n">axes</span><span class="o">=</span><span class="p">[</span><span class="n">axis</span><span class="p">],</span>
                                       <span class="n">decay_rate</span><span class="o">=</span><span class="n">decay_rate</span><span class="p">,</span>
                                       <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
                                       <span class="n">batch_stat</span><span class="o">=</span><span class="n">batch_stat</span><span class="p">,</span>
                                       <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">in_adapter</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span> <span class="o">=</span> <span class="n">batch_normalization_base</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span>
                                                   <span class="n">axes</span><span class="o">=</span><span class="p">[</span><span class="n">axis</span><span class="p">],</span>
                                                   <span class="n">decay_rate</span><span class="o">=</span><span class="n">decay_rate</span><span class="p">,</span>
                                                   <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
                                                   <span class="n">batch_stat</span><span class="o">=</span><span class="n">batch_stat</span><span class="p">,</span>
                                                   <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">in_adapter</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">param_adapter</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">param_adapter</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">variance</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span></div>


<div class="viewcode-block" id="fused_batch_normalization"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.fused_batch_normalization">[docs]</a><span class="k">def</span> <span class="nf">fused_batch_normalization</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">batch_stat</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">output_stat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Batch normalization fused with an add operation and an activation.</span>

<span class="sd">    References:</span>

<span class="sd">        * `Ioffe and Szegedy, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1502.03167&gt;`_</span>

<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array of input.</span>
<span class="sd">        beta(~nnabla.Variable or None): N-D array of beta which is learned. If None, the bias term is omitted.</span>
<span class="sd">        gamma(~nnabla.Variable or None): N-D array of gamma which is learned. If None, the scale term is omitted.</span>
<span class="sd">        mean(~nnabla.Variable or None):</span>
<span class="sd">            N-D array of running mean (modified during forward execution).</span>
<span class="sd">            If None, dummy variable is created and running mean is never updated.</span>
<span class="sd">            mean=None with batch_stat=False is prohibited.</span>
<span class="sd">        variance(~nnabla.Variable):</span>
<span class="sd">            N-D array of running variance (modified during forward execution).</span>
<span class="sd">            If None, dummy variable is created and running variance is not updated.</span>
<span class="sd">            variance=None with batch_stat=False is prohibited.</span>
<span class="sd">        z(~nnabla.Variable, optional): N-D array</span>
<span class="sd">        axes(list of int or int): Mean and variance are calculated along these axes.</span>
<span class="sd">        decay_rate(float): Decay rate of running mean and variance.</span>
<span class="sd">        eps(float): Tiny value to avoid zero division by std.</span>
<span class="sd">        batch_stat(bool):</span>
<span class="sd">            Use mini-batch statistics rather than running ones.</span>
<span class="sd">            If False, mean and variance must be `~nnabla.Variable`. (None is prohibited.)</span>
<span class="sd">        nonlinearity(str): Nonlinearity chosen from relu. Default is relu.</span>
<span class="sd">        output_stat(bool): It true, the batch statistics of mean and variance,</span>
<span class="sd">            will be returned as Variables. They are also differentiable.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns batch normalization output as :obj:`~nnabla.Variable`.</span>
<span class="sd">        If ``output_stat=True``, it also returns the mean and variance</span>
<span class="sd">        of the mini-batch</span>

<span class="sd">        * :obj:`~nnabla.Variable`: Output of the batch normalization</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Mean (if ``output_stat=True`)</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Variance (if ``output_stat=True`)</span>

<span class="sd">    See Also:</span>
<span class="sd">        ``nnabla.function_bases.batch_normalization``.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">.function_bases</span> <span class="kn">import</span> <span class="n">fused_batch_normalization</span> <span class="k">as</span> <span class="n">fused_batch_normalization_base</span>
    <span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">3</span> <span class="k">if</span> <span class="n">output_stat</span> <span class="k">else</span> <span class="mi">1</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">_force_list</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">batch_stat</span> <span class="ow">and</span> <span class="p">(</span><span class="n">mean</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">variance</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;If batch_stat is False, mean and variable must not be None.&quot;</span><span class="p">)</span>

    <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span> <span class="o">=</span> <span class="n">_create_bn_dummy_vars</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">batch_stat</span> <span class="ow">and</span> <span class="p">(</span><span class="n">mean</span><span class="o">.</span><span class="n">parent</span> <span class="ow">or</span> <span class="n">variance</span><span class="o">.</span><span class="n">parent</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;if batch_stat is True, mean and variable must not have a parent function.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">fused_batch_normalization_base</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span>
                                              <span class="n">axes</span><span class="o">=</span><span class="n">axes</span><span class="p">,</span>
                                              <span class="n">decay_rate</span><span class="o">=</span><span class="n">decay_rate</span><span class="p">,</span>
                                              <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
                                              <span class="n">batch_stat</span><span class="o">=</span><span class="n">batch_stat</span><span class="p">,</span>
                                              <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span>
                                              <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">)</span>

    <span class="n">in_adapter</span> <span class="o">=</span> <span class="n">BatchNormalizationInOutAdapter</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
    <span class="n">param_adapter</span> <span class="o">=</span> <span class="n">BatchNormalizationInOutAdapter</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
    <span class="n">inp</span> <span class="o">=</span> <span class="n">in_adapter</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">in_adapter</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">param_adapter</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="n">param_adapter</span><span class="p">(</span><span class="n">gamma</span><span class="p">)</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">param_adapter</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">param_adapter</span><span class="p">(</span><span class="n">variance</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">output_stat</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">fused_batch_normalization_base</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span>
                                             <span class="n">axes</span><span class="o">=</span><span class="p">[</span><span class="n">axis</span><span class="p">],</span>
                                             <span class="n">decay_rate</span><span class="o">=</span><span class="n">decay_rate</span><span class="p">,</span>
                                             <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
                                             <span class="n">batch_stat</span><span class="o">=</span><span class="n">batch_stat</span><span class="p">,</span>
                                             <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span>
                                             <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">in_adapter</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="n">out</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span> <span class="o">=</span> <span class="n">fused_batch_normalization_base</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span>
                                                         <span class="n">axes</span><span class="o">=</span><span class="p">[</span><span class="n">axis</span><span class="p">],</span>
                                                         <span class="n">decay_rate</span><span class="o">=</span><span class="n">decay_rate</span><span class="p">,</span>
                                                         <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
                                                         <span class="n">batch_stat</span><span class="o">=</span><span class="n">batch_stat</span><span class="p">,</span>
                                                         <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span>
                                                         <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">in_adapter</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">param_adapter</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">param_adapter</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">variance</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span></div>


<div class="viewcode-block" id="sync_batch_normalization"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.sync_batch_normalization">[docs]</a><span class="k">def</span> <span class="nf">sync_batch_normalization</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">comm</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="s2">&quot;world&quot;</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">batch_stat</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_stat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Synchronized batch normalization.</span>

<span class="sd">    For some tasks (e.g., semantic segmentation), batch size will be too small and BatchNormalization layer might not work well.</span>
<span class="sd">    SyncBatchNorlization layer solves these problems by synchronizing batch stats (mean and var) between multiple processes.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{eqnarray}</span>
<span class="sd">          \mu &amp;=&amp; \frac{1}{M} \sum x_i \\</span>
<span class="sd">          \sigma^2 &amp;=&amp; \frac{1}{M} \left(\sum x_i - \mu\right)^2 \\</span>
<span class="sd">          \hat{x}_i &amp;=&amp; \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} \\</span>
<span class="sd">          y_i &amp;=&amp; \hat{x}_i \gamma + \beta.</span>
<span class="sd">        \end{eqnarray}</span>

<span class="sd">    References:</span>

<span class="sd">        * Implementing Synchronized Multi-GPU Batch Normalization https://hangzhang.org/PyTorch-Encoding/notes/syncbn.html</span>

<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array of input.</span>
<span class="sd">        beta(~nnabla.Variable or None): N-D array of beta which is learned. If None, the bias term is omitted.</span>
<span class="sd">        gamma(~nnabla.Variable or None): N-D array of gamma which is learned. If None, the scale term is omitted.</span>
<span class="sd">        mean(~nnabla.Variable or None):</span>
<span class="sd">            N-D array of running mean (modified during forward execution).</span>
<span class="sd">            If None, dummy variable is created and running mean is never updated.</span>
<span class="sd">            mean=None with batch_stat=False is prohibited.</span>
<span class="sd">        variance(~nnabla.Variable or None):</span>
<span class="sd">            N-D array of running variance (modified during forward execution).</span>
<span class="sd">            If None, dummy variable is created and running variance is never updated.</span>
<span class="sd">            variance=None with batch_stat=False is prohibited.</span>
<span class="sd">        comm(~nnabla.communicators.Communicator): The communicator</span>
<span class="sd">        group(string): The name of the communicator group</span>
<span class="sd">        axes(list of int or int): Mean and variance are calculated along these axes.</span>
<span class="sd">        decay_rate(float): Decay rate of running mean and variance.</span>
<span class="sd">        eps(float): Tiny value to avoid zero division by std.</span>
<span class="sd">        batch_stat(bool):</span>
<span class="sd">            Use mini-batch statistics rather than running ones.</span>
<span class="sd">            If False, mean and variance must be `~nnabla.Variable`. (None is prohibited.)</span>
<span class="sd">        output_stat(bool): It true, the batch statistics of mean and variance,</span>
<span class="sd">            will be returned as Variables. They are also differentiable.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns batch normalization output as :obj:`~nnabla.Variable`.</span>
<span class="sd">        If ``output_stat=True``, it also returns the mean and variance</span>
<span class="sd">        of the mini-batch</span>

<span class="sd">        * :obj:`~nnabla.Variable`: Output of the batch normalization</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Mean (if ``output_stat=True`)</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Variance (if ``output_stat=True`)</span>

<span class="sd">    See Also:</span>
<span class="sd">        ``nnabla.function_bases.batch_normalization``.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">.function_bases</span> <span class="kn">import</span> <span class="n">sync_batch_normalization</span> <span class="k">as</span> <span class="n">batch_normalization_base</span>
    <span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">3</span> <span class="k">if</span> <span class="n">output_stat</span> <span class="k">else</span> <span class="mi">1</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">_force_list</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">batch_stat</span> <span class="ow">and</span> <span class="p">(</span><span class="n">mean</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">variance</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;If batch_stat is False, mean and variable must not be None.&quot;</span><span class="p">)</span>

    <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span> <span class="o">=</span> <span class="n">_create_bn_dummy_vars</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">batch_normalization_base</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span>
                                    <span class="n">comm</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">,</span>
                                    <span class="n">axes</span><span class="o">=</span><span class="n">axes</span><span class="p">,</span>
                                    <span class="n">decay_rate</span><span class="o">=</span><span class="n">decay_rate</span><span class="p">,</span>
                                    <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
                                    <span class="n">batch_stat</span><span class="o">=</span><span class="n">batch_stat</span><span class="p">,</span>
                                    <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">tensor_normalization</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">output_stat</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    General tensor normalization.</span>
<span class="sd">    Input variable `x` is normalized by mean and std calculated by `x` itself.</span>
<span class="sd">    Mean and variance are calculated along `axes`.</span>
<span class="sd">    For example, if the input shape is (B, C, H, W) and axes is [0, 1],</span>
<span class="sd">     the shape of calculated mean and std are (B, C, 1 ,1).</span>

<span class="sd">    Note:</span>
<span class="sd">        Currently tensor_normalization is implemented not as cpp function</span>
<span class="sd">        but as wrapper function which just calls F.batch_normalization internally.</span>
<span class="sd">        That means F.reshape or F.transpose may be additionally called to satisfy the condition required by F.batch_normalization,</span>
<span class="sd">        and if you serialize graphs including tensor_normalization to nnp file,</span>
<span class="sd">        that nnp includes reshape, transpose and batch_normalization rather than tensor_normalization layer itself.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Variable): N-D array of input variable.</span>
<span class="sd">        axes (int or repeated int): Mean and variance are calculated along these axes.</span>
<span class="sd">        beta (Variable or None): An Adaptive biases. If None, the bias term is omitted.</span>
<span class="sd">        gamma (Variable or None): An Adaptive scales. If None, the scale term is omitted.</span>
<span class="sd">        eps (float): Tiny value to avoid zero division by std.</span>
<span class="sd">        output_stat(bool): It true, the batch statistics of mean and variance.</span>
<span class="sd">        will be returned as Variables. They are also differentiable.</span>

<span class="sd">    Returns:</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Normalized output variable.</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Mean (if ``output_stat=True`)</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Std (if ``output_stat=True`)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># todo: should implement cpp function rather than just calling batch_normalization in python.</span>

    <span class="k">return</span> <span class="n">batch_normalization</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
                               <span class="n">axes</span><span class="o">=</span><span class="n">axes</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">batch_stat</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_stat</span><span class="o">=</span><span class="n">output_stat</span><span class="p">)</span>


<div class="viewcode-block" id="weight_standardization"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.weight_standardization">[docs]</a><span class="k">def</span> <span class="nf">weight_standardization</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">channel_axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">output_stat</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies Weight Standardization over an input weight, which is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">      \begin{eqnarray}</span>
<span class="sd">        \mu_{W_i} &amp;=&amp; \frac{1}{I} \sum_{j=1}^{I} W_{ij} \\</span>
<span class="sd">        \sigma_{W_i} &amp;=&amp; \sqrt{\frac{1}{I} \sum_{i=1}^{I} \left(W_{ij} - \mu_{W_{i}}\right)^2 + \epsilon} \\</span>
<span class="sd">        \hat{W_{ij}} &amp;=&amp; \frac{W_{ij} - \mu_{W_i}}{\sigma_{W_i}} \\</span>
<span class="sd">        y &amp;=&amp; \hat{W} \ast x</span>
<span class="sd">      \end{eqnarray}</span>

<span class="sd">    Example:</span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">            import numpy as np</span>
<span class="sd">            import nnabla as nn</span>
<span class="sd">            import nnabla.functions as F</span>
<span class="sd">            import nnabla.parametric_functions as PF</span>

<span class="sd">            rng = np.random.RandomState(313)</span>
<span class="sd">            x = nn.Variable.from_numpy_array(rng.randn(*(32, 16, 3, 3)))</span>

<span class="sd">            # For convolution:</span>

<span class="sd">            def ws_callback_conv(w):</span>
<span class="sd">                return F.weight_standardization(w, channel_axis=0)</span>

<span class="sd">            y = PF.convolution(x, 10, (2, 2), apply_w=ws_callback_conv)</span>

<span class="sd">            # For affine:</span>

<span class="sd">            def ws_callback_affine(w): </span>
<span class="sd">                return F.weight_standardization(w, channel_axis=1)</span>

<span class="sd">            y = PF.affine(x, 10, apply_w=ws_callback_affine)</span>


<span class="sd">    References:</span>

<span class="sd">      * `Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, Alan Yuille, Weight Standardization</span>
<span class="sd">        &lt;https://arxiv.org/pdf/1903.10520v1.pdf&gt;`_</span>

<span class="sd">    Args:</span>
<span class="sd">        w (Variable): A weight variable.</span>
<span class="sd">        channel_axis (int): An axis for output channel. Default value is 0 which assumes the weights of convolution.</span>
<span class="sd">        eps (float): Tiny value to avoid zero division by std.</span>
<span class="sd">        output_stat(bool): If true, the batch statistics of mean and variance.</span>

<span class="sd">    Returns:</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Standardized output weight.</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Mean (if ``output_stat=True`)</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Std (if ``output_stat=True`)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># check channel axis</span>
    <span class="n">_check_axis</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">channel_axis</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tensor_normalization</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">channel_axis</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">output_stat</span><span class="o">=</span><span class="n">output_stat</span><span class="p">)</span></div>


<div class="viewcode-block" id="layer_normalization"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.layer_normalization">[docs]</a><span class="k">def</span> <span class="nf">layer_normalization</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">output_stat</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies Layer Normalization over an input tensor, which is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">      \begin{eqnarray}</span>
<span class="sd">        \mu^l &amp;=&amp; \frac{1}{H} \sum_{i=1}^{H} x_i^l \\</span>
<span class="sd">        \sigma^l &amp;=&amp; \sqrt{\frac{1}{H} \sum_{i=1}^{H} \left(x_i^l - \mu^l\right)^2 + \epsilon} \\</span>
<span class="sd">        y &amp;=&amp; \frac{x - \mu^l}{\sigma^l} \gamma + \beta</span>
<span class="sd">      \end{eqnarray}</span>

<span class="sd">    where :math:`x` and :math:`y` are input and output variable,</span>
<span class="sd">    :math:`\mu^l` and :math:`\sigma^l` are the mean and std of each layer which is separately calculated for each batch,</span>
<span class="sd">    and :math:`\beta` and :math:`\gamma` are adaptive biases and gains.</span>

<span class="sd">    If the input shape is [B, C, H, W] (= batch_axis=0), the shape of calculated mean and std are [B, 1, 1, 1]</span>

<span class="sd">    References:</span>

<span class="sd">        * `Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton, Layer Normalization.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1607.06450&gt;`_</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Variable): An input variable.</span>
<span class="sd">        beta (Variable or None): An Adaptive biases. If None, the bias term is omitted.</span>
<span class="sd">        gamma (Variable or None): An Adaptive gains. If None, the scale term is omitted.</span>
<span class="sd">        batch_axis (int or repeated int): Axes mean and variance are taken.</span>
<span class="sd">        eps (float): Tiny value to avoid zero division by std.</span>
<span class="sd">        output_stat(bool): If true, calculated mean and variance are also returned.</span>

<span class="sd">    Returns:</span>
<span class="sd">        * :obj:`~nnabla.Variable`: output variable which is normalized its statics and rescaled by alpha and beta.</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Mean (if ``output_stat=True`).</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Std (if ``output_stat=True`)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">batch_axis</span> <span class="o">=</span> <span class="n">_check_batch_axis_and_force_list</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">batch_axis</span><span class="p">)</span>

    <span class="c1"># cannot broadcast beta &amp; gamma from [1, C, 1, 1] to [N, 1 ,1 ,1],</span>
    <span class="c1"># so these adaptation is applied after calling bn with dummy beta &amp; gamma.</span>
    <span class="c1"># (Currently bn only accepts the case when reduction shape and adaptive parameter shape are the same.)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">tensor_normalization</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">batch_axis</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">output_stat</span><span class="o">=</span><span class="n">output_stat</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">output_stat</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_apply_affine</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span>

    <span class="n">y</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">out</span>
    <span class="k">return</span> <span class="n">_apply_affine</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">beta</span><span class="p">),</span> <span class="n">mean</span><span class="p">,</span> <span class="n">var</span></div>


<div class="viewcode-block" id="instance_normalization"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.instance_normalization">[docs]</a><span class="k">def</span> <span class="nf">instance_normalization</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">channel_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">output_stat</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies Instance Normalization over an input tensor, which is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">      \begin{eqnarray}</span>
<span class="sd">        \mu^i &amp;=&amp; \frac{1}{H} \sum_{i=1}^{H} x_i^i \\</span>
<span class="sd">        \sigma^i &amp;=&amp; \sqrt{\frac{1}{H} \sum_{i=1}^{H} \left(x_i^i - \mu^i\right)^2 + \epsilon} \\</span>
<span class="sd">        y &amp;=&amp; \frac{x - \mu^i}{\sigma^i} \gamma + \beta</span>
<span class="sd">      \end{eqnarray}</span>

<span class="sd">    where :math:`x` and :math:`y` are input and output variable,</span>
<span class="sd">    :math:`\mu^i` and :math:`\sigma^i` are the mean and std of each instance which is separately calculated for each batch and channel,</span>
<span class="sd">    and :math:`\gamma` and :math:`\beta` are adaptive gains and biases.</span>

<span class="sd">    If the input shape is [B, C, H, W] (= channel_axis=1, batch_axis=0), the shape of calculated mean and std are [B, C, 1, 1]</span>

<span class="sd">    References:</span>

<span class="sd">        * `Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky, Instance Normalization: The Missing Ingredient for Fast Stylization.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1607.08022&gt;`_</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Variable): An input variable.</span>
<span class="sd">        beta (Variable): An Adaptive biases.</span>
<span class="sd">        gamma (Variable): An Adaptive gains.</span>
<span class="sd">        channel_axis (int): Channel axis.</span>
<span class="sd">        batch_axis (int or repeated int): Batch axes.</span>
<span class="sd">        eps (float): Tiny value to avoid zero division by std.</span>
<span class="sd">        output_stat(bool): If true, the batch statistics of mean and variance.</span>

<span class="sd">    Returns:</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Normalized output variable.</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Mean (if ``output_stat=True`)</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Std (if ``output_stat=True`)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># check channel axis</span>
    <span class="n">_check_axis</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">channel_axis</span><span class="p">)</span>

    <span class="c1"># check batch axis</span>
    <span class="n">batch_axis</span> <span class="o">=</span> <span class="n">_check_batch_axis_and_force_list</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">batch_axis</span><span class="p">)</span>

    <span class="c1"># check whether broadcast is needed or not.</span>
    <span class="c1"># Unlike layer_norm and group_norm, only instance_norm can use bn scale bias &amp; scale adaptation</span>
    <span class="c1"># by broadcasting channel axis to channel * batch axis. (like [1, C, 1, 1] -&gt; [N, C, 1, 1])</span>

    <span class="n">adapt_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">))]</span>
    <span class="k">for</span> <span class="n">baxis</span> <span class="ow">in</span> <span class="n">batch_axis</span><span class="p">:</span>
        <span class="n">adapt_shape</span><span class="p">[</span><span class="n">baxis</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">baxis</span><span class="p">]</span>
    <span class="n">adapt_shape</span><span class="p">[</span><span class="n">channel_axis</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">channel_axis</span><span class="p">]</span>
    <span class="n">adapt_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">adapt_shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">beta</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">beta</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">adapt_shape</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">beta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">channel_axis</span><span class="p">]</span> <span class="o">==</span> <span class="n">adapt_shape</span><span class="p">[</span><span class="n">channel_axis</span><span class="p">],</span>\
            <span class="s2">&quot;channel size of beta: </span><span class="si">{}</span><span class="s2"> != channel size of x (</span><span class="si">{}</span><span class="s2">).&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">beta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">channel_axis</span><span class="p">],</span>
                                                                         <span class="n">adapt_shape</span><span class="p">[</span><span class="n">channel_axis</span><span class="p">])</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">broadcast</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">adapt_shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">gamma</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">gamma</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">adapt_shape</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">gamma</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">channel_axis</span><span class="p">]</span> <span class="o">==</span> <span class="n">adapt_shape</span><span class="p">[</span><span class="n">channel_axis</span><span class="p">],</span> \
            <span class="s2">&quot;channel size of gamma: </span><span class="si">{}</span><span class="s2"> != channel size of x (</span><span class="si">{}</span><span class="s2">).&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">gamma</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">channel_axis</span><span class="p">],</span>
                                                                          <span class="n">adapt_shape</span><span class="p">[</span><span class="n">channel_axis</span><span class="p">])</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="n">broadcast</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">adapt_shape</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tensor_normalization</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch_axis</span> <span class="o">+</span> <span class="p">[</span><span class="n">channel_axis</span><span class="p">,</span> <span class="p">],</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">output_stat</span><span class="p">)</span></div>


<div class="viewcode-block" id="group_normalization"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.group_normalization">[docs]</a><span class="k">def</span> <span class="nf">group_normalization</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="n">channel_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">output_stat</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies Group Normalization over an input tensor, which is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">      \begin{eqnarray}</span>
<span class="sd">        \mu^g &amp;=&amp; \frac{1}{H} \sum_{i=1}^{H} x_i^g \\</span>
<span class="sd">        \sigma^g &amp;=&amp; \sqrt{\frac{1}{H} \sum_{i=1}^{H} \left(x_i^g - \mu^g\right)^2 + \epsilon} \\</span>
<span class="sd">        y &amp;=&amp; \frac{x - \mu^g}{\sigma^g} \gamma + \beta</span>
<span class="sd">      \end{eqnarray}</span>

<span class="sd">    where :math:`x` and :math:`y` are input and output variable,</span>
<span class="sd">    :math:`\mu^g` and :math:`\sigma^g` are the mean and std of each group which contains `num_channels / num_groups` channels,</span>
<span class="sd">    and :math:`\gamma` and :math:`\beta` are adaptive gains and biases.</span>

<span class="sd">    The input channels, specified by :attr:`channel_axis`, are separated into :attr:`num_groups` groups,</span>
<span class="sd">    and the mean and std are calculated over the each group.</span>
<span class="sd">    For example, if the input shape is [B, C, H, W] (= channel_axis=1, batch_axis=0),</span>
<span class="sd">    an input variable is once reshaped to [B, num_groups, C / num_groups, H, W]</span>
<span class="sd">    and standardize by its mean and std whose shapes are [B, num_groups, 1, 1, 1].</span>
<span class="sd">    Finally, an output variable is reshaped again to the original input shape (= [B, C, H, W] in the case above).</span>

<span class="sd">    References:</span>

<span class="sd">        * `Yuxin Wu, Kaiming He, Group Normalization.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1803.08494&gt;`_</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Variable): An input variable.</span>
<span class="sd">        beta (Variable or None): An Adaptive biases. If None, the bias term is omitted.</span>
<span class="sd">        gamma (Variable or None): An Adaptive gains. If None, the scale term is omitted.</span>
<span class="sd">        num_groups (int): A number of groups. The channel dim of &#39;x&#39; must be integer multiple of `num_groups`.</span>
<span class="sd">        channel_axis (int): Channel axis.</span>
<span class="sd">        batch_axis (int or repeated int): Batch axes.</span>
<span class="sd">        eps (float): Tiny value to avoid zero division by std.</span>
<span class="sd">        output_stat(bool): If true, the batch statistics of mean and variance.</span>

<span class="sd">    Returns:</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Normalized output variable.</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Mean (if ``output_stat=True`)</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Std (if ``output_stat=True`)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_axis</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">channel_axis</span><span class="p">)</span>

    <span class="n">cdim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">channel_axis</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">cdim</span> <span class="o">%</span> <span class="n">num_groups</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Channel dim (</span><span class="si">{}</span><span class="s2">) must be integer multiple of num_groups (</span><span class="si">{}</span><span class="s2">).&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cdim</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">))</span>

    <span class="n">shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="n">channel_axis</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">num_groups</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">cdim</span> <span class="o">/</span> <span class="n">num_groups</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">channel_axis</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">shape</span> <span class="o">+=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">channel_axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]</span>

    <span class="c1"># create dummy adaptive constants and pass these to BN.</span>
    <span class="c1"># GN normalizes input along group axis but applies adaptive rescaling along the original channel axis,</span>
    <span class="c1"># so we have to apply adaptive scaling after reshaping the output from batch normalization.</span>
    <span class="c1"># (Currently bn only accepts the case when reduction shape and adaptive parameter shape are the same.)</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">instance_normalization</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span> <span class="n">beta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">channel_axis</span><span class="o">=</span><span class="n">channel_axis</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="n">batch_axis</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">output_stat</span><span class="o">=</span><span class="n">output_stat</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">output_stat</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_apply_affine</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">scale</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span>

    <span class="n">y</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">out</span>
    <span class="k">return</span> <span class="n">_apply_affine</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">scale</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">beta</span><span class="p">),</span> <span class="n">mean</span><span class="p">,</span> <span class="n">var</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Sony Corporation

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>