

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>NNabla Python API Demonstration Tutorial &mdash; Neural Network Libraries 1.13.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="NNabla Models Finetuning Tutorial" href="model_finetuning.html" />
    <link rel="prev" title="NNabla by Examples" href="by_examples.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> Neural Network Libraries
          

          
          </a>

          
            
            
              <div class="version">
                1.13.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../python.html">Python Package</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../installation.html">Python Package Installation</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../tutorial.html">Python API Tutorial</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="by_examples.html">NNabla by Examples</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">NNabla Python API Demonstration Tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#ndarray">NdArray</a></li>
<li class="toctree-l4"><a class="reference internal" href="#variable">Variable</a></li>
<li class="toctree-l4"><a class="reference internal" href="#function">Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="#parametric-function">Parametric Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="#mlp-example-for-explanation">MLP Example For Explanation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#imperative-mode">Imperative Mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="#solver">Solver</a></li>
<li class="toctree-l4"><a class="reference internal" href="#toy-problem-to-demonstrate-training">Toy Problem To Demonstrate Training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="model_finetuning.html">NNabla Models Finetuning Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_finetuning.html#finetuning-more">Finetuning more</a></li>
<li class="toctree-l3"><a class="reference internal" href="debugging.html">Debugging</a></li>
<li class="toctree-l3"><a class="reference internal" href="dynamic_and_static_nn.html">Static vs Dynamic Neural Networks in NNabla</a></li>
<li class="toctree-l3"><a class="reference internal" href="graph_converters.html">Graph Converters</a></li>
<li class="toctree-l3"><a class="reference internal" href="mixed_precision_training.html">Mixed Precision Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="multi_device_training.html">Data Parallel Distributed Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="function_list_and_converter.html">Function list and converter</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../command_line_interface.html">Python Command Line Interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html">Python API Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html">Python API Reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../cpp.html">C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data_exchange_file_format.html">Data exchange file format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../format.html">Data Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../file_format_converter/file_format_converter.html">File format converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../support_status.html">Support Status</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">Contributing Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../license.html">License</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Neural Network Libraries</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../python.html">Python Package</a> &raquo;</li>
        
          <li><a href="../tutorial.html">Python API Tutorial</a> &raquo;</li>
        
      <li>NNabla Python API Demonstration Tutorial</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/python/tutorial/python_api.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="nnabla-python-api-demonstration-tutorial">
<h1>NNabla Python API Demonstration Tutorial<a class="headerlink" href="#nnabla-python-api-demonstration-tutorial" title="Permalink to this headline">¶</a></h1>
<p>Let us import nnabla first, and some additional useful tools.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># python2/3 compatibility</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nnabla</span> <span class="k">as</span> <span class="nn">nn</span>  <span class="c1"># Abbreviate as nn for convenience.</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2017</span><span class="o">-</span><span class="mi">09</span><span class="o">-</span><span class="mi">27</span> <span class="mi">14</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mi">30</span><span class="p">,</span><span class="mi">785</span> <span class="p">[</span><span class="n">nnabla</span><span class="p">][</span><span class="n">INFO</span><span class="p">]:</span> <span class="n">Initializing</span> <span class="n">CPU</span> <span class="n">extension</span><span class="o">...</span>
</pre></div>
</div>
<div class="section" id="ndarray">
<h2>NdArray<a class="headerlink" href="#ndarray" title="Permalink to this headline">¶</a></h2>
<p>NdArray is a data container of a multi-dimensional array. NdArray is
device (e.g. CPU, CUDA) and type (e.g. uint8, float32) agnostic, in
which both type and device are implicitly casted or transferred when it
is used. Below, you create a NdArray with a shape of <code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">3,</span> <span class="pre">4)</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NdArray</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
<p>You can see the values held inside <code class="docutils literal notranslate"><span class="pre">a</span></code> by the following. The values
are not initialized, and are created as float32 by default.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[[[</span>  <span class="mf">9.42546995e+24</span>   <span class="mf">4.56809286e-41</span>   <span class="mf">8.47690058e-38</span>   <span class="mf">0.00000000e+00</span><span class="p">]</span>
  <span class="p">[</span>  <span class="mf">7.38056336e+34</span>   <span class="mf">7.50334969e+28</span>   <span class="mf">1.17078231e-32</span>   <span class="mf">7.58387310e+31</span><span class="p">]</span>
  <span class="p">[</span>  <span class="mf">7.87001454e-12</span>   <span class="mf">9.84394250e-12</span>   <span class="mf">6.85712044e+22</span>   <span class="mf">1.81785692e+31</span><span class="p">]]</span>

 <span class="p">[[</span>  <span class="mf">1.84681296e+25</span>   <span class="mf">1.84933247e+20</span>   <span class="mf">4.85656319e+33</span>   <span class="mf">2.06176836e-19</span><span class="p">]</span>
  <span class="p">[</span>  <span class="mf">6.80020530e+22</span>   <span class="mf">1.69307638e+22</span>   <span class="mf">2.11235872e-19</span>   <span class="mf">1.94316151e-19</span><span class="p">]</span>
  <span class="p">[</span>  <span class="mf">1.81805047e+31</span>   <span class="mf">3.01289097e+29</span>   <span class="mf">2.07004908e-19</span>   <span class="mf">1.84648795e+25</span><span class="p">]]]</span>
</pre></div>
</div>
<p>The accessor <code class="docutils literal notranslate"><span class="pre">.data</span></code> returns a reference to the values of NdArray as
<code class="docutils literal notranslate"><span class="pre">numpy.ndarray</span></code>. You can modify these by using the NumPy API as
follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;[Substituting random values]&#39;</span><span class="p">)</span>
<span class="n">a</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;[Slicing]&#39;</span><span class="p">)</span>
<span class="n">a</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">Substituting</span> <span class="n">random</span> <span class="n">values</span><span class="p">]</span>
<span class="p">[[[</span> <span class="mf">0.36133638</span>  <span class="mf">0.22121875</span> <span class="o">-</span><span class="mf">1.5912329</span>  <span class="o">-</span><span class="mf">0.33490974</span><span class="p">]</span>
  <span class="p">[</span> <span class="mf">1.35962474</span>  <span class="mf">0.2165522</span>   <span class="mf">0.54483992</span> <span class="o">-</span><span class="mf">0.61813235</span><span class="p">]</span>
  <span class="p">[</span><span class="o">-</span><span class="mf">0.13718799</span> <span class="o">-</span><span class="mf">0.44104072</span> <span class="o">-</span><span class="mf">0.51307833</span>  <span class="mf">0.73900551</span><span class="p">]]</span>

 <span class="p">[[</span><span class="o">-</span><span class="mf">0.59464753</span> <span class="o">-</span><span class="mf">2.17738533</span> <span class="o">-</span><span class="mf">0.28626776</span> <span class="o">-</span><span class="mf">0.45654735</span><span class="p">]</span>
  <span class="p">[</span> <span class="mf">0.73566747</span>  <span class="mf">0.87292582</span> <span class="o">-</span><span class="mf">0.41605178</span>  <span class="mf">0.04792296</span><span class="p">]</span>
  <span class="p">[</span><span class="o">-</span><span class="mf">0.63856047</span>  <span class="mf">0.31966645</span> <span class="o">-</span><span class="mf">0.63974309</span> <span class="o">-</span><span class="mf">0.61385244</span><span class="p">]]]</span>
<span class="p">[</span><span class="n">Slicing</span><span class="p">]</span>
<span class="p">[[[</span> <span class="mf">0.</span>          <span class="mf">0.22121875</span>  <span class="mf">0.</span>         <span class="o">-</span><span class="mf">0.33490974</span><span class="p">]</span>
  <span class="p">[</span> <span class="mf">0.</span>          <span class="mf">0.2165522</span>   <span class="mf">0.</span>         <span class="o">-</span><span class="mf">0.61813235</span><span class="p">]</span>
  <span class="p">[</span> <span class="mf">0.</span>         <span class="o">-</span><span class="mf">0.44104072</span>  <span class="mf">0.</span>          <span class="mf">0.73900551</span><span class="p">]]</span>

 <span class="p">[[</span><span class="o">-</span><span class="mf">0.59464753</span> <span class="o">-</span><span class="mf">2.17738533</span> <span class="o">-</span><span class="mf">0.28626776</span> <span class="o">-</span><span class="mf">0.45654735</span><span class="p">]</span>
  <span class="p">[</span> <span class="mf">0.73566747</span>  <span class="mf">0.87292582</span> <span class="o">-</span><span class="mf">0.41605178</span>  <span class="mf">0.04792296</span><span class="p">]</span>
  <span class="p">[</span><span class="o">-</span><span class="mf">0.63856047</span>  <span class="mf">0.31966645</span> <span class="o">-</span><span class="mf">0.63974309</span> <span class="o">-</span><span class="mf">0.61385244</span><span class="p">]]]</span>
</pre></div>
</div>
<p>Note that the above operation is all done in the host device (CPU).
NdArray provides more efficient functions in case you want to fill all
values with a constant, <code class="docutils literal notranslate"><span class="pre">.zero</span></code> and <code class="docutils literal notranslate"><span class="pre">.fill</span></code>. They are lazily
evaluated when the data is requested (when neural network computation
requests the data, or when NumPy array is requested by Python) The
filling operation is executed within a specific device (e.g. CUDA GPU),
and more efficient if you specify the device setting, which we explain
later.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Filling all values with one.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[[[</span> <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span><span class="p">]</span>
  <span class="p">[</span> <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span><span class="p">]</span>
  <span class="p">[</span> <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span><span class="p">]]</span>

 <span class="p">[[</span> <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span><span class="p">]</span>
  <span class="p">[</span> <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span><span class="p">]</span>
  <span class="p">[</span> <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span><span class="p">]]]</span>
</pre></div>
</div>
<p>You can create an NdArray instance directly from a NumPy array object.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">b</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NdArray</span><span class="o">.</span><span class="n">from_numpy_array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[[[</span> <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span><span class="p">]</span>
  <span class="p">[</span> <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span><span class="p">]</span>
  <span class="p">[</span> <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span><span class="p">]]</span>

 <span class="p">[[</span> <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span><span class="p">]</span>
  <span class="p">[</span> <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span><span class="p">]</span>
  <span class="p">[</span> <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span><span class="p">]]]</span>
</pre></div>
</div>
<p>NdArray is used in Variable class, as well as NNabla’s imperative
computation of neural networks. We describe them in the later sections.</p>
</div>
<div class="section" id="variable">
<h2>Variable<a class="headerlink" href="#variable" title="Permalink to this headline">¶</a></h2>
<p>Variable class is used when you construct a neural network. The neural
network can be described as a graph in which an edge represents a
function (a.k.a operator and layer) which defines operation of a minimum
unit of computation, and a node represents a variable which holds
input/output values of a function (Function class is explained later).
The graph is called “Computation Graph”.</p>
<p>In NNabla, a Variable, a node of a computation graph, holds two
<code class="docutils literal notranslate"><span class="pre">NdArray</span></code>s, one for storing the input or output values of a function
during forward propagation (executing computation graph in the forward
order), while another for storing the backward error signal (gradient)
during backward propagation (executing computation graph in backward
order to propagate error signals down to parameters (weights) of neural
networks). The first one is called <code class="docutils literal notranslate"><span class="pre">data</span></code>, the second is <code class="docutils literal notranslate"><span class="pre">grad</span></code> in
NNabla.</p>
<p>The following line creates a Variable instance with a shape of (2, 3,
4). It has <code class="docutils literal notranslate"><span class="pre">data</span></code> and <code class="docutils literal notranslate"><span class="pre">grad</span></code> as <code class="docutils literal notranslate"><span class="pre">NdArray</span></code>. The flag <code class="docutils literal notranslate"><span class="pre">need_grad</span></code>
is used to omit unnecessary gradient computation during backprop if set
to False.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x.data:&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x.grad:&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">NdArray</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span> <span class="n">at</span> <span class="mh">0x7f575caf4ea0</span><span class="o">&gt;</span>
<span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">NdArray</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span> <span class="n">at</span> <span class="mh">0x7f575caf4ea0</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>You can get the shape by:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<p>Since both <code class="docutils literal notranslate"><span class="pre">data</span></code> and <code class="docutils literal notranslate"><span class="pre">grad</span></code> are <code class="docutils literal notranslate"><span class="pre">NdArray</span></code>, you can get a
reference to its values as NdArray with the <code class="docutils literal notranslate"><span class="pre">.data</span></code> accessor, but also
it can be referred by <code class="docutils literal notranslate"><span class="pre">.d</span></code> or <code class="docutils literal notranslate"><span class="pre">.g</span></code> property for <code class="docutils literal notranslate"><span class="pre">data</span></code> and <code class="docutils literal notranslate"><span class="pre">grad</span></code>
respectively.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x.data&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">d</span><span class="p">)</span>
<span class="n">x</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="mf">1.2345</span>  <span class="c1"># To avoid NaN</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">d</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="s1">&#39;d: </span><span class="si">{}</span><span class="s1"> != </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x.grad&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">g</span><span class="p">)</span>
<span class="n">x</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="mf">1.2345</span>  <span class="c1"># To avoid NaN</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">g</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="s1">&#39;g: </span><span class="si">{}</span><span class="s1"> != </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">g</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Zeroing grad values</span>
<span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x.grad (after `.zero()`)&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">g</span><span class="p">)</span>
</pre></div>
</div>
<pre class="literal-block">x.data
[[[  9.42553452e+24   4.56809286e-41   8.32543479e-38   0.00000000e+00]
  [             nan              nan   0.00000000e+00   0.00000000e+00]
  [  3.70977305e+25   4.56809286e-41   3.78350585e-44   0.00000000e+00]]

 [[  5.68736600e-38   0.00000000e+00   1.86176378e-13   4.56809286e-41]
  [  4.74367616e+25   4.56809286e-41   5.43829710e+19   4.56809286e-41]
  [  0.00000000e+00   0.00000000e+00   2.93623372e-38   0.00000000e+00]]]
x.grad
[[[  9.42576510e+24   4.56809286e-41   9.42576510e+24   4.56809286e-41]
  [  9.27127763e-38   0.00000000e+00   9.27127763e-38   0.00000000e+00]
  [  1.69275966e+22   4.80112800e+30   1.21230330e+25   7.22962302e+31]]

 [[  1.10471027e-32   4.63080422e+27   2.44632805e+20   2.87606258e+20]
  [  4.46263300e+30   4.62311881e+30   7.65000750e+28   3.01339003e+29]
  [  2.08627352e-10   1.03961868e+21   7.99576678e+20   1.74441223e+22]]]
x.grad (after <code class="xref any docutils literal notranslate"><span class="pre">.zero()</span></code>)
[[[ 0.  0.  0.  0.]
  [ 0.  0.  0.  0.]
  [ 0.  0.  0.  0.]]

 [[ 0.  0.  0.  0.]
  [ 0.  0.  0.  0.]
  [ 0.  0.  0.  0.]]]</pre>
<p>Like <code class="docutils literal notranslate"><span class="pre">NdArray</span></code>, a <code class="docutils literal notranslate"><span class="pre">Variable</span></code> can also be created from NumPy
array(s).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="o">.</span><span class="n">from_numpy_array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,)),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x2</span><span class="o">.</span><span class="n">d</span><span class="p">)</span>
<span class="n">x3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="o">.</span><span class="n">from_numpy_array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,)),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,)),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x3</span><span class="o">.</span><span class="n">d</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x3</span><span class="o">.</span><span class="n">g</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">3</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572a5242c8</span><span class="o">&gt;</span>
<span class="p">[</span> <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span><span class="p">]</span>
<span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">3</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572a5244a8</span><span class="o">&gt;</span>
<span class="p">[</span> <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span><span class="p">]</span>
<span class="p">[</span> <span class="mf">0.</span>  <span class="mf">0.</span>  <span class="mf">0.</span><span class="p">]</span>
</pre></div>
</div>
<p>Besides storing values of a computation graph, pointing a parent edge
(function) to trace the computation graph is an important role. Here
<code class="docutils literal notranslate"><span class="pre">x</span></code> doesn’t have any connection. Therefore, the <code class="docutils literal notranslate"><span class="pre">.parent</span></code> property
returns None.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">parent</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kc">None</span>
</pre></div>
</div>
</div>
<div class="section" id="function">
<h2>Function<a class="headerlink" href="#function" title="Permalink to this headline">¶</a></h2>
<p>A function defines an operation block of a computation graph as we
described above. The module <code class="docutils literal notranslate"><span class="pre">nnabla.functions</span></code> offers various
functions (e.g. Convolution, Affine and ReLU). You can see the list of
functions available in the <a class="reference external" href="http://nnabla.readthedocs.io/en/latest/python/api/function.html#module-nnabla.functions">API reference
guide</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nnabla.functions</span> <span class="k">as</span> <span class="nn">F</span>
</pre></div>
</div>
<p>As an example, here you will defines a computation graph that computes
the element-wise Sigmoid function outputs for the input variable and
sums up all values into a scalar. (This is simple enough to explain how
it behaves but a meaningless example in the context of neural network
training. We will show you a neural network example later.)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sigmoid_output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">sum_output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">sigmoid_output</span><span class="p">)</span>
</pre></div>
</div>
<p>The function API in <code class="docutils literal notranslate"><span class="pre">nnabla.functions</span></code> takes one (or several)
Variable(s) and arguments (if any), and returns one (or several) output
Variable(s). The <code class="docutils literal notranslate"><span class="pre">.parent</span></code> points to the function instance which
created it. Note that no computation occurs at this time since we just
define the graph. (This is the default behavior of NNabla computation
graph API. You can also fire actual computation during graph definition
which we call “Dynamic mode” (explained later)).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;sigmoid_output.parent.name:&quot;</span><span class="p">,</span> <span class="n">sigmoid_output</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;sigmoid_output.parent.inputs refers to x:&quot;</span><span class="p">,</span> <span class="n">sigmoid_output</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sigmoid_output</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">Sigmoid</span>
<span class="n">x</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572a51a778</span><span class="o">&gt;</span>
<span class="n">sigmoid_output</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">inputs</span> <span class="n">refers</span> <span class="n">to</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span><span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572a273a48</span><span class="o">&gt;</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;sum_output.parent.name:&quot;</span><span class="p">,</span> <span class="n">sum_output</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;sigmoid_output:&quot;</span><span class="p">,</span> <span class="n">sigmoid_output</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;sum_output.parent.inputs refers to sigmoid_output:&quot;</span><span class="p">,</span> <span class="n">sum_output</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sum_output</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">ReduceSum</span>
<span class="n">sigmoid_output</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572a524638</span><span class="o">&gt;</span>
<span class="n">sum_output</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">inputs</span> <span class="n">refers</span> <span class="n">to</span> <span class="n">sigmoid_output</span><span class="p">:</span> <span class="p">[</span><span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572a273a48</span><span class="o">&gt;</span><span class="p">]</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">.forward()</span></code> at a leaf Variable executes the forward pass
computation in the computation graph.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sum_output</span><span class="o">.</span><span class="n">forward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CG output:&quot;</span><span class="p">,</span> <span class="n">sum_output</span><span class="o">.</span><span class="n">d</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reference:&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">.</span><span class="n">d</span><span class="p">))))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">CG</span> <span class="n">output</span><span class="p">:</span> <span class="mf">18.59052085876465</span>
<span class="n">Reference</span><span class="p">:</span> <span class="mf">18.5905</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> does the backward propagation through the graph.
Here we initialize the <code class="docutils literal notranslate"><span class="pre">grad</span></code> values as zero before backprop since the
NNabla backprop algorithm always accumulates the gradient in the root
variables.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>
<span class="n">sum_output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;d sum_o / d sigmoid_o:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sigmoid_output</span><span class="o">.</span><span class="n">g</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;d sum_o / d x:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">g</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">d</span> <span class="n">sum_o</span> <span class="o">/</span> <span class="n">d</span> <span class="n">sigmoid_o</span><span class="p">:</span>
<span class="p">[[[</span> <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span><span class="p">]</span>
  <span class="p">[</span> <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span><span class="p">]</span>
  <span class="p">[</span> <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span><span class="p">]]</span>

 <span class="p">[[</span> <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span><span class="p">]</span>
  <span class="p">[</span> <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span><span class="p">]</span>
  <span class="p">[</span> <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span><span class="p">]]]</span>
<span class="n">d</span> <span class="n">sum_o</span> <span class="o">/</span> <span class="n">d</span> <span class="n">x</span><span class="p">:</span>
<span class="p">[[[</span> <span class="mf">0.17459197</span>  <span class="mf">0.17459197</span>  <span class="mf">0.17459197</span>  <span class="mf">0.17459197</span><span class="p">]</span>
  <span class="p">[</span> <span class="mf">0.17459197</span>  <span class="mf">0.17459197</span>  <span class="mf">0.17459197</span>  <span class="mf">0.17459197</span><span class="p">]</span>
  <span class="p">[</span> <span class="mf">0.17459197</span>  <span class="mf">0.17459197</span>  <span class="mf">0.17459197</span>  <span class="mf">0.17459197</span><span class="p">]]</span>

 <span class="p">[[</span> <span class="mf">0.17459197</span>  <span class="mf">0.17459197</span>  <span class="mf">0.17459197</span>  <span class="mf">0.17459197</span><span class="p">]</span>
  <span class="p">[</span> <span class="mf">0.17459197</span>  <span class="mf">0.17459197</span>  <span class="mf">0.17459197</span>  <span class="mf">0.17459197</span><span class="p">]</span>
  <span class="p">[</span> <span class="mf">0.17459197</span>  <span class="mf">0.17459197</span>  <span class="mf">0.17459197</span>  <span class="mf">0.17459197</span><span class="p">]]]</span>
</pre></div>
</div>
<p>NNabla is developed by mainly focused on neural network training and
inference. Neural networks have parameters to be learned associated with
computation blocks such as Convolution, Affine (a.k.a. fully connected,
dense etc.). In NNabla, the learnable parameters are also represented as
<code class="docutils literal notranslate"><span class="pre">Variable</span></code> objects. Just like input variables, those parameter
variables are also used by passing into <code class="docutils literal notranslate"><span class="pre">Function</span></code>s. For example,
Affine function takes input, weights and biases as inputs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>  <span class="c1"># Input</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Weights</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mi">3</span><span class="p">],</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Biases</span>
<span class="n">affine_out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>  <span class="c1"># Create a graph including only affine</span>
</pre></div>
</div>
<p>The above example takes an input with B=5 (batchsize) and D=2
(dimensions) and maps it to D’=3 outputs, i.e. (B, D’) output.</p>
<p>You may also notice that here you set <code class="docutils literal notranslate"><span class="pre">need_grad=True</span></code> only for
parameter variables (w and b). The x is a non-parameter variable and the
root of computation graph. Therefore, it doesn’t require gradient
computation. In this configuration, the gradient computation for x is
not executed in the first affine, which will omit the computation of
unnecessary backpropagation.</p>
<p>The next block sets data and initializes grad, then applies forward and
backward computation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set random input and parameters</span>
<span class="n">x</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">w</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">b</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># Initialize grad</span>
<span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>  <span class="c1"># Just for showing gradients are not computed when need_grad=False (default).</span>
<span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>
<span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>

<span class="c1"># Forward and backward</span>
<span class="n">affine_out</span><span class="o">.</span><span class="n">forward</span><span class="p">()</span>
<span class="n">affine_out</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="c1"># Note: Calling backward at non-scalar Variable propagates 1 as error message from all element of outputs. .</span>
</pre></div>
</div>
<p>You can see that affine_out holds an output of Affine.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;F.affine&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">affine_out</span><span class="o">.</span><span class="n">d</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Reference&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">d</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">d</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">F</span><span class="o">.</span><span class="n">affine</span>
<span class="p">[[</span><span class="o">-</span><span class="mf">0.17701732</span>  <span class="mf">2.86095762</span> <span class="o">-</span><span class="mf">0.82298267</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.75544345</span> <span class="o">-</span><span class="mf">1.16702223</span> <span class="o">-</span><span class="mf">2.44841242</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.36278027</span> <span class="o">-</span><span class="mf">3.4771595</span>  <span class="o">-</span><span class="mf">0.75681627</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.32743117</span>  <span class="mf">0.24258983</span>  <span class="mf">1.30944324</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.87201929</span>  <span class="mf">1.94556415</span> <span class="o">-</span><span class="mf">3.23357344</span><span class="p">]]</span>
<span class="n">Reference</span>
<span class="p">[[</span><span class="o">-</span><span class="mf">0.1770173</span>   <span class="mf">2.86095762</span> <span class="o">-</span><span class="mf">0.82298267</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.75544345</span> <span class="o">-</span><span class="mf">1.16702223</span> <span class="o">-</span><span class="mf">2.44841242</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.3627803</span>  <span class="o">-</span><span class="mf">3.4771595</span>  <span class="o">-</span><span class="mf">0.75681627</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.32743117</span>  <span class="mf">0.24258983</span>  <span class="mf">1.309443</span>  <span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.87201929</span>  <span class="mf">1.94556415</span> <span class="o">-</span><span class="mf">3.23357344</span><span class="p">]]</span>
</pre></div>
</div>
<p>The resulting gradients of weights and biases are as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dw&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">g</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;db&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">g</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dw</span>
<span class="p">[[</span> <span class="mf">3.10820675</span>  <span class="mf">3.10820675</span>  <span class="mf">3.10820675</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.37446201</span>  <span class="mf">0.37446201</span>  <span class="mf">0.37446201</span><span class="p">]]</span>
<span class="n">db</span>
<span class="p">[</span> <span class="mf">5.</span>  <span class="mf">5.</span>  <span class="mf">5.</span><span class="p">]</span>
</pre></div>
</div>
<p>The gradient of <code class="docutils literal notranslate"><span class="pre">x</span></code> is not changed because <code class="docutils literal notranslate"><span class="pre">need_grad</span></code> is set as
False.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">g</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span> <span class="mf">0.</span>  <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.</span>  <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.</span>  <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.</span>  <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.</span>  <span class="mf">0.</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="section" id="parametric-function">
<h2>Parametric Function<a class="headerlink" href="#parametric-function" title="Permalink to this headline">¶</a></h2>
<p>Considering parameters as inputs of <code class="docutils literal notranslate"><span class="pre">Function</span></code> enhances expressiveness
and flexibility of computation graphs. However, to define all parameters
for each learnable function is annoying for users to define a neural
network. In NNabla, trainable models are usually created by composing
functions that have optimizable parameters. These functions are called
“Parametric Functions”. The Parametric Function API provides various
parametric functions and an interface for composing trainable models.</p>
<p>To use parametric functions, import:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nnabla.parametric_functions</span> <span class="k">as</span> <span class="nn">PF</span>
</pre></div>
</div>
<p>The function with optimizable parameter can be created as below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;affine1&quot;</span><span class="p">):</span>
    <span class="n">c1</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<p>The first line creates a <strong>parameter scope</strong>. The second line then
applies <code class="docutils literal notranslate"><span class="pre">PF.affine</span></code> - an affine transform - to <code class="docutils literal notranslate"><span class="pre">x</span></code>, and creates a
variable <code class="docutils literal notranslate"><span class="pre">c1</span></code> holding that result. The parameters are created and
initialized randomly at function call, and registered by a name
“affine1” using <code class="docutils literal notranslate"><span class="pre">parameter_scope</span></code> context. The function
<code class="docutils literal notranslate"><span class="pre">nnabla.get_parameters()</span></code> allows to get the registered parameters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nn</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">OrderedDict</span><span class="p">([(</span><span class="s1">&#39;affine1/affine/W&#39;</span><span class="p">,</span>
              <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572822f0e8</span><span class="o">&gt;</span><span class="p">),</span>
             <span class="p">(</span><span class="s1">&#39;affine1/affine/b&#39;</span><span class="p">,</span>
              <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">3</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572822f138</span><span class="o">&gt;</span><span class="p">)])</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">name=</span></code> argument of any PF function creates the equivalent
parameter space to the above definition of <code class="docutils literal notranslate"><span class="pre">PF.affine</span></code> transformation
as below. It could save the space of your Python code. The
<code class="docutils literal notranslate"><span class="pre">nnabla.parametric_scope</span></code> is more useful when you group multiple
parametric functions such as Convolution-BatchNormalization found in a
typical unit of CNNs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">c1</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;affine1&#39;</span><span class="p">)</span>
<span class="n">nn</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">OrderedDict</span><span class="p">([(</span><span class="s1">&#39;affine1/affine/W&#39;</span><span class="p">,</span>
              <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572822f0e8</span><span class="o">&gt;</span><span class="p">),</span>
             <span class="p">(</span><span class="s1">&#39;affine1/affine/b&#39;</span><span class="p">,</span>
              <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">3</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572822f138</span><span class="o">&gt;</span><span class="p">)])</span>
</pre></div>
</div>
<p>It is worth noting that the shapes of both outputs and parameter
variables (as you can see above) are automatically determined by only
providing the output size of affine transformation(in the example above
the output size is 3). This helps to create a graph in an easy way.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">c1</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<p>Parameter scope can be nested as follows (although a meaningless
example).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s1">&#39;foo&#39;</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s1">&#39;bar&#39;</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<p>This creates the following.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nn</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">OrderedDict</span><span class="p">([(</span><span class="s1">&#39;affine1/affine/W&#39;</span><span class="p">,</span>
              <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572822f0e8</span><span class="o">&gt;</span><span class="p">),</span>
             <span class="p">(</span><span class="s1">&#39;affine1/affine/b&#39;</span><span class="p">,</span>
              <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">3</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572822f138</span><span class="o">&gt;</span><span class="p">),</span>
             <span class="p">(</span><span class="s1">&#39;foo/affine/W&#39;</span><span class="p">,</span>
              <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572822fa98</span><span class="o">&gt;</span><span class="p">),</span>
             <span class="p">(</span><span class="s1">&#39;foo/affine/b&#39;</span><span class="p">,</span>
              <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">3</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572822fae8</span><span class="o">&gt;</span><span class="p">),</span>
             <span class="p">(</span><span class="s1">&#39;foo/bar/affine/W&#39;</span><span class="p">,</span>
              <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572822f728</span><span class="o">&gt;</span><span class="p">),</span>
             <span class="p">(</span><span class="s1">&#39;foo/bar/affine/b&#39;</span><span class="p">,</span>
              <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">4</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572822fdb8</span><span class="o">&gt;</span><span class="p">)])</span>
</pre></div>
</div>
<p>Also, <code class="docutils literal notranslate"><span class="pre">get_parameters()</span></code> can be used in <code class="docutils literal notranslate"><span class="pre">parameter_scope</span></code>. For
example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;foo&quot;</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">OrderedDict</span><span class="p">([(</span><span class="s1">&#39;affine/W&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572822fa98</span><span class="o">&gt;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;affine/b&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">3</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572822fae8</span><span class="o">&gt;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;bar/affine/W&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572822f728</span><span class="o">&gt;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;bar/affine/b&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">4</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572822fdb8</span><span class="o">&gt;</span><span class="p">)])</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">nnabla.clear_parameters()</span></code> can be used to delete registered
parameters under the scope.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;foo&quot;</span><span class="p">):</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">clear_parameters</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">OrderedDict</span><span class="p">([(</span><span class="s1">&#39;affine1/affine/W&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572822f0e8</span><span class="o">&gt;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;affine1/affine/b&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">3</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572822f138</span><span class="o">&gt;</span><span class="p">)])</span>
</pre></div>
</div>
</div>
<div class="section" id="mlp-example-for-explanation">
<h2>MLP Example For Explanation<a class="headerlink" href="#mlp-example-for-explanation" title="Permalink to this headline">¶</a></h2>
<p>The following block creates a computation graph to predict one
dimensional output from two dimensional inputs by a 2 layer fully
connected neural network (multi-layer perceptron).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nn</span><span class="o">.</span><span class="n">clear_parameters</span><span class="p">()</span>
<span class="n">batchsize</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="n">batchsize</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;fc1&quot;</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">PF</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">512</span><span class="p">))</span>
<span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s2">&quot;fc2&quot;</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shapes:&quot;</span><span class="p">,</span> <span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Shapes</span><span class="p">:</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>This will create the following parameter variables.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nn</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">OrderedDict</span><span class="p">([(</span><span class="s1">&#39;fc1/affine/W&#39;</span><span class="p">,</span>
              <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572822fef8</span><span class="o">&gt;</span><span class="p">),</span>
             <span class="p">(</span><span class="s1">&#39;fc1/affine/b&#39;</span><span class="p">,</span>
              <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">512</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572822f9a8</span><span class="o">&gt;</span><span class="p">),</span>
             <span class="p">(</span><span class="s1">&#39;fc2/affine/W&#39;</span><span class="p">,</span>
              <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572822f778</span><span class="o">&gt;</span><span class="p">),</span>
             <span class="p">(</span><span class="s1">&#39;fc2/affine/b&#39;</span><span class="p">,</span>
              <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">1</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572822ff98</span><span class="o">&gt;</span><span class="p">)])</span>
</pre></div>
</div>
<p>As described above, you can execute the forward pass by calling forward
method at the terminal variable.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Set random input</span>
<span class="n">y</span><span class="o">.</span><span class="n">forward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span><span class="o">-</span><span class="mf">0.05708594</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.01661986</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.34168088</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.05822293</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.16566885</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.04867431</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.2633169</span> <span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.10496549</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.01291842</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.09726256</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.05720493</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.09691752</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.07822668</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.17180404</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.11970415</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.08222144</span><span class="p">]]</span>
</pre></div>
</div>
<p>Training a neural networks needs a loss value to be minimized by
gradient descent with backprop. In NNabla, loss function is also a just
function, and packaged in the functions module.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Variable for label</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="n">batchsize</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="c1"># Set loss</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="p">))</span>

<span class="c1"># Execute forward pass.</span>
<span class="n">label</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">label</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Randomly generate labels</span>
<span class="n">loss</span><span class="o">.</span><span class="n">forward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">1.9382084608078003</span>
</pre></div>
</div>
<p>As you’ve seen above, NNabla <code class="docutils literal notranslate"><span class="pre">backward</span></code> accumulates the gradients at
the root variables. You have to initialize the grad of the parameter
variables before backprop (We will show you the easiest way with
<code class="docutils literal notranslate"><span class="pre">Solver</span></code> API).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Collect all parameter variables and init grad.</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">nn</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>
<span class="c1"># Gradients are accumulated to grad of params.</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="imperative-mode">
<h2>Imperative Mode<a class="headerlink" href="#imperative-mode" title="Permalink to this headline">¶</a></h2>
<p>After performing backprop, gradients are held in parameter variable
grads. The next block will update the parameters with vanilla gradient
descent.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">nn</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="mf">0.001</span>  <span class="c1"># 0.001 as learning rate</span>
</pre></div>
</div>
<p>The above computation is an example of NNabla’s “Imperative Mode” for
executing neural networks. Normally, NNabla functions (instances of
<a class="reference external" href="https://nnabla.readthedocs.io/en/latest/python/api/function.html#module-nnabla.functions">nnabla.functions</a>)
take <code class="docutils literal notranslate"><span class="pre">Variable</span></code>s as their input. When at least one <code class="docutils literal notranslate"><span class="pre">NdArray</span></code> is
provided as an input for NNabla functions (instead of <code class="docutils literal notranslate"><span class="pre">Variable</span></code>s),
the function computation will be fired immediately, and returns an
<code class="docutils literal notranslate"><span class="pre">NdArray</span></code> as the output, instead of returning a <code class="docutils literal notranslate"><span class="pre">Variable</span></code>. In the
above example, the NNabla functions <code class="docutils literal notranslate"><span class="pre">F.mul_scalar</span></code> and <code class="docutils literal notranslate"><span class="pre">F.sub2</span></code> are
called by the overridden operators <code class="docutils literal notranslate"><span class="pre">*</span></code> and <code class="docutils literal notranslate"><span class="pre">-=</span></code>, respectively.</p>
<p>In other words, NNabla’s “Imperative mode” doesn’t create a computation
graph, and can be used like NumPy. If device acceleration such as CUDA
is enabled, it can be used like NumPy empowered with device
acceleration. Parametric functions can also be used with NdArray
input(s). The following block demonstrates a simple imperative execution
example.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># A simple example of imperative mode.</span>
<span class="n">xi</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NdArray</span><span class="o">.</span><span class="n">from_numpy_array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">yi</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">xi</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">xi</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">yi</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span><span class="mi">0</span> <span class="mi">1</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">2</span> <span class="mi">3</span><span class="p">]]</span>
<span class="p">[[</span> <span class="mf">0.</span>  <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">1.</span>  <span class="mf">2.</span><span class="p">]]</span>
</pre></div>
</div>
<p>Note that in-place substitution from the rhs to the lhs cannot be done
by the <code class="docutils literal notranslate"><span class="pre">=</span></code> operator. For example, when <code class="docutils literal notranslate"><span class="pre">x</span></code> is an <code class="docutils literal notranslate"><span class="pre">NdArray</span></code>,
writing <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">x</span> <span class="pre">+</span> <span class="pre">1</span></code> will <em>not</em> increment all values of <code class="docutils literal notranslate"><span class="pre">x</span></code> -
instead, the expression on the rhs will create a <em>new</em> <code class="docutils literal notranslate"><span class="pre">NdArray</span></code>
object that is different from the one originally bound by <code class="docutils literal notranslate"><span class="pre">x</span></code>, and binds
the new <code class="docutils literal notranslate"><span class="pre">NdArray</span></code> object to the Python variable <code class="docutils literal notranslate"><span class="pre">x</span></code> on the lhs.</p>
<p>For in-place editing of <code class="docutils literal notranslate"><span class="pre">NdArrays</span></code>, the in-place assignment operators
<code class="docutils literal notranslate"><span class="pre">+=</span></code>, <code class="docutils literal notranslate"><span class="pre">-=</span></code>, <code class="docutils literal notranslate"><span class="pre">*=</span></code>, and <code class="docutils literal notranslate"><span class="pre">/=</span></code> can be used. The <code class="docutils literal notranslate"><span class="pre">copy_from</span></code> method
can also be used to copy values of an existing <code class="docutils literal notranslate"><span class="pre">NdArray</span></code> to another.
For example, incrementing 1 to <code class="docutils literal notranslate"><span class="pre">x</span></code>, an <code class="docutils literal notranslate"><span class="pre">NdArray</span></code>, can be done by
<code class="docutils literal notranslate"><span class="pre">x.copy_from(x+1)</span></code>. The copy is performed with device acceleration if
a device context is specified by using <code class="docutils literal notranslate"><span class="pre">nnabla.set_default_context</span></code> or
<code class="docutils literal notranslate"><span class="pre">nnabla.context_scope</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># The following doesn&#39;t perform substitution but assigns a new NdArray object to `xi`.</span>
<span class="c1"># xi = xi + 1</span>

<span class="c1"># The following copies the result of `xi + 1` to `xi`.</span>
<span class="n">xi</span><span class="o">.</span><span class="n">copy_from</span><span class="p">(</span><span class="n">xi</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">xi</span><span class="o">.</span><span class="n">data</span> <span class="o">==</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># Inplace operations like `+=`, `*=` can also be used (more efficient).</span>
<span class="n">xi</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">xi</span><span class="o">.</span><span class="n">data</span> <span class="o">==</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="solver">
<h2>Solver<a class="headerlink" href="#solver" title="Permalink to this headline">¶</a></h2>
<p>NNabla provides stochastic gradient descent algorithms to optimize
parameters listed in the <code class="docutils literal notranslate"><span class="pre">nnabla.solvers</span></code> module. The parameter
updates demonstrated above can be replaced with this Solver API, which
is easier and usually faster.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nnabla</span> <span class="kn">import</span> <span class="n">solvers</span> <span class="k">as</span> <span class="n">S</span>
<span class="n">solver</span> <span class="o">=</span> <span class="n">S</span><span class="o">.</span><span class="n">Sgd</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">)</span>
<span class="n">solver</span><span class="o">.</span><span class="n">set_parameters</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set random data</span>
<span class="n">x</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">label</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">label</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Forward</span>
<span class="n">loss</span><span class="o">.</span><span class="n">forward</span><span class="p">()</span>
</pre></div>
</div>
<p>Just call the the following solver method to fill zero grad region, then
backprop</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">solver</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<p>The following block updates parameters with the Vanilla Sgd rule
(equivalent to the imperative example above).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">solver</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="toy-problem-to-demonstrate-training">
<h2>Toy Problem To Demonstrate Training<a class="headerlink" href="#toy-problem-to-demonstrate-training" title="Permalink to this headline">¶</a></h2>
<p>The following function defines a regression problem which computes the
norm of a vector.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">vector2length</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># x : [B, 2] where B is number of samples.</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
<p>We visualize this mapping with the contour plot by matplotlib as
follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Data for plotting contour on a grid data.</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">grid</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">grid</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">plot_true</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;Plotting contour of true mapping from a grid data created above.&quot;&quot;&quot;</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">vector2length</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">X</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">Y</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]]))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>

<span class="n">plot_true</span><span class="p">()</span>
</pre></div>
</div>
<img alt="../../_images/python_api_98_0.png" src="../../_images/python_api_98_0.png" />
<p>We define a deep prediction neural network.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">length_mlp</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">x</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">hnum</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">]):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">PF</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">hnum</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;fc</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">)))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;fc&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nn</span><span class="o">.</span><span class="n">clear_parameters</span><span class="p">()</span>
<span class="n">batchsize</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="n">batchsize</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">length_mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="n">batchsize</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="p">))</span>
</pre></div>
</div>
<p>We created a 5 layers deep MLP using for-loop. Note that only 3 lines of
the code potentially create infinitely deep neural networks. The next
block adds helper functions to visualize the learned function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">inp</span><span class="p">):</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">xx</span> <span class="o">=</span> <span class="n">inp</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="c1"># Imperative execution</span>
        <span class="n">xi</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NdArray</span><span class="o">.</span><span class="n">from_numpy_array</span><span class="p">(</span><span class="n">xx</span><span class="p">)</span>
        <span class="n">yi</span> <span class="o">=</span> <span class="n">length_mlp</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span>
        <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">yi</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_prediction</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">X</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">Y</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]]))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Next we instantiate a solver object as follows. We use Adam optimizer
which is one of the most popular SGD algorithm used in the literature.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nnabla</span> <span class="kn">import</span> <span class="n">solvers</span> <span class="k">as</span> <span class="n">S</span>
<span class="n">solver</span> <span class="o">=</span> <span class="n">S</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">solver</span><span class="o">.</span><span class="n">set_parameters</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">())</span>
</pre></div>
</div>
<p>The following function generates data from the true system infinitely.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">random_data_provider</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">vector2length</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
</pre></div>
</div>
<p>In the next block, we run 2000 training steps (SGD updates).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">num_iter</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iter</span><span class="p">):</span>
    <span class="c1"># Sample data and set them to input variables of training.</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">ll</span> <span class="o">=</span> <span class="n">random_data_provider</span><span class="p">(</span><span class="n">batchsize</span><span class="p">)</span>
    <span class="n">x</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">xx</span>
    <span class="n">label</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">ll</span>
    <span class="c1"># Forward propagation given inputs.</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">clear_no_need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># Parameter gradients initialization and gradients computation by backprop.</span>
    <span class="n">solver</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">clear_buffer</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># Apply weight decay and update by Adam rule.</span>
    <span class="n">solver</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">(</span><span class="mf">1e-6</span><span class="p">)</span>
    <span class="n">solver</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
    <span class="c1"># Just print progress.</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">==</span> <span class="n">num_iter</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loss@</span><span class="si">{:4d}</span><span class="s2">: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">d</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Loss</span><span class="o">@</span>   <span class="mi">0</span><span class="p">:</span> <span class="mf">0.6976373195648193</span>
<span class="n">Loss</span><span class="o">@</span> <span class="mi">100</span><span class="p">:</span> <span class="mf">0.08075223118066788</span>
<span class="n">Loss</span><span class="o">@</span> <span class="mi">200</span><span class="p">:</span> <span class="mf">0.005213144235312939</span>
<span class="n">Loss</span><span class="o">@</span> <span class="mi">300</span><span class="p">:</span> <span class="mf">0.001955194864422083</span>
<span class="n">Loss</span><span class="o">@</span> <span class="mi">400</span><span class="p">:</span> <span class="mf">0.0011660841992124915</span>
<span class="n">Loss</span><span class="o">@</span> <span class="mi">500</span><span class="p">:</span> <span class="mf">0.0006421314901672304</span>
<span class="n">Loss</span><span class="o">@</span> <span class="mi">600</span><span class="p">:</span> <span class="mf">0.0009330055327154696</span>
<span class="n">Loss</span><span class="o">@</span> <span class="mi">700</span><span class="p">:</span> <span class="mf">0.0008817618945613503</span>
<span class="n">Loss</span><span class="o">@</span> <span class="mi">800</span><span class="p">:</span> <span class="mf">0.0006205961108207703</span>
<span class="n">Loss</span><span class="o">@</span> <span class="mi">900</span><span class="p">:</span> <span class="mf">0.0009072928223758936</span>
<span class="n">Loss</span><span class="o">@</span><span class="mi">1000</span><span class="p">:</span> <span class="mf">0.0008160348515957594</span>
<span class="n">Loss</span><span class="o">@</span><span class="mi">1100</span><span class="p">:</span> <span class="mf">0.0011569359339773655</span>
<span class="n">Loss</span><span class="o">@</span><span class="mi">1200</span><span class="p">:</span> <span class="mf">0.000837412488181144</span>
<span class="n">Loss</span><span class="o">@</span><span class="mi">1300</span><span class="p">:</span> <span class="mf">0.0011542742140591145</span>
<span class="n">Loss</span><span class="o">@</span><span class="mi">1400</span><span class="p">:</span> <span class="mf">0.0005833200993947685</span>
<span class="n">Loss</span><span class="o">@</span><span class="mi">1500</span><span class="p">:</span> <span class="mf">0.0009848927147686481</span>
<span class="n">Loss</span><span class="o">@</span><span class="mi">1600</span><span class="p">:</span> <span class="mf">0.0005141657311469316</span>
<span class="n">Loss</span><span class="o">@</span><span class="mi">1700</span><span class="p">:</span> <span class="mf">0.0009339841199107468</span>
<span class="n">Loss</span><span class="o">@</span><span class="mi">1800</span><span class="p">:</span> <span class="mf">0.000950580753851682</span>
<span class="n">Loss</span><span class="o">@</span><span class="mi">1900</span><span class="p">:</span> <span class="mf">0.0005430278833955526</span>
<span class="n">Loss</span><span class="o">@</span><span class="mi">1999</span><span class="p">:</span> <span class="mf">0.0007046313839964569</span>
</pre></div>
</div>
<p><strong>Memory usage optimization</strong>: You may notice that, in the above
updates, <code class="docutils literal notranslate"><span class="pre">.forward()</span></code> is called with the <code class="docutils literal notranslate"><span class="pre">clear_no_need_grad=</span></code>
option, and <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> is called with the <code class="docutils literal notranslate"><span class="pre">clear_buffer=</span></code> option.
Training of neural network in more realistic scenarios usually consumes
huge memory due to the nature of backpropagation algorithm, in which all
of the forward variable buffer <code class="docutils literal notranslate"><span class="pre">data</span></code> should be kept in order to
compute the gradient of a function. In a naive implementation, we keep
all the variable <code class="docutils literal notranslate"><span class="pre">data</span></code> and <code class="docutils literal notranslate"><span class="pre">grad</span></code> living until the <code class="docutils literal notranslate"><span class="pre">NdArray</span></code>
objects are not referenced (i.e. the graph is deleted). The <code class="docutils literal notranslate"><span class="pre">clear_*</span></code>
options in <code class="docutils literal notranslate"><span class="pre">.forward()</span></code> and <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> enables to save memory
consumption due to that by clearing (erasing) memory of <code class="docutils literal notranslate"><span class="pre">data</span></code> and
<code class="docutils literal notranslate"><span class="pre">grad</span></code> when it is not referenced by any subsequent computation. (More
precisely speaking, it doesn’t free memory actually. We use our memory
pool engine by default to avoid memory alloc/free overhead). The
unreferenced buffers can be re-used in subsequent computation. See the
document of <code class="docutils literal notranslate"><span class="pre">Variable</span></code> for more details. Note that the following
<code class="docutils literal notranslate"><span class="pre">loss.forward(clear_buffer=True)</span></code> clears <code class="docutils literal notranslate"><span class="pre">data</span></code> of any intermediate
variables. If you are interested in intermediate variables for some
purposes (e.g. debug, log), you can use the <code class="docutils literal notranslate"><span class="pre">.persistent</span></code> flag to
prevent clearing buffer of a specific <code class="docutils literal notranslate"><span class="pre">Variable</span></code> like below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">clear_buffer</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The prediction `y` is cleared because it&#39;s an intermediate variable.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">d</span><span class="o">.</span><span class="n">flatten</span><span class="p">()[:</span><span class="mi">4</span><span class="p">])</span>  <span class="c1"># to save space show only 4 values</span>
<span class="n">y</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">loss</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">clear_buffer</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The prediction `y` is kept by the persistent flag.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">d</span><span class="o">.</span><span class="n">flatten</span><span class="p">()[:</span><span class="mi">4</span><span class="p">])</span>  <span class="c1"># to save space show only 4 value</span>
</pre></div>
</div>
<pre class="literal-block">The prediction <code class="xref any docutils literal notranslate"><span class="pre">y</span></code> is cleared because it's an intermediate variable.
[  2.27279830e-04   6.02164946e-05   5.33679675e-04   2.35557582e-05]
The prediction <code class="xref any docutils literal notranslate"><span class="pre">y</span></code> is kept by the persistent flag.
[ 1.0851264   0.87657517  0.79603785  0.40098712]</pre>
<p>We can confirm the prediction performs fairly well by looking at the
following visualization of the ground truth and prediction function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Ground truth&quot;</span><span class="p">)</span>
<span class="n">plot_true</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Prediction&quot;</span><span class="p">)</span>
<span class="n">plot_prediction</span><span class="p">()</span>
</pre></div>
</div>
<img alt="../../_images/python_api_113_0.png" src="../../_images/python_api_113_0.png" />
<p>You can save learned parameters by <code class="docutils literal notranslate"><span class="pre">nnabla.save_parameters</span></code> and load
by <code class="docutils literal notranslate"><span class="pre">nnabla.load_parameters</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">path_param</span> <span class="o">=</span> <span class="s2">&quot;param-vector2length.h5&quot;</span>
<span class="n">nn</span><span class="o">.</span><span class="n">save_parameters</span><span class="p">(</span><span class="n">path_param</span><span class="p">)</span>
<span class="c1"># Remove all once</span>
<span class="n">nn</span><span class="o">.</span><span class="n">clear_parameters</span><span class="p">()</span>
<span class="n">nn</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2017</span><span class="o">-</span><span class="mi">09</span><span class="o">-</span><span class="mi">27</span> <span class="mi">14</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mi">40</span><span class="p">,</span><span class="mi">544</span> <span class="p">[</span><span class="n">nnabla</span><span class="p">][</span><span class="n">INFO</span><span class="p">]:</span> <span class="n">Parameter</span> <span class="n">save</span> <span class="p">(</span><span class="o">.</span><span class="n">h5</span><span class="p">):</span> <span class="n">param</span><span class="o">-</span><span class="n">vector2length</span><span class="o">.</span><span class="n">h5</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">OrderedDict</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load again</span>
<span class="n">nn</span><span class="o">.</span><span class="n">load_parameters</span><span class="p">(</span><span class="n">path_param</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">())))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2017</span><span class="o">-</span><span class="mi">09</span><span class="o">-</span><span class="mi">27</span> <span class="mi">14</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mi">40</span><span class="p">,</span><span class="mi">564</span> <span class="p">[</span><span class="n">nnabla</span><span class="p">][</span><span class="n">INFO</span><span class="p">]:</span> <span class="n">Parameter</span> <span class="n">load</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">built</span><span class="o">-</span><span class="ow">in</span> <span class="n">function</span> <span class="nb">format</span><span class="o">&gt;</span><span class="p">):</span> <span class="n">param</span><span class="o">-</span><span class="n">vector2length</span><span class="o">.</span><span class="n">h5</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="s1">&#39;fc0/affine/W&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f576328df48</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;fc0/affine/b&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">4</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f57245f2868</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;fc1/affine/W&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f576328def8</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;fc1/affine/b&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">8</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f5727ee5c78</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;fc2/affine/W&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f5763297318</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;fc2/affine/b&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">4</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f5727d29908</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;fc3/affine/W&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f57632973b8</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;fc3/affine/b&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">2</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f57632974a8</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;fc/affine/W&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f57632974f8</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;fc/affine/b&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">1</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f5763297598</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
<p>Both save and load functions can also be used in a parameter scope.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">(</span><span class="s1">&#39;foo&#39;</span><span class="p">):</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">load_parameters</span><span class="p">(</span><span class="n">path_param</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">())))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2017</span><span class="o">-</span><span class="mi">09</span><span class="o">-</span><span class="mi">27</span> <span class="mi">14</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mi">40</span><span class="p">,</span><span class="mi">714</span> <span class="p">[</span><span class="n">nnabla</span><span class="p">][</span><span class="n">INFO</span><span class="p">]:</span> <span class="n">Parameter</span> <span class="n">load</span> <span class="p">(</span><span class="o">&lt;</span><span class="n">built</span><span class="o">-</span><span class="ow">in</span> <span class="n">function</span> <span class="nb">format</span><span class="o">&gt;</span><span class="p">):</span> <span class="n">param</span><span class="o">-</span><span class="n">vector2length</span><span class="o">.</span><span class="n">h5</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="s1">&#39;fc0/affine/W&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f576328df48</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;fc0/affine/b&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">4</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f57245f2868</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;fc1/affine/W&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f576328def8</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;fc1/affine/b&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">8</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f5727ee5c78</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;fc2/affine/W&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f5763297318</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;fc2/affine/b&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">4</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f5727d29908</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;fc3/affine/W&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f57632973b8</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;fc3/affine/b&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">2</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f57632974a8</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;fc/affine/W&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f57632974f8</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;fc/affine/b&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">1</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f5763297598</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;foo/fc0/affine/W&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f5763297958</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;foo/fc0/affine/b&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">4</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f57632978b8</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;foo/fc1/affine/W&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f572a51ac78</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;foo/fc1/affine/b&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">8</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f5763297c78</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;foo/fc2/affine/W&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f5763297a98</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;foo/fc2/affine/b&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">4</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f5763297d68</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;foo/fc3/affine/W&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f5763297e08</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;foo/fc3/affine/b&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">2</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f5763297ea8</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;foo/fc/affine/W&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f5763297f48</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="s1">&#39;foo/fc/affine/b&#39;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">Variable</span><span class="p">((</span><span class="mi">1</span><span class="p">,),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="n">at</span> <span class="mh">0x7f5763297cc8</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>!rm {path_param}  # Clean ups
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="model_finetuning.html" class="btn btn-neutral float-right" title="NNabla Models Finetuning Tutorial" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="by_examples.html" class="btn btn-neutral float-left" title="NNabla by Examples" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Sony Corporation

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>