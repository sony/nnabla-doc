<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Data Parallel Distributed Training &mdash; Neural Network Libraries 1.39.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinx_highlight.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Function list and converter" href="function_list_and_converter.html" />
    <link rel="prev" title="Mixed Precision Training" href="mixed_precision_training.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Neural Network Libraries
          </a>
              <div class="version">
                1.39.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../python.html">Python Package</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../installation.html">Python Package Installation</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../tutorial.html">Python API Tutorial</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="by_examples.html">NNabla by Examples</a></li>
<li class="toctree-l3"><a class="reference internal" href="python_api.html">NNabla Python API Demonstration Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_finetuning.html">NNabla Models Finetuning Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_finetuning.html#finetuning-more">Finetuning more</a></li>
<li class="toctree-l3"><a class="reference internal" href="debugging.html">Debugging</a></li>
<li class="toctree-l3"><a class="reference internal" href="dynamic_and_static_nn.html">Static vs Dynamic Neural Networks in NNabla</a></li>
<li class="toctree-l3"><a class="reference internal" href="graph_converters.html">Graph Converters</a></li>
<li class="toctree-l3"><a class="reference internal" href="mixed_precision_training.html">Mixed Precision Training</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Data Parallel Distributed Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#note">NOTE</a></li>
<li class="toctree-l4"><a class="reference internal" href="#prepare-the-dependencies">Prepare the dependencies</a></li>
<li class="toctree-l4"><a class="reference internal" href="#define-the-communicator-for-gradients-exchange">Define the communicator for gradients exchange.</a></li>
<li class="toctree-l4"><a class="reference internal" href="#create-data-points-and-a-very-simple-neural-network">Create data points and a very simple neural network</a></li>
<li class="toctree-l4"><a class="reference internal" href="#create-a-solver">Create a solver.</a></li>
<li class="toctree-l4"><a class="reference internal" href="#training">Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="#advanced-topics">Advanced Topics</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="function_list_and_converter.html">Function list and converter</a></li>
<li class="toctree-l3"><a class="reference internal" href="quantization_aware_training.html">Quantization-Aware-Training Tutorial</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../command_line_interface.html">Python Command Line Interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html">Python API Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html">Python API Reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../cpp.html">C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data_exchange_file_format.html">Data exchange file format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../format.html">Data Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../file_format_converter/file_format_converter.html">File format converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../support_status.html">Support Status</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">Contributing Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Neural Network Libraries</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../python.html">Python Package</a></li>
          <li class="breadcrumb-item"><a href="../tutorial.html">Python API Tutorial</a></li>
      <li class="breadcrumb-item active">Data Parallel Distributed Training</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/python/tutorial/multi_device_training.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="data-parallel-distributed-training">
<h1>Data Parallel Distributed Training<a class="headerlink" href="#data-parallel-distributed-training" title="Permalink to this heading"></a></h1>
<p>DataParallelCommunicator enables to train your neural network using
multiple devices. It is normally used for gradients exchange in data
parallel distributed training. Basically, there are two types of
distributed training in Neural Network literature: Data Parallel and
Model Parallel. Here we only focus on the former, Data Parallel
Training. Data Parallel Distributed Training is based on the very simple
equation used for the optimization of a neural network called
(Mini-Batch) Stochastic Gradient Descent.</p>
<p>In the optimization process, the objective one tries to minimize is</p>
<div class="math notranslate nohighlight">
\[f(\mathbf{w}; X) = \frac{1}{B \times N} \sum_{i=1}^{B \times N} \ell(\mathbf{w}, \mathbf{x}_i),\]</div>
<p>where <span class="math notranslate nohighlight">\(f\)</span> is a neural network, <span class="math notranslate nohighlight">\(B \times N\)</span> is the batch
size, <span class="math notranslate nohighlight">\(\ell\)</span> is a loss function for each data point
<span class="math notranslate nohighlight">\(\mathbf{x} \in X\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> is the trainable
parameter of the neural network.</p>
<p>When taking the derivative of this objective, one gets,</p>
<div class="math notranslate nohighlight">
\[\nabla_{\mathbf{w}} f(\mathbf{w}; X) = \frac{1}{B \times N} \sum_{i=1}^{B \times N} \nabla_{\mathbf{w}} \ell (\mathbf{w}, \mathbf{x}_i).\]</div>
<p>Since the derivative has linearity, one can change the objective to the
sum of summations each of which is the sum of derivatives over <span class="math notranslate nohighlight">\(B\)</span>
data points.</p>
<div class="math notranslate nohighlight">
\[\nabla_{\mathbf{w}} f(\mathbf{w}; X) = \frac{1}{N} \left(
 \frac{1}{B} \sum_{i=1}^{B} \nabla_{\mathbf{w}} \ell (\mathbf{w}, \mathbf{x}_i) \
 + \frac{1}{B} \sum_{i=B+1}^{B \times 2} \nabla_{\mathbf{w}} \ell (\mathbf{w}, \mathbf{x}_i) \
 + \ldots \
 + \frac{1}{B} \sum_{i=B \times (N-1) + 1}^{B \times N} \nabla_{\mathbf{w}} \ell (\mathbf{w}, \mathbf{x}_i)
\right)\]</div>
<p>In data parallel distributed training, the following steps are performed
according to the above equation,</p>
<ol class="arabic simple">
<li><p>each term, summation of derivatives (gradients) divided by batch size
<span class="math notranslate nohighlight">\(B\)</span>, is computed on a separated device (typically GPU),</p></li>
<li><p>take the sum over devices,</p></li>
<li><p>divide the result by the number of devices, <span class="math notranslate nohighlight">\(N\)</span>.</p></li>
</ol>
<p>That is the underlying foundation of Data Parallel Distributed Training.</p>
<p>This tutorial shows the usage of Multi Process Data Parallel
Communicator for data parallel distributed training with a very simple
example.</p>
<section id="note">
<h2>NOTE<a class="headerlink" href="#note" title="Permalink to this heading"></a></h2>
<p>This tutorial depends on <strong>IPython Cluster</strong>, thus when you want to run
the following excerpts of the scripts on Jupyter Notebook, follow
<a class="reference external" href="https://ipython.org/ipython-doc/3/parallel/parallel_process.html#using-ipcluster-in-mpiexec-mpirun-mode">this</a>
to enable mpiexec/mpirun mode, then launch a corresponding Ipython
Cluster on Ipython Clusters tab.</p>
<section id="launch-client">
<h3>Launch client<a class="headerlink" href="#launch-client" title="Permalink to this heading"></a></h3>
<p>This code is <strong>only</strong> needed for this tutorial via <strong>Jupyter Notebook</strong>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ipyparallel</span> <span class="k">as</span> <span class="nn">ipp</span>
<span class="n">rc</span> <span class="o">=</span> <span class="n">ipp</span><span class="o">.</span><span class="n">Client</span><span class="p">(</span><span class="n">profile</span><span class="o">=</span><span class="s1">&#39;mpi&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="prepare-the-dependencies">
<h2>Prepare the dependencies<a class="headerlink" href="#prepare-the-dependencies" title="Permalink to this heading"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">px</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="kn">import</span> <span class="nn">nnabla</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">nnabla.communicators</span> <span class="k">as</span> <span class="nn">C</span>
<span class="kn">from</span> <span class="nn">nnabla.ext_utils</span> <span class="kn">import</span> <span class="n">get_extension_context</span>
<span class="kn">import</span> <span class="nn">nnabla.functions</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">nnabla.initializer</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">calc_uniform_lim_glorot</span><span class="p">,</span>
    <span class="n">UniformInitializer</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">nnabla.parametric_functions</span> <span class="k">as</span> <span class="nn">PF</span>
<span class="kn">import</span> <span class="nn">nnabla.solvers</span> <span class="k">as</span> <span class="nn">S</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</section>
<section id="define-the-communicator-for-gradients-exchange">
<h2>Define the communicator for gradients exchange.<a class="headerlink" href="#define-the-communicator-for-gradients-exchange" title="Permalink to this heading"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">px</span>
<span class="n">extension_module</span> <span class="o">=</span> <span class="s2">&quot;cudnn&quot;</span>
<span class="n">ctx</span> <span class="o">=</span> <span class="n">get_extension_context</span><span class="p">(</span><span class="n">extension_module</span><span class="p">)</span>
<span class="n">comm</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">MultiProcessCommunicator</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
<span class="n">comm</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">n_devices</span> <span class="o">=</span> <span class="n">comm</span><span class="o">.</span><span class="n">size</span>
<span class="n">mpi_rank</span> <span class="o">=</span> <span class="n">comm</span><span class="o">.</span><span class="n">rank</span>
<span class="n">device_id</span> <span class="o">=</span> <span class="n">mpi_rank</span>
<span class="n">ctx</span> <span class="o">=</span> <span class="n">get_extension_context</span><span class="p">(</span><span class="n">extension_module</span><span class="p">,</span> <span class="n">device_id</span><span class="o">=</span><span class="n">device_id</span><span class="p">)</span>
</pre></div>
</div>
<p>Check different ranks are assigned to different devices</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">px</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;n_devices=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_devices</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;mpi_rank=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mpi_rank</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">stdout</span><span class="p">:</span><span class="mi">0</span><span class="p">]</span>
<span class="n">n_devices</span><span class="o">=</span><span class="mi">2</span>
<span class="n">mpi_rank</span><span class="o">=</span><span class="mi">1</span>
<span class="p">[</span><span class="n">stdout</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span>
<span class="n">n_devices</span><span class="o">=</span><span class="mi">2</span>
<span class="n">mpi_rank</span><span class="o">=</span><span class="mi">0</span>
</pre></div>
</div>
</section>
<section id="create-data-points-and-a-very-simple-neural-network">
<h2>Create data points and a very simple neural network<a class="headerlink" href="#create-data-points-and-a-very-simple-neural-network" title="Permalink to this heading"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">px</span>
<span class="c1"># Data points setting</span>
<span class="n">n_class</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span>

<span class="c1"># Data points</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_class</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">b</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">x_data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">y_data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">x</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">x_data</span>
<span class="n">y</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">y_data</span>

<span class="c1"># Network setting</span>
<span class="n">C</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">kernel</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">px</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
                    <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">C</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                    <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">px</span>
<span class="c1"># Network</span>
<span class="k">with</span> <span class="n">nn</span><span class="o">.</span><span class="n">context_scope</span><span class="p">(</span><span class="n">ctx</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">w_init</span><span class="o">=</span><span class="n">w_init</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">PF</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">n_class</span><span class="p">,</span> <span class="n">w_init</span><span class="o">=</span><span class="n">w_init</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">softmax_cross_entropy</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
<p><strong>Important notice</strong> here is that <code class="docutils literal notranslate"><span class="pre">w_init</span></code> is passed to parametric
functions to let the network on each GPU start from the same values of
trainable parameters in the optimization process.</p>
</section>
<section id="create-a-solver">
<h2>Create a solver.<a class="headerlink" href="#create-a-solver" title="Permalink to this heading"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">px</span>
<span class="c1"># Solver and add parameters</span>
<span class="n">solver</span> <span class="o">=</span> <span class="n">S</span><span class="o">.</span><span class="n">Adam</span><span class="p">()</span>
<span class="n">solver</span><span class="o">.</span><span class="n">set_parameters</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">())</span>
</pre></div>
</div>
</section>
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this heading"></a></h2>
<p>Recall the basic usage of <code class="docutils literal notranslate"><span class="pre">nnabla</span></code> API for training a neural network,
it is</p>
<ol class="arabic simple">
<li><p>loss.forward()</p></li>
<li><p>solver.zero_grad()</p></li>
<li><p>loss.backward()</p></li>
<li><p>solver.update()</p></li>
</ol>
<p>In use of <code class="docutils literal notranslate"><span class="pre">C.MultiProcessCommunicator</span></code>, these steps are
performed in different GPUs, and the <strong>only difference</strong> from these
steps is <code class="docutils literal notranslate"><span class="pre">comm.all_reduce()</span></code>. Thus, in case of
<code class="docutils literal notranslate"><span class="pre">C.MultiProcessCommunicator</span></code> training steps are as
follows,</p>
<ol class="arabic simple">
<li><p>loss.forward()</p></li>
<li><p>solver.zero_grad()</p></li>
<li><p>loss.backward()</p></li>
<li><p><strong>comm.all_reduce([x.grad for x in nn.get_parameters().values()])</strong></p></li>
<li><p>solver.update()</p></li>
</ol>
<p>First, forward, zero_grad, and backward,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">px</span>
<span class="c1"># Training steps</span>
<span class="n">loss</span><span class="o">.</span><span class="n">forward</span><span class="p">()</span>
<span class="n">solver</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<p>Check gradients of weights once,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">px</span>
<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">nn</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">g</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">stdout</span><span class="p">:</span><span class="mi">0</span><span class="p">]</span>
<span class="p">(</span><span class="s1">&#39;conv/W&#39;</span><span class="p">,</span> <span class="n">array</span><span class="p">([[[[</span> <span class="mf">5.0180483</span><span class="p">,</span>  <span class="mf">0.457942</span> <span class="p">,</span> <span class="o">-</span><span class="mf">2.8701296</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">2.0715926</span><span class="p">,</span>  <span class="mf">3.0698593</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6650047</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">2.5591214</span><span class="p">,</span>  <span class="mf">6.4248834</span><span class="p">,</span>  <span class="mf">9.881935</span> <span class="p">]]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;conv/b&#39;</span><span class="p">,</span> <span class="n">array</span><span class="p">([</span><span class="mf">8.658947</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;affine/W&#39;</span><span class="p">,</span> <span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.93160367</span><span class="p">,</span>  <span class="mf">0.9316036</span> <span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">1.376812</span>  <span class="p">,</span>  <span class="mf">1.376812</span>  <span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">1.8957546</span> <span class="p">,</span>  <span class="mf">1.8957543</span> <span class="p">],</span>
       <span class="o">...</span><span class="p">,</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">0.33000934</span><span class="p">,</span>  <span class="mf">0.33000934</span><span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">0.7211893</span> <span class="p">,</span>  <span class="mf">0.72118926</span><span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">0.25237036</span><span class="p">,</span>  <span class="mf">0.25237036</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;affine/b&#39;</span><span class="p">,</span> <span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.48865744</span><span class="p">,</span>  <span class="mf">0.48865741</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">))</span>
<span class="p">[</span><span class="n">stdout</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span>
<span class="p">(</span><span class="s1">&#39;conv/W&#39;</span><span class="p">,</span> <span class="n">array</span><span class="p">([[[[</span> <span class="o">-</span><span class="mf">1.2505884</span> <span class="p">,</span>  <span class="o">-</span><span class="mf">0.87151337</span><span class="p">,</span>  <span class="o">-</span><span class="mf">8.685524</span>  <span class="p">],</span>
         <span class="p">[</span> <span class="mf">10.738419</span>  <span class="p">,</span>  <span class="mf">14.676786</span>  <span class="p">,</span>   <span class="mf">7.483423</span>  <span class="p">],</span>
         <span class="p">[</span>  <span class="mf">5.612471</span>  <span class="p">,</span> <span class="o">-</span><span class="mf">12.880402</span>  <span class="p">,</span>  <span class="mf">19.141157</span>  <span class="p">]]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;conv/b&#39;</span><span class="p">,</span> <span class="n">array</span><span class="p">([</span><span class="mf">13.196114</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;affine/W&#39;</span><span class="p">,</span> <span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.6865108</span> <span class="p">,</span>  <span class="mf">1.6865108</span> <span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">0.938529</span>  <span class="p">,</span>  <span class="mf">0.938529</span>  <span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">1.028422</span>  <span class="p">,</span>  <span class="mf">1.028422</span>  <span class="p">],</span>
       <span class="o">...</span><span class="p">,</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">0.98217344</span><span class="p">,</span>  <span class="mf">0.98217344</span><span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">0.97528917</span><span class="p">,</span>  <span class="mf">0.97528917</span><span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">0.413546</span>  <span class="p">,</span>  <span class="mf">0.413546</span>  <span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;affine/b&#39;</span><span class="p">,</span> <span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.7447065</span><span class="p">,</span>  <span class="mf">0.7447065</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">))</span>
</pre></div>
</div>
<p>You can see the different values on each device, then call
<code class="docutils literal notranslate"><span class="pre">all_reduce</span></code>,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">px</span>
<span class="n">comm</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">nn</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="p">()],</span> <span class="n">division</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Commonly, <code class="docutils literal notranslate"><span class="pre">all_reduce</span></code> only means the sum; however,
<code class="docutils literal notranslate"><span class="pre">comm.all_reduce</span></code> addresses both cases: summation and summation
division.</p>
<p>Again, check gradients of weights,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">px</span>
<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">nn</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">g</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">stdout</span><span class="p">:</span><span class="mi">0</span><span class="p">]</span>
<span class="p">(</span><span class="s1">&#39;conv/W&#39;</span><span class="p">,</span> <span class="n">array</span><span class="p">([[[[</span> <span class="mf">1.8837299</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.20678568</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.777827</span>  <span class="p">],</span>
         <span class="p">[</span> <span class="mf">6.4050055</span> <span class="p">,</span>  <span class="mf">8.8733225</span> <span class="p">,</span>  <span class="mf">2.9092093</span> <span class="p">],</span>
         <span class="p">[</span> <span class="mf">1.5266749</span> <span class="p">,</span> <span class="o">-</span><span class="mf">3.2277591</span> <span class="p">,</span> <span class="mf">14.511546</span>  <span class="p">]]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;conv/b&#39;</span><span class="p">,</span> <span class="n">array</span><span class="p">([</span><span class="mf">21.85506</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;affine/W&#39;</span><span class="p">,</span> <span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">2.6181145</span><span class="p">,</span>  <span class="mf">2.6181145</span><span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">2.315341</span> <span class="p">,</span>  <span class="mf">2.315341</span> <span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">2.9241767</span><span class="p">,</span>  <span class="mf">2.9241762</span><span class="p">],</span>
       <span class="o">...</span><span class="p">,</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">1.3121828</span><span class="p">,</span>  <span class="mf">1.3121828</span><span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">1.6964785</span><span class="p">,</span>  <span class="mf">1.6964784</span><span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">0.6659163</span><span class="p">,</span>  <span class="mf">0.6659163</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;affine/b&#39;</span><span class="p">,</span> <span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.233364</span> <span class="p">,</span>  <span class="mf">1.2333639</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">))</span>
<span class="p">[</span><span class="n">stdout</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span>
<span class="p">(</span><span class="s1">&#39;conv/W&#39;</span><span class="p">,</span> <span class="n">array</span><span class="p">([[[[</span> <span class="mf">1.8837299</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.20678568</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.777827</span>  <span class="p">],</span>
         <span class="p">[</span> <span class="mf">6.4050055</span> <span class="p">,</span>  <span class="mf">8.8733225</span> <span class="p">,</span>  <span class="mf">2.9092093</span> <span class="p">],</span>
         <span class="p">[</span> <span class="mf">1.5266749</span> <span class="p">,</span> <span class="o">-</span><span class="mf">3.2277591</span> <span class="p">,</span> <span class="mf">14.511546</span>  <span class="p">]]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;conv/b&#39;</span><span class="p">,</span> <span class="n">array</span><span class="p">([</span><span class="mf">21.85506</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;affine/W&#39;</span><span class="p">,</span> <span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">2.6181145</span><span class="p">,</span>  <span class="mf">2.6181145</span><span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">2.315341</span> <span class="p">,</span>  <span class="mf">2.315341</span> <span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">2.9241767</span><span class="p">,</span>  <span class="mf">2.9241762</span><span class="p">],</span>
       <span class="o">...</span><span class="p">,</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">1.3121828</span><span class="p">,</span>  <span class="mf">1.3121828</span><span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">1.6964785</span><span class="p">,</span>  <span class="mf">1.6964784</span><span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">0.6659163</span><span class="p">,</span>  <span class="mf">0.6659163</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">))</span>
<span class="p">(</span><span class="s1">&#39;affine/b&#39;</span><span class="p">,</span> <span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.233364</span> <span class="p">,</span>  <span class="mf">1.2333639</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">))</span>
</pre></div>
</div>
<p>You can see the same values over the devices because of <code class="docutils literal notranslate"><span class="pre">all_reduce</span></code>.</p>
<p>Update weights,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">px</span>
<span class="n">solver</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
</pre></div>
</div>
<p>This concludes the usage of <code class="docutils literal notranslate"><span class="pre">C.MultiProcessDataCommunicator</span></code>
for Data Parallel Distributed Training.</p>
<p>Now you should have an understanding of how to use
<code class="docutils literal notranslate"><span class="pre">C.MultiProcessCommunicator</span></code>, go to the cifar10 example,</p>
<ol class="arabic simple">
<li><p><strong>classification.py</strong></p></li>
</ol>
<p>for more details.</p>
</section>
<section id="advanced-topics">
<h2>Advanced Topics<a class="headerlink" href="#advanced-topics" title="Permalink to this heading"></a></h2>
<p>When working with multiple nodes with multiple devices (e.g. GPUs),
one or a few of them might stop response for some special cases.
When your training process originally takes time, it is hard to
identify the elapsed time is in training or for dead device.</p>
<p>In current implementation, we introduced the watch dog in all_reduce().
When any node or any device stop response, the watch dog will raise an exception.
The typical time for all_reduce() is 60 seconds. It means the process in any
node or any device cannot wait at all_reduce() for more than 60 seconds, otherwise,
some node or device might highly definitely stop response.</p>
<p>The watch dog is default disabled, if want to enable it, please set environment variable
<code class="docutils literal notranslate"><span class="pre">NNABLA_MPI_WATCH_DOG_ENABLE</span></code> to any none-zero value:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">NNABLA_MPI_WATCH_DOG_ENABLE</span><span class="o">=</span><span class="m">1</span>
</pre></div>
</div>
<p>But in practice, some task required to be performed on one a few of nodes,
and let other nodes wait there. If no explicitly sychronization, the watch dog might
be unexpectedly triggered. As the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">extension_module</span> <span class="o">=</span> <span class="s2">&quot;cudnn&quot;</span>
<span class="n">type_config</span> <span class="o">=</span> <span class="s2">&quot;float&quot;</span>
<span class="n">ctx</span> <span class="o">=</span> <span class="n">get_extension_context</span><span class="p">(</span><span class="n">extension_module</span><span class="p">,</span> <span class="n">type_config</span><span class="o">=</span><span class="n">type_config</span><span class="p">)</span>
<span class="n">comm</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">MultiProcessDataParalellCommunicator</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
<span class="n">comm</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="k">if</span> <span class="n">comm</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
   <span class="o">...</span>  <span class="c1"># Here, we do some task on node 0</span>

<span class="k">if</span> <span class="n">comm</span><span class="o">.</span><span class="n">rank</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
   <span class="o">...</span>  <span class="c1"># here, we do some task on other nodes</span>

 <span class="c1"># Till here, multiple nodes has different progress</span>

 <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">data_iterator</span><span class="p">():</span>
     <span class="o">...</span>
     <span class="n">comm</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># Here, since different nodes has different</span>
                           <span class="c1"># start points, all_reduce() might trigger</span>
                           <span class="c1"># watch dog timeout exception.</span>
</pre></div>
</div>
<p>In order to avoid above unexpected exception, we have to explicitly set the
synchronization point.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">extension_module</span> <span class="o">=</span> <span class="s2">&quot;cudnn&quot;</span>
<span class="n">type_config</span> <span class="o">=</span> <span class="s2">&quot;float&quot;</span>
<span class="n">ctx</span> <span class="o">=</span> <span class="n">get_extension_context</span><span class="p">(</span><span class="n">extension_module</span><span class="p">,</span> <span class="n">type_config</span><span class="o">=</span><span class="n">type_config</span><span class="p">)</span>
<span class="n">comm</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">MultiProcessDataParalellCommunicator</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
<span class="n">comm</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="k">if</span> <span class="n">comm</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
   <span class="o">...</span>  <span class="c1"># Here, we do some task on node 0</span>

<span class="k">if</span> <span class="n">comm</span><span class="o">.</span><span class="n">rank</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
   <span class="o">...</span>  <span class="c1"># here, we do some task on other nodes</span>

 <span class="n">comm</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>  <span class="c1"># we placed the synchronization point immediate before</span>
                 <span class="c1"># comm.all_reduce().</span>

 <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">data_iterator</span><span class="p">():</span>
     <span class="o">...</span>
     <span class="n">comm</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="o">...</span><span class="p">)</span> <span class="c1"># The wait time at all_reduce() should be strictly</span>
                          <span class="c1"># limited in a relative short time.</span>
</pre></div>
</div>
<p>We placed the synchronization point immediately before comm.all_reduce(), which means
that we knew comm.all_reduce() should be perform synchronously after this point.
Thus, we may ensure the whole training can be performed stably and not need to wait
forever due to a corrupted process.</p>
<p>When watch dog is enabled, developers may also change the timeout time if they think
the default timeout time (default is 60s) is not proper. The timeout can be set by the
following:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">NNABLA_MPI_WATCH_DOG_TIMEOUT</span><span class="o">=</span><span class="m">30</span>
</pre></div>
</div>
<p>The time unit is second. Here, 30 means 30 seconds. It means if any node stops response
for more than 30 seconds, the watch dog will kill the training process and show fatal
error message.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="mixed_precision_training.html" class="btn btn-neutral float-left" title="Mixed Precision Training" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="function_list_and_converter.html" class="btn btn-neutral float-right" title="Function list and converter" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2017, Sony Corporation.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>