<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>nnabla.parametric_functions &mdash; Neural Network Libraries 1.38.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Neural Network Libraries
          </a>
              <div class="version">
                1.38.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../python.html">Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cpp.html">C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data_exchange_file_format.html">Data exchange file format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../format.html">Data Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python/file_format_converter/file_format_converter.html">File format converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../support_status.html">Support Status</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">Contributing Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Neural Network Libraries</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Module code</a></li>
      <li class="breadcrumb-item active">nnabla.parametric_functions</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for nnabla.parametric_functions</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2017,2018,2019,2020,2021 Sony Corporation.</span>
<span class="c1"># Copyright 2021 Sony Group Corporation.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>

<span class="kn">from</span> <span class="nn">six</span> <span class="kn">import</span> <span class="n">exec_</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">nnabla</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">nnabla.functions</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">nnabla.parameter</span> <span class="kn">import</span> <span class="n">get_parameter_or_create</span><span class="p">,</span> <span class="n">get_parameter</span>
<span class="kn">from</span> <span class="nn">nnabla.initializer</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">calc_uniform_lim_glorot</span><span class="p">,</span>
    <span class="n">ConstantInitializer</span><span class="p">,</span> <span class="n">NormalInitializer</span><span class="p">,</span> <span class="n">UniformInitializer</span><span class="p">,</span>
    <span class="n">WeightNormalizationScaleInitializer</span><span class="p">)</span>


<div class="viewcode-block" id="parametric_function_api"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.parametric_function_api">[docs]</a><span class="k">def</span> <span class="nf">parametric_function_api</span><span class="p">(</span><span class="n">scope_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">param_desc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Decorator for parametric functions.</span>

<span class="sd">    The decorated function is always called under</span>
<span class="sd">    a parameter scope ``scope_name``.</span>
<span class="sd">    Also, the decorator adds an additional argument ``name`` (:obj:`str`,</span>
<span class="sd">    default is ``None``) at the end. If ``name`` is specified, the</span>
<span class="sd">    scope ``scope_name`` comes under a scope ``name``. This feature</span>
<span class="sd">    could reduce vertical space usage of the source code.</span>
<span class="sd">    Any parametric function should be decorated by this.</span>

<span class="sd">    Args:</span>
<span class="sd">        scope_name (str, optional): The original function will be called</span>
<span class="sd">            under a parameter scope named by ``scope_name``.</span>
<span class="sd">        param_desc (list, optional):</span>
<span class="sd">            Descriptions of parameters will be automatically included into docstring.</span>
<span class="sd">            This must be a list of tuples with 4 elements composed of</span>
<span class="sd">            (name (str), description (str), shape info (str), need_grad (bool)).</span>

<span class="sd">    Returns:</span>
<span class="sd">        function: A decorated parametric function.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">scope_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scope_name</span> <span class="o">=</span> <span class="n">name</span>

    <span class="k">def</span> <span class="nf">parametric_function_api_inside</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">.utils.signature_utils</span> <span class="kn">import</span> <span class="n">SignatureEx</span>

        <span class="n">name</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="n">doc</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="vm">__doc__</span>

        <span class="k">if</span> <span class="n">param_desc</span><span class="p">:</span>
            <span class="n">indent</span> <span class="o">=</span> <span class="mi">8</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">desc</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">d</span><span class="p">:</span> <span class="s1">&#39; &#39;</span> <span class="o">*</span> <span class="n">indent</span> <span class="o">+</span>
                           <span class="s1">&#39;* </span><span class="si">{}</span><span class="s1"> (``need_grad=</span><span class="si">{}</span><span class="s1">``) : </span><span class="si">{}</span><span class="s1">. (shape: ``</span><span class="si">{}</span><span class="s1">``)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">d</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">d</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">d</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span> <span class="n">param_desc</span><span class="p">)</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;param_desc argument of parametric_function_api must be &#39;</span>
                    <span class="s1">&#39;None or a list of tuple with three elements composed of &#39;</span>
                    <span class="s1">&#39;(name(str), description(str), need_grad(bool)).&#39;</span><span class="p">)</span>
            <span class="n">doc</span> <span class="o">+=</span> <span class="s1">&#39;&#39;&#39;</span>
<span class="s1">    Parameters to be registered</span>
<span class="s1">        The following variables are registered in a parameter scope ``&quot;</span><span class="si">{}</span><span class="s1">&quot;``;</span>

<span class="si">{}</span>

<span class="s1">            &#39;&#39;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">scope_name</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">desc</span><span class="p">))</span>

        <span class="n">doc</span> <span class="o">+=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Note:</span>

<span class="s2">        If the ``name`` option is passed, the parameters become wrapped inside the parameter scope</span>
<span class="s2">        with the specified name, yielding the same results as the following code.</span>
<span class="s2">        This can be used to simplify the code.</span>

<span class="s2">        .. code-block:: python</span>

<span class="s2">            with parameter_scope(name):</span>
<span class="s2">                output = </span><span class="si">{name}</span><span class="s2">(&lt;args&gt;)</span>

<span class="s2">        &quot;&quot;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

        <span class="c1"># Parsing argspecs</span>
        <span class="n">sig</span> <span class="o">=</span> <span class="n">SignatureEx</span><span class="o">.</span><span class="n">from_callable</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
        <span class="n">sig2</span> <span class="o">=</span> <span class="n">sig</span><span class="o">.</span><span class="n">add_arg</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
        <span class="n">signature</span> <span class="o">=</span> <span class="s1">&#39;(&#39;</span> <span class="o">+</span> <span class="n">sig2</span><span class="o">.</span><span class="n">format_argument_signature</span><span class="p">()</span> <span class="o">+</span> <span class="s1">&#39;)&#39;</span> <span class="o">+</span> \
            <span class="n">sig</span><span class="o">.</span><span class="n">format_return_annotation</span><span class="p">()</span>
        <span class="n">shortsignature</span> <span class="o">=</span> <span class="n">sig</span><span class="o">.</span><span class="n">format_caller_argument_signature</span><span class="p">()</span>

        <span class="c1"># Check required argument</span>
        <span class="k">assert</span> <span class="s1">&#39;fix_parameters&#39;</span> <span class="ow">in</span> <span class="n">sig</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span> \
            <span class="s2">&quot;A parametric function must take `fix_parameters` as an argument.&quot;</span> \
            <span class="s2">&quot; `</span><span class="si">{}{}</span><span class="s2">` doesn&#39;t have it.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">signature</span><span class="p">)</span>

        <span class="n">code</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">def </span><span class="si">{name}{signature}</span><span class="s2">:</span>
<span class="s2">    if name is None:</span>
<span class="s2">        with parameter_scope(scope_name):</span>
<span class="s2">            return func(</span><span class="si">{shortsignature}</span><span class="s2">)</span>
<span class="s2">    with parameter_scope(name):</span>
<span class="s2">        with parameter_scope(scope_name):</span>
<span class="s2">            return func(</span><span class="si">{shortsignature}</span><span class="s2">)</span>
<span class="s2">        &quot;&quot;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="nb">locals</span><span class="p">())</span>
        <span class="n">execdict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="n">func</span><span class="o">=</span><span class="n">func</span><span class="p">,</span> <span class="n">parameter_scope</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">,</span> <span class="n">scope_name</span><span class="o">=</span><span class="n">scope_name</span><span class="p">)</span>
        <span class="n">exec_</span><span class="p">(</span><span class="n">code</span><span class="p">,</span> <span class="n">execdict</span><span class="p">)</span>
        <span class="n">newfunc</span> <span class="o">=</span> <span class="n">execdict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
        <span class="n">newfunc</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">doc</span>
        <span class="n">newfunc</span><span class="o">.</span><span class="n">__parametric_function_api_base__</span> <span class="o">=</span> <span class="n">func</span>
        <span class="n">newfunc</span><span class="o">.</span><span class="n">__scope_name__</span> <span class="o">=</span> <span class="n">scope_name</span>
        <span class="n">newfunc</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">=</span> <span class="vm">__name__</span>
        <span class="k">return</span> <span class="n">newfunc</span>
    <span class="k">return</span> <span class="n">parametric_function_api_inside</span></div>


<div class="viewcode-block" id="affine"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.affine">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;affine&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Weight matrix&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outputs,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span>
           <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
           <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
           <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
           <span class="n">apply_w</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">apply_b</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The affine layer, also known as the fully connected layer. Computes</span>

<span class="sd">    .. math::</span>
<span class="sd">        {\\mathbf y} = {\\mathbf A} {\\mathbf x} + {\\mathbf b}.</span>

<span class="sd">    where :math:`{\\mathbf x}, {\\mathbf y}` are the inputs and outputs respectively,</span>
<span class="sd">    and :math:`{\\mathbf A}, {\\mathbf b}` are constants.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): Input N-D array with shape (:math:`M_0 \\times \ldots \\times M_{B-1} \\times D_B \\times \ldots \\times D_N`). Dimensions before and after base_axis are flattened as if it is a matrix.</span>
<span class="sd">        n_outmaps (:obj:`int` or :obj:`tuple` of :obj:`int`): Number of output neurons per data.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.</span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>
<span class="sd">        apply_w (function): Lambda, function, or callable object applied to the weights.</span>
<span class="sd">        apply_b (function): Lambda, function, or callable object applied to the bias.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: :math:`(B + 1)`-D array. (:math:`M_0 \\times \ldots \\times M_{B-1} \\times L`)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">,</span> <span class="s1">&#39;__iter__&#39;</span><span class="p">):</span>
        <span class="n">n_outmaps</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_outmaps</span><span class="p">]</span>
    <span class="n">n_outmaps</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">)</span>
    <span class="n">n_outmap</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inmaps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inmaps</span><span class="p">,</span> <span class="n">n_outmap</span><span class="p">),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">apply_w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">apply_w</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">apply_b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">apply_b</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="svd_affine"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.svd_affine">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;svd_affine&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;U&#39;</span><span class="p">,</span> <span class="s1">&#39;:math:`{</span><span class="se">\\</span><span class="s1">mathbf U}`&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, r)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;V&#39;</span><span class="p">,</span> <span class="s1">&#39;:math:`{</span><span class="se">\\</span><span class="s1">mathbf V}`&#39;</span><span class="p">,</span> <span class="s1">&#39;(r, outmaps)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">svd_affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">uv_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;SVD affine is a low rank approximation of the affine layer. It can</span>
<span class="sd">    be seen as two consecutive affine layers with a bottleneck. It</span>
<span class="sd">    computes:</span>

<span class="sd">    .. math::</span>
<span class="sd">        {\\mathbf y} = {\\mathbf U} {\\mathbf V} {\\mathbf x} + {\\mathbf b}.</span>

<span class="sd">    where :math:`{\\mathbf x}, {\\mathbf y}` are the inputs and</span>
<span class="sd">    outputs respectively, and :math:`{\\mathbf U}, {\\mathbf V},</span>
<span class="sd">    {\\mathbf b}` are constants.</span>

<span class="sd">    The weights :math:`{\\mathbf U}` and :math:`{\\mathbf V}` are</span>
<span class="sd">    approximated with singular value decomposition (SVD) of the</span>
<span class="sd">    original weight matrix :math:`{\\mathbf W}` and by selecting the</span>
<span class="sd">    :math:`{R}` dominant singular values and the corresponding</span>
<span class="sd">    singular vectors. Therefore the low rank :math:`{R}` is the size</span>
<span class="sd">    of the bottleneck.</span>

<span class="sd">    If `uv_init` is a numpy array, :math:`{\\mathbf U}` and</span>
<span class="sd">    :math:`{\\mathbf V}` are computed such that `uv_init` is</span>
<span class="sd">    approximated by :math:`{\\mathbf{UV}}`. If `uv_init` is `None` or</span>
<span class="sd">    an initializer, the product of :math:`{\\mathbf U}` and</span>
<span class="sd">    :math:`{\\mathbf V}` approximates the random initialization.</span>

<span class="sd">    If :math:`{\\mathbf U}` and :math:`{\\mathbf V}` exist in the context,</span>
<span class="sd">    they take precedence over `uv_init`.</span>

<span class="sd">    Suppose the weight of the affine is of :math:`{I \\times O}` and</span>
<span class="sd">    the compression rate you want to specify is :math:`{CR}`, then you</span>
<span class="sd">    set :math:`{R}` as</span>

<span class="sd">    .. math::</span>

<span class="sd">        R = \\left\\lfloor \\frac{(1 - CR)OI}{O + I} \\right\\rfloor.</span>

<span class="sd">    Args:</span>

<span class="sd">        inp (~nnabla.Variable): Input N-D array with shape (:math:`M_0</span>
<span class="sd">          \\times \ldots \\times M_{B-1} \\times D_B \\times \ldots</span>
<span class="sd">          \\times D_N`). Dimensions before and after base_axis are</span>
<span class="sd">          flattened as if it is a matrix.</span>

<span class="sd">        n_outmaps (int or tuple): Number of output neurons per data.</span>

<span class="sd">        r (int): rank of the factorized layer (size of the bottleneck)</span>

<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as</span>
<span class="sd">          the sample dimensions.</span>

<span class="sd">        uv_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`):</span>
<span class="sd">          Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`. </span>

<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights</span>
<span class="sd">          and biases will not be updated.</span>

<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>

<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: :math:`(B + 1)`-D array.</span>
<span class="sd">        (:math:`M_0 \\times \ldots \\times M_{B-1} \\times L`)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">,</span> <span class="s1">&#39;__iter__&#39;</span><span class="p">):</span>
        <span class="n">n_outmaps</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_outmaps</span><span class="p">]</span>

    <span class="n">n_outmaps</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">)</span>
    <span class="n">n_outmap</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">))</span>
    <span class="n">inmaps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])</span>

    <span class="k">if</span> <span class="n">uv_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">uv_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inmaps</span><span class="p">,</span> <span class="n">n_outmap</span><span class="p">),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">uv_init</span><span class="p">)</span> <span class="ow">is</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="c1"># TODO: Assert that size of uv_init is correct</span>
        <span class="c1"># uv is initialize with numpy array</span>
        <span class="n">uv</span> <span class="o">=</span> <span class="n">uv_init</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># uv is initialize from initializer</span>
        <span class="n">uv</span> <span class="o">=</span> <span class="n">uv_init</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])),</span> <span class="p">]</span> <span class="o">+</span>
                     <span class="nb">list</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">))</span>

    <span class="n">u</span> <span class="o">=</span> <span class="n">get_parameter</span><span class="p">(</span><span class="s1">&#39;U&#39;</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">get_parameter</span><span class="p">(</span><span class="s1">&#39;V&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">u</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">r</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;svd_ffine: The rank must be larger than zero&quot;</span>
        <span class="n">u_</span><span class="p">,</span> <span class="n">s_</span><span class="p">,</span> <span class="n">v_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">uv</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">inmaps</span><span class="p">,</span> <span class="n">n_outmap</span><span class="p">),</span>
                                   <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">u_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u_</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s_</span><span class="p">))</span>  <span class="c1"># fold s into u</span>
        <span class="n">u_</span> <span class="o">=</span> <span class="n">u_</span><span class="p">[:,</span> <span class="p">:</span><span class="n">r</span><span class="p">]</span>
        <span class="n">v_</span> <span class="o">=</span> <span class="n">v_</span><span class="p">[:</span><span class="n">r</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">v_</span> <span class="o">=</span> <span class="n">v_</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">r</span><span class="p">]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">)</span>

        <span class="n">u</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])),</span> <span class="n">r</span><span class="p">],</span>
                        <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">u</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">u_</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">set_parameter</span><span class="p">(</span><span class="s2">&quot;U&quot;</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span>

        <span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="n">r</span><span class="p">]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">v</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">v_</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">set_parameter</span><span class="p">(</span><span class="s2">&quot;V&quot;</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fix_parameters</span> <span class="o">==</span> <span class="n">u</span><span class="o">.</span><span class="n">need_grad</span><span class="p">:</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">u</span><span class="o">.</span><span class="n">get_unlinked_variable</span><span class="p">(</span><span class="n">need_grad</span><span class="o">=</span><span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fix_parameters</span> <span class="o">==</span> <span class="n">v</span><span class="o">.</span><span class="n">need_grad</span><span class="p">:</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">get_unlinked_variable</span><span class="p">(</span><span class="n">need_grad</span><span class="o">=</span><span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
        <span class="n">v</span><span class="o">.</span><span class="n">need_grad</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">fix_parameters</span>

    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>

    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="n">base_axis</span><span class="p">),</span>
                    <span class="n">v</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="n">base_axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="binary_connect_affine"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.binary_connect_affine">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;bicon_affine&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Weight matrix in floating type&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;Wb&#39;</span><span class="p">,</span> <span class="s1">&#39;Binarized weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">binary_connect_affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span>
                          <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">quantize_zero_to</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                          <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">wb_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Binary Connect Affine, multiplier-less inner-product.</span>

<span class="sd">    Binary Connect Affine is an affine function,</span>
<span class="sd">    except the definition of the inner product is modified.</span>
<span class="sd">    The input-output relation of this function is as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_i = \sum_{i} sign(w_i) x_i.</span>

<span class="sd">    Therefore :math:`sign(w_i)` is either :math:`1` or :math:`-1` and the inner product</span>
<span class="sd">    simplifies to addition.</span>

<span class="sd">    This function should be used together with Batch Normalization.</span>

<span class="sd">    References:</span>

<span class="sd">        M. Courbariaux, Y. Bengio, and J.-P. David. &quot;BinaryConnect:</span>
<span class="sd">        Training Deep Neural Networks with binary weights during propagations.&quot;</span>
<span class="sd">        Advances in Neural Information Processing Systems. 2015.</span>

<span class="sd">    .. note::</span>

<span class="sd">        1) if you would like to share weights between some layers, please</span>
<span class="sd">        make sure to share the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the binarized weights (`binary_weight`)</span>

<span class="sd">        2) The weights and the binary weights become synced only after :func:`~nnabla._variable.Variable.forward` is called,</span>
<span class="sd">        and not after a call to :func:`~nnabla._variable.Variable.backward`.</span>
<span class="sd">        To access the parameters of the network, remember to call :func:`~nnabla._variable.Variable.forward` once before doing so, otherwise the</span>
<span class="sd">        float weights and the binary weights will not be in sync.</span>

<span class="sd">        3) Quantized values are stored as floating point number for `binary_weight`,</span>
<span class="sd">        since this function is only for simulation purposes.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): Input N-D array with shape (:math:`M_0 \\times \ldots \\times M_{B-1} \\times D_B \\times \ldots \\times D_N`). Dimensions before and after base_axis are flattened as if it is a matrix.</span>
<span class="sd">        n_outmaps (int or :obj:`tuple` of :obj:`int`): Number of output neurons per data.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        quantize_zero_to (float): Input value at zero is quantized to this value.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.  </span>
<span class="sd">        wb_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for binary weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.   </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">,</span> <span class="s1">&#39;__iter__&#39;</span><span class="p">):</span>
        <span class="n">n_outmaps</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_outmaps</span><span class="p">]</span>
    <span class="n">n_outmaps</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">)</span>
    <span class="n">n_outmap</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fan_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">fan_in</span><span class="p">,</span> <span class="n">n_outmap</span><span class="p">),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">wb_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fan_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])</span>
        <span class="n">wb_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">fan_in</span><span class="p">,</span> <span class="n">n_outmap</span><span class="p">),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">wb</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;Wb&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
        <span class="n">wb_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_connect_affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">wb</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">quantize_zero_to</span><span class="p">)</span></div>


<div class="viewcode-block" id="binary_weight_affine"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.binary_weight_affine">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;bwn_affine&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Weight matrix in floating type&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;Wb&#39;</span><span class="p">,</span> <span class="s1">&#39;Binarized weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="s1">&#39;Scaling factor :math:`</span><span class="se">\\</span><span class="s1">alpha`&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">binary_weight_affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span>
                         <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">quantize_zero_to</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                         <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">wb_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Binary Weight Affine, multiplier-less inner-product with a scale factor.</span>

<span class="sd">    Binary Weight Affine is the affine function, but the inner product</span>
<span class="sd">    in this function is the following,</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_j = \\frac{1}{\\|\\mathbf{w}_j\\|_{\\ell_1}} \sum_{i} sign(w_{ji}) x_i</span>

<span class="sd">    Therefore :math:`sign(w_{ji})` is either :math:`1` or :math:`-1` and the inner product</span>
<span class="sd">    simplifies to addition followed by scaling factor :math:`\\alpha = \\frac{1}{\\|\\mathbf{w}_j\\|_{\\ell_1}}`.</span>
<span class="sd">    The number of ::math:`\\alpha` is the outmaps of the affine function.</span>

<span class="sd">    References:</span>

<span class="sd">        Rastegari, Mohammad, et al. &quot;XNOR-Net: ImageNet Classification Using</span>
<span class="sd">        Binary Convolutional Neural Networks.&quot; arXiv preprint</span>
<span class="sd">        arXiv:1603.05279 (2016).</span>

<span class="sd">    .. note::</span>

<span class="sd">        1) if you would like to share weights between some layers, please</span>
<span class="sd">        make sure to share the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the binarized weights (`binary_weight`)</span>

<span class="sd">        2) The weights and the binary weights become synced only after :func:`~nnabla._variable.Variable.forward` is called,</span>
<span class="sd">        and not after a call to :func:`~nnabla._variable.Variable.backward`.</span>
<span class="sd">        To access the parameters of the network, remember to call :func:`~nnabla._variable.Variable.forward` once before doing so, otherwise the</span>
<span class="sd">        float weights and the binary weights will not be in sync.</span>

<span class="sd">        3) Quantized values are stored as floating point number for `binary_weight`,</span>
<span class="sd">        since this function is only for simulation purposes.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): Input N-D array with shape (:math:`M_0 \\times \ldots \\times M_{B-1} \\times D_B \\times \ldots \\times D_N`). Dimensions before and after base_axis are flattened as if it was a matrix.</span>
<span class="sd">        n_outmaps (int or :obj:`tuple` of :obj:`int`): Number of output neurons per data.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        quantize_zero_to (float): Input value at zero is quantized to this value.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for the weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`. </span>
<span class="sd">        wb_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for the binary weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.</span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for the bias. By defalut, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weight and bias will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">,</span> <span class="s1">&#39;__iter__&#39;</span><span class="p">):</span>
        <span class="n">n_outmaps</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_outmaps</span><span class="p">]</span>
    <span class="n">n_outmaps</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">)</span>
    <span class="n">n_outmap</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fan_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">fan_in</span><span class="p">,</span> <span class="n">n_outmap</span><span class="p">),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">wb_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fan_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])</span>
        <span class="n">wb_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">fan_in</span><span class="p">,</span> <span class="n">n_outmap</span><span class="p">),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">wb</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;Wb&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
        <span class="n">wb_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_weight_affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">wb</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">quantize_zero_to</span><span class="p">)</span></div>


<div class="viewcode-block" id="inq_affine"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.inq_affine">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;inq_affine&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Weight matrix in floating type&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;I&#39;</span><span class="p">,</span> <span class="s1">&#39;Binary indicator matrix of fixed weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">inq_affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
               <span class="n">inq_iterations</span><span class="o">=</span><span class="p">(),</span> <span class="n">selection_algorithm</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">,</span>
               <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">i_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Incremental Network Quantization Affine Layer</span>

<span class="sd">    During training, the weights are sequentially quantized to power-of-two</span>
<span class="sd">    values, which allows the training of a multiplierless network.</span>

<span class="sd">    Using `inq_iterations`, one can specify after how many forward passes</span>
<span class="sd">    half of the learnable weights are fixed and quantized to powers-of-two.</span>
<span class="sd">    After reaching the last value in `inq_iterations`, all weights are fixed.</span>

<span class="sd">    For more details, please refer to the reference.</span>

<span class="sd">    Reference:</span>
<span class="sd">    Zhou A, Yao A, Guo Y, Xu L, Chen Y. Incremental network quantization:</span>
<span class="sd">    Towards lossless CNNs with low-precision weights.</span>
<span class="sd">    &lt;https://arxiv.org/abs/1702.03044&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): Input N-D array with shape (:math:`M_0 \\times \ldots \\times M_{B-1} \\times D_B \\times \ldots \\times D_N`). Dimensions before and after base_axis are flattened as if it was a matrix.</span>
<span class="sd">        n_outmaps (int or :obj:`tuple` of :obj:`int`): Number of output neurons per data.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        quantize_zero_to (float): Input value at zero is quantized to this value.</span>
<span class="sd">        num_bits (int): Number of bits per weight. Value has to be larger than 1 as one bit is already used to code the value &quot;0&quot;</span>
<span class="sd">        inq_iterations (tuple of int): Tuple of iteration numbers at which we fix half of the weights.</span>
<span class="sd">        selection_algorithm (str): Chooses algorithm that is used to decide which weights are fixed. (&quot;largest_abs&quot; ... fix weights with largest absolute value, &quot;random&quot; ... fix weights randomly)</span>
<span class="sd">        seed (int): Random seed for INQ algorithm</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.  </span>
<span class="sd">        i_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for indicators (0 ... learnable, 1 ... fixed). By default, it is initialized with zeros.</span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weight and bias will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">,</span> <span class="s1">&#39;__iter__&#39;</span><span class="p">):</span>
        <span class="n">n_outmaps</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_outmaps</span><span class="p">]</span>
    <span class="n">n_outmaps</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">)</span>
    <span class="n">n_outmap</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fan_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">fan_in</span><span class="p">,</span> <span class="n">n_outmap</span><span class="p">),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fan_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])</span>
        <span class="n">i_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;I&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
        <span class="n">i_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">inq_affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">num_bits</span><span class="p">,</span> <span class="n">inq_iterations</span><span class="p">,</span> <span class="n">selection_algorithm</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>


<div class="viewcode-block" id="convolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.convolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;conv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps // group, *kernel)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span>
                <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">channel_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">apply_w</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">apply_b</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;N-D Convolution with a bias term.</span>

<span class="sd">    For Dilated Convolution (a.k.a. Atrous Convolution), refer to:</span>

<span class="sd">    - Chen et al., DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. https://arxiv.org/abs/1606.00915</span>

<span class="sd">    - Yu et al., Multi-Scale Context Aggregation by Dilated Convolutions. https://arxiv.org/abs/1511.07122</span>

<span class="sd">    Note:</span>

<span class="sd">        Convolution is a computationally intensive operation that</span>
<span class="sd">        should preferably be run with the `cudnn` backend. NNabla</span>
<span class="sd">        then uses CuDNN library functions to determine and cache the</span>
<span class="sd">        fastest algorithm for the given set of convolution parameters,</span>
<span class="sd">        which results in additional memory consumption which may pose</span>
<span class="sd">        a problem for GPUs with insufficient memory size. In that</span>
<span class="sd">        case, the `NNABLA_CUDNN_WORKSPACE_LIMIT` environment variable</span>
<span class="sd">        can be used to restrict the choice of algorithms to those that</span>
<span class="sd">        fit the given workspace memory limit, expressed in bytes. In</span>
<span class="sd">        some cases it may also be desired to restrict the automatic</span>
<span class="sd">        search to algorithms that produce deterministic (reproducible)</span>
<span class="sd">        results. This can be requested by setting the the environment</span>
<span class="sd">        variable `NNABLA_CUDNN_DETERMINISTIC` to a non-zero value.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array.</span>
<span class="sd">        outmaps (int): Number of convolution kernels (which is equal to the number of output channels). For example, to apply convolution on an input with 16 types of filters, specify 16.</span>
<span class="sd">        kernel (:obj:`tuple` of :obj:`int`): Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</span>
<span class="sd">        pad (:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">        stride (:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">        dilation (:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">        group (int): Number of groups of channels. This makes connections across channels more sparse by grouping connections along map direction.</span>
<span class="sd">        channel_last (bool): If True, the last dimension is considered as channel dimension, a.k.a. NHWC order.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.  </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>
<span class="sd">        apply_w (function): Lambda, function, or callable object applied to the weights.</span>
<span class="sd">        apply_b (function): Lambda, function, or callable object applied to the bias.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: N-D array. See :obj:`~nnabla.functions.convolution` for the output shape.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">channel_last</span><span class="p">:</span>
        <span class="n">channels</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">filter_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">channels</span> <span class="o">//</span> <span class="n">group</span><span class="p">,)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">channels</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span>
        <span class="n">filter_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">channels</span> <span class="o">//</span> <span class="n">group</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,)</span> <span class="o">+</span> <span class="n">filter_shape</span><span class="p">,</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">apply_w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">apply_w</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">apply_b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">apply_b</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">channel_last</span><span class="p">)</span></div>


<div class="viewcode-block" id="svd_convolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.svd_convolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;svd_conv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;U&#39;</span><span class="p">,</span>
     <span class="s1">&#39;Decomposed filter weights :math:`{</span><span class="se">\\</span><span class="s1">mathbf U}`&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps * r, *kernel)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;V&#39;</span><span class="p">,</span> <span class="s1">&#39;Decomposed filter weights :math:`{</span><span class="se">\\</span><span class="s1">mathbf V}`&#39;</span><span class="p">,</span>
     <span class="s1">&#39;(outmaps, inmaps * r, 1, ...)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">svd_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">uv_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;SVD convolution is a low rank approximation of the convolution</span>
<span class="sd">    layer. It can be seen as a depth wise convolution followed by a</span>
<span class="sd">    1x1 convolution.</span>

<span class="sd">    The flattened kernels for the i-th input map are expressed by</span>
<span class="sd">    their low rank approximation. The kernels for the i-th input</span>
<span class="sd">    :math:`{\\mathbf W_i}` are approximated with the singular value</span>
<span class="sd">    decomposition (SVD) and by selecting the :math:`{R}` dominant</span>
<span class="sd">    singular values and the corresponding singular vectors.</span>

<span class="sd">    .. math::</span>
<span class="sd">        {\\mathbf W_{:,i,:}} ~ {\\mathbf U_i} {\\mathbf V_i}.</span>

<span class="sd">    :math:`{\\mathbf U}` contains the weights of the depthwise</span>
<span class="sd">    convolution with multiplier :math:`{R}` and :math:`{\\mathbf V}`</span>
<span class="sd">    contains the weights of the 1x1 convolution.</span>

<span class="sd">    If `uv_init` is a numpy array, :math:`{\\mathbf U}` and</span>
<span class="sd">    :math:`{\\mathbf V}` are computed such that `uv_init` is</span>
<span class="sd">    approximated by :math:`{\\mathbf{UV}}`. If `uv_init` is `None` or</span>
<span class="sd">    an initializer, the product of :math:`{\\mathbf U}` and</span>
<span class="sd">    :math:`{\\mathbf V}` approximates the random initialization.</span>

<span class="sd">    If :math:`{\\mathbf U}` and :math:`{\\mathbf V}` exist in the</span>
<span class="sd">    context, they take precedence over `uv_init`.</span>

<span class="sd">    Suppose the kernel tensor of the convolution is of :math:`{O \\times I \\times K \\times K}` and</span>
<span class="sd">    the compression rate you want to specify is :math:`{CR}`, then you</span>
<span class="sd">    set :math:`{R}` as</span>

<span class="sd">    .. math::</span>

<span class="sd">        R = \\left\\lfloor \\frac{(1 - CR)OIK^2}{I(O + K^2)} \\right\\rfloor.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array.</span>

<span class="sd">        outmaps (int): Number of convolution kernels (which is equal</span>
<span class="sd">          to the number of output channels). For example, to apply</span>
<span class="sd">          convolution on an input with 16 types of filters, specify</span>
<span class="sd">          16.</span>

<span class="sd">        kernel (tuple): Convolution kernel size. For example,</span>
<span class="sd">          to apply convolution on an image with a 3 (height) by 5</span>
<span class="sd">          (width) two-dimensional kernel, specify (3, 5).</span>

<span class="sd">        r (int): Rank of the factorized layer.</span>

<span class="sd">        pad (tuple): Padding sizes (`int`) for dimensions.</span>

<span class="sd">        stride (tuple): Stride sizes (`int`) for dimensions.</span>

<span class="sd">        dilation (tuple): Dilation sizes (`int`) for dimensions.</span>

<span class="sd">        uv_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`):</span>
<span class="sd">          Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`. </span>

<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>

<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the</span>
<span class="sd">          sample dimensions.</span>

<span class="sd">        fix_parameters (bool): When set to `True`, the weights and</span>
<span class="sd">          biases will not be updated.</span>

<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>

<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: :math:`(B + 1)`-D array.</span>
<span class="sd">        (:math:`M_0 \\times \ldots \\times M_{B-1} \\times L`)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">r</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;svd_convolution: The rank must be larger than zero&quot;</span>

    <span class="k">if</span> <span class="n">uv_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">uv_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="n">outmaps</span><span class="p">,</span>
                                    <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">uv_init</span><span class="p">)</span> <span class="ow">is</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="c1"># TODO: Assert that size of uv_init is correct</span>
        <span class="c1"># uv is initialize with numpy array</span>
        <span class="n">uv</span> <span class="o">=</span> <span class="n">uv_init</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># uv is initialize from initializer</span>
        <span class="n">uv</span> <span class="o">=</span> <span class="n">uv_init</span><span class="p">((</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">])</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">))</span>

    <span class="c1"># flatten kernels</span>
    <span class="n">uv</span> <span class="o">=</span> <span class="n">uv</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">kernel</span><span class="p">)))</span>

    <span class="n">u</span> <span class="o">=</span> <span class="n">get_parameter</span><span class="p">(</span><span class="s1">&#39;U&#39;</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">get_parameter</span><span class="p">(</span><span class="s1">&#39;V&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">u</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>

        <span class="n">inmaps</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span>
        <span class="n">u_low_rank</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">inmaps</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span> <span class="n">r</span><span class="p">))</span>
        <span class="n">v_low_rank</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">inmaps</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">inmaps</span><span class="p">):</span>
            <span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">uv</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:])</span>
            <span class="n">u_</span><span class="p">,</span> <span class="n">s_</span><span class="p">,</span> <span class="n">v_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">u_low_rank</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u_</span><span class="p">[:,</span> <span class="p">:</span><span class="n">r</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s_</span><span class="p">[:</span><span class="n">r</span><span class="p">]))</span>
            <span class="n">v_low_rank</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">v_</span><span class="p">[:</span><span class="n">r</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1"># reshape U : (I,K*K,r) -&gt; (I*r,K,K) for depthwise conv</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">inmaps</span> <span class="o">*</span> <span class="n">r</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
                        <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">u</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">u_low_rank</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
               <span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">inmaps</span> <span class="o">*</span> <span class="n">r</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)))</span>

        <span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">set_parameter</span><span class="p">(</span><span class="s2">&quot;U&quot;</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span>

        <span class="c1"># reshape V :  (I,r,O) -&gt; (O,I*r,1,1) for 1X1 conv</span>
        <span class="n">kernel_one</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>  <span class="c1"># 1x1 for 2D convolution</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inmaps</span> <span class="o">*</span> <span class="n">r</span><span class="p">)</span> <span class="o">+</span> <span class="n">kernel_one</span><span class="p">,</span>
                        <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">v</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">v_low_rank</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
               <span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inmaps</span> <span class="o">*</span> <span class="n">r</span><span class="p">)</span> <span class="o">+</span> <span class="n">kernel_one</span><span class="p">))</span>

        <span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">set_parameter</span><span class="p">(</span><span class="s2">&quot;V&quot;</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">fix_parameters</span> <span class="o">==</span> <span class="n">u</span><span class="o">.</span><span class="n">need_grad</span><span class="p">:</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">u</span><span class="o">.</span><span class="n">get_unlinked_variable</span><span class="p">(</span><span class="n">need_grad</span><span class="o">=</span><span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fix_parameters</span> <span class="o">==</span> <span class="n">v</span><span class="o">.</span><span class="n">need_grad</span><span class="p">:</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">get_unlinked_variable</span><span class="p">(</span><span class="n">need_grad</span><span class="o">=</span><span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depthwise_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="n">base_axis</span><span class="p">,</span>
                                <span class="n">pad</span><span class="o">=</span><span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
                                <span class="n">multiplier</span><span class="o">=</span><span class="n">r</span><span class="p">)</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span></div>


<div class="viewcode-block" id="cpd3_convolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.cpd3_convolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;cpd3_conv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;I&#39;</span><span class="p">,</span>
     <span class="s1">&#39;Decomposed filter weights :math:`{</span><span class="se">\\</span><span class="s1">mathbf I}`&#39;</span><span class="p">,</span> <span class="s1">&#39;(r, inmaps, 1, ...)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;K&#39;</span><span class="p">,</span>
     <span class="s1">&#39;Decomposed filter weights :math:`{</span><span class="se">\\</span><span class="s1">mathbf K}`&#39;</span><span class="p">,</span> <span class="s1">&#39;(r, *kernel)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;O&#39;</span><span class="p">,</span>
     <span class="s1">&#39;Decomposed filter weights :math:`{</span><span class="se">\\</span><span class="s1">mathbf O}`&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, r, 1, ...)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">cpd3_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span>
                     <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                     <span class="n">oik_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                     <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                     <span class="n">max_iter</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">stopping_criterion</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;CP convolution is a low rank approximation of a convolution layer. A 3D tensor containing the parameter is built by collapsing the N-D kernels into 1D, then the tensor is decomposed into three matrices. The decomposed layer can be seen as linear combinations of the input feature maps to :math:`{R}` feature maps followed by a depthwise convolution and followed by linear combinations of the feature maps to compute the output feature maps.</span>

<span class="sd">    The CP decomposition allows to approximate the kernel tensor by :math:`{R}` rank-1 tensors of the form:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \\sum_{r=1}^{R} \\lambda_r {\\mathbf{o}^{(r)} \\otimes \\mathbf{i}^{(r)} \\otimes \\mathbf{k}^{(r)}},</span>

<span class="sd">    where :math:`{\\lambda}_r` is the normalization coefficient and :math:`{\\otimes}` is the outer product.</span>


<span class="sd">    If `oik_init` is a numpy array, U and V are computed so that uv_init can be approximates from UV</span>
<span class="sd">    If `oik_init` is None or an initializer, the product of U and V approximate the randomly initialized array</span>

<span class="sd">    If `O`, `I` and `K` exist in context, they are used to initialize the layer and oik_init is not used.</span>

<span class="sd">    Suppose the kernel tensor of the affine is of :math:`{I \\times O}` and</span>
<span class="sd">    the compression rate you want to specify is :math:`{CR}`, then you</span>
<span class="sd">    set :math:`{R}` as</span>

<span class="sd">    .. math::</span>

<span class="sd">        R = \\left\\lfloor \\frac{(1 - CR)OIK^2}{O + I + K^2} \\right\\rfloor.</span>

<span class="sd">    References:</span>
<span class="sd">        - Lebedev, Vadim, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and Victor Lempitsky,  &quot;Speeding-up convolutional neural networks using fine-tuned cp-decomposition.&quot;, arXiv preprint arXiv:1412.6553 (2014).</span>

<span class="sd">        - Marcella Astrid, Seung-Ik Lee, &quot;CP-decomposition with Tensor Power Method for Convolutional Neural Networks Compression&quot;, BigComp 2017.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array.</span>
<span class="sd">        outmaps (int): Number of convolution kernels (which is equal to the number of output channels). For example, to apply convolution on an input with 16 types of filters, specify 16.</span>
<span class="sd">        kernel (:obj:`tuple` of :obj:`int`): Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</span>
<span class="sd">        r (int): rank of the factorized layer</span>
<span class="sd">        pad (:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">        stride (:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">        dilation (:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">        oik_init (numpy array or :obj:`nnabla.initializer.BaseInitializer`): Initializer for weight. Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`. </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. It is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>
<span class="sd">        max_iter (int): Max iteration of the ALS.</span>
<span class="sd">        stopping_criterion (float): Threshold for stopping the ALS.</span>
<span class="sd">                If the value is negative, the convergence check is ignored;</span>
<span class="sd">                in other words, it may reduce the computation time.</span>
<span class="sd">        lambda_reg (float): regularization parameter for the ALS. Larger</span>
<span class="sd">                lambda_reg means larger regularization.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: :math:`(B + 1)`-D array. (:math:`M_0 \\times \ldots \\times M_{B-1} \\times L`)</span>


<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">oik_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">oik_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="n">outmaps</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">oik_init</span><span class="p">)</span> <span class="ow">is</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="c1"># TODO: Assert that size of uv_init is correct</span>
        <span class="c1"># uv is initialize with numpy array</span>
        <span class="n">oik</span> <span class="o">=</span> <span class="n">oik_init</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># uv is initialize from initializer</span>
        <span class="n">oik</span> <span class="o">=</span> <span class="n">oik_init</span><span class="p">((</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">])</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">))</span>

    <span class="c1"># flatten kernels</span>
    <span class="n">oik</span> <span class="o">=</span> <span class="n">oik</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">kernel</span><span class="p">)))</span>

    <span class="n">o</span> <span class="o">=</span> <span class="n">get_parameter</span><span class="p">(</span><span class="s1">&#39;O&#39;</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">get_parameter</span><span class="p">(</span><span class="s1">&#39;I&#39;</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">get_parameter</span><span class="p">(</span><span class="s1">&#39;K&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">o</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">i</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">k</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">r</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;cpd3_convolution: The rank must be larger than zero&quot;</span>
        <span class="kn">from</span> <span class="nn">nnabla.utils.factorization</span> <span class="kn">import</span> <span class="n">cpd</span>
        <span class="n">als</span> <span class="o">=</span> <span class="n">cpd</span><span class="o">.</span><span class="n">ALS</span><span class="p">()</span>
        <span class="n">U</span><span class="p">,</span> <span class="n">lmbda</span> <span class="o">=</span> <span class="n">als</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">oik</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">r</span><span class="p">,</span>
                             <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
                             <span class="n">stopping_criterion</span><span class="o">=</span><span class="n">stopping_criterion</span><span class="p">,</span>
                             <span class="n">lambda_reg</span><span class="o">=</span><span class="n">lambda_reg</span><span class="p">,</span>
                             <span class="n">dtype</span><span class="o">=</span><span class="n">oik</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                             <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

        <span class="n">o_</span> <span class="o">=</span> <span class="n">U</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">lmbda</span>
        <span class="n">i_</span> <span class="o">=</span> <span class="n">U</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">k_</span> <span class="o">=</span> <span class="n">U</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

        <span class="n">kernel_one</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>  <span class="c1"># 1x1 for 2D convolution</span>
        <span class="n">inmaps</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span>

        <span class="c1"># reshape I :  (I,r) -&gt; (r,I,1,1)</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">r</span><span class="p">,</span> <span class="n">inmaps</span><span class="p">)</span> <span class="o">+</span> <span class="n">kernel_one</span><span class="p">,</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">i</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">i_</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">r</span><span class="p">,</span> <span class="n">inmaps</span><span class="p">)</span> <span class="o">+</span> <span class="n">kernel_one</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">set_parameter</span><span class="p">(</span><span class="s2">&quot;I&quot;</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>

        <span class="c1"># reshape O :  (O,r) -&gt; (O,r,1,1)</span>
        <span class="n">o</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span> <span class="o">+</span> <span class="n">kernel_one</span><span class="p">,</span>
                        <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">o</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">o_</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span> <span class="o">+</span> <span class="n">kernel_one</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">set_parameter</span><span class="p">(</span><span class="s2">&quot;O&quot;</span><span class="p">,</span> <span class="n">o</span><span class="p">)</span>

        <span class="c1"># reshape K :  (K*K,r) -&gt; (r,K,K)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">r</span><span class="p">,)</span> <span class="o">+</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">k</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">k_</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">r</span><span class="p">,)</span> <span class="o">+</span> <span class="n">kernel</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">set_parameter</span><span class="p">(</span><span class="s2">&quot;K&quot;</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">fix_parameters</span> <span class="o">==</span> <span class="n">o</span><span class="o">.</span><span class="n">need_grad</span><span class="p">:</span>
        <span class="n">o</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="n">get_unlinked_variable</span><span class="p">(</span><span class="n">need_grad</span><span class="o">=</span><span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fix_parameters</span> <span class="o">==</span> <span class="n">i</span><span class="o">.</span><span class="n">need_grad</span><span class="p">:</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">i</span><span class="o">.</span><span class="n">get_unlinked_variable</span><span class="p">(</span><span class="n">need_grad</span><span class="o">=</span><span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fix_parameters</span> <span class="o">==</span> <span class="n">k</span><span class="o">.</span><span class="n">need_grad</span><span class="p">:</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">get_unlinked_variable</span><span class="p">(</span><span class="n">need_grad</span><span class="o">=</span><span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depthwise_convolution</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="n">base_axis</span><span class="p">,</span>
                                <span class="n">pad</span><span class="o">=</span><span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
                                <span class="n">multiplier</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span></div>


<div class="viewcode-block" id="binary_connect_convolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.binary_connect_convolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;bicon_conv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps, *kernel)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;Wb&#39;</span><span class="p">,</span> <span class="s1">&#39;Binarized filter weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps, *kernel)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">binary_connect_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span>
                               <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                               <span class="n">quantize_zero_to</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                               <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">wb_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                               <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                               <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Binary Connect Convolution, multiplier-less inner-product.</span>

<span class="sd">    Binary Connect Convolution is the convolution function,</span>
<span class="sd">    except the definition of the inner product is modified.</span>
<span class="sd">    The input-output relation of this function is as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_{n, a, b} = \sum_{m} \sum_{i} \sum_{j} sign(w_{n, m, i, j}) x_{m, a + i, b + j}.</span>

<span class="sd">    Therefore :math:`sign(w_i)` is either :math:`1` or :math:`-1` and the inner product</span>
<span class="sd">    simplifies to addition.</span>

<span class="sd">    This function should be used together with BatchNormalization.</span>

<span class="sd">    References:</span>

<span class="sd">        M. Courbariaux, Y. Bengio, and J.-P. David. &quot;BinaryConnect:</span>
<span class="sd">        Training Deep Neural Networks with binary weights during propagations.&quot;</span>
<span class="sd">        Advances in Neural Information Processing Systems. 2015.</span>

<span class="sd">    .. note::</span>

<span class="sd">        1) if you would like to share weights between some layers, please</span>
<span class="sd">        make sure to share the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the binarized weights (`binary_weight`)</span>

<span class="sd">        2) The weights and the binary weights become synced only after :func:`~nnabla._variable.Variable.forward` is called,</span>
<span class="sd">        and not after a call to :func:`~nnabla._variable.Variable.backward`.</span>
<span class="sd">        To access the parameters of the network, remember to call :func:`~nnabla._variable.Variable.forward` once before doing so, otherwise the</span>
<span class="sd">        float weights and the binary weights will not be in sync.</span>

<span class="sd">        3) Quantized values are stored as floating point number for `binary_weight`,</span>
<span class="sd">        since this function is only for simulation purposes.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array.</span>
<span class="sd">        outmaps (int): Number of convolution kernels (which is equal to the number of output channels). For example, to apply convolution on an input with 16 types of filters, specify 16.</span>
<span class="sd">        kernel (:obj:`tuple` of :obj:`int`): Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</span>
<span class="sd">        pad (:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">        stride (:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">        dilation (:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">        group (int): Number of groups of channels. This makes connections across channels sparser by grouping connections along map direction.</span>
<span class="sd">        quantize_zero_to (float): Input value at zero is quantized to this value.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`. </span>
<span class="sd">        wb_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for binary weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.   </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="n">outmaps</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">wb_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">wb_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="n">outmaps</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">])</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">wb</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;Wb&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">])</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
        <span class="n">wb_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_connect_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">wb</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">quantize_zero_to</span><span class="p">)</span></div>


<div class="viewcode-block" id="binary_weight_convolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.binary_weight_convolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;bwn_conv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps, *kernel)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;Wb&#39;</span><span class="p">,</span> <span class="s1">&#39;Binarized filter weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps, *kernel)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="s1">&#39;Scaling factor :math:`</span><span class="se">\\</span><span class="s1">alpha`&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">binary_weight_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span>
                              <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                              <span class="n">quantize_zero_to</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                              <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">wb_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Binary Weight Convolution, multiplier-less inner-product with a scale factor.</span>

<span class="sd">    Binary Weight Convolution is the convolution function, but the</span>
<span class="sd">    inner product in this function is the following,</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_{n, a, b} = \\frac{1}{\\|\\mathbf{w}_n\\|_{\\ell_1}} \sum_{m} \sum_{i} \sum_{j} sign(w_{n, m, i, j}) x_{m, a + i, b + j}.</span>


<span class="sd">    Therefore :math:`sign(w_{n, m, i, j})`  is either :math:`1` or :math:`-1` and the inner product</span>
<span class="sd">    simplifies to addition followed by scaling factor :math:`\\alpha = \\frac{1}{\\|\\mathbf{w}_n\\|_{\\ell_1}}`.</span>
<span class="sd">    The number of :math:`n` is the number of outmaps of the convolution</span>
<span class="sd">    function.</span>

<span class="sd">    References:</span>

<span class="sd">        Rastegari, Mohammad, et al. &quot;XNOR-Net: ImageNet Classification Using</span>
<span class="sd">        Binary Convolutional Neural Networks.&quot; arXiv preprint</span>
<span class="sd">        arXiv:1603.05279 (2016).</span>

<span class="sd">    .. note::</span>

<span class="sd">        1) if you would like to share weights between some layers, please</span>
<span class="sd">        make sure to share the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the binarized weights (`binary_weight`)</span>

<span class="sd">        2) The weights and the binary weights become synced only after :func:`~nnabla._variable.Variable.forward` is called,</span>
<span class="sd">        and not after a call to :func:`~nnabla._variable.Variable.backward`.</span>
<span class="sd">        To access the parameters of the network, remember to call :func:`~nnabla._variable.Variable.forward` once before doing so, otherwise the</span>
<span class="sd">        float weights and the binary weights will not be in sync.</span>

<span class="sd">        3) Quantized values are stored as floating point number for `binary_weight`,</span>
<span class="sd">        since this function is only for simulation purposes.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array.</span>
<span class="sd">        outmaps (int): Number of convolution kernels (which is equal to the number of output channels). For example, to apply convolution on an input with 16 types of filters, specify 16.</span>
<span class="sd">        kernel (:obj:`tuple` of :obj:`int`): Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</span>
<span class="sd">        pad (:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">        stride (:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">        dilation (:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">        group (int): Number of groups of channels. This makes connections across channels sparser by grouping connections along map direction.</span>
<span class="sd">        quantize_zero_to (float): Input value at zero is quantized to this value.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`. </span>
<span class="sd">        wb_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for binary weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.  </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="n">outmaps</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">wb_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">wb_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="n">outmaps</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">])</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">wb</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;Wb&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">])</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
        <span class="n">wb_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="p">),</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_weight_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">wb</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">quantize_zero_to</span><span class="p">)</span></div>


<div class="viewcode-block" id="inq_convolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.inq_convolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;inq_conv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps, *kernel)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;I&#39;</span><span class="p">,</span> <span class="s1">&#39;Binary indicator matrix of fixed weights&#39;</span><span class="p">,</span>
     <span class="s1">&#39;(outmaps, inmaps, *kernel)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">inq_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span>
                    <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">num_bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">inq_iterations</span><span class="o">=</span><span class="p">(),</span> <span class="n">selection_algorithm</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">,</span>
                    <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">i_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Incremental Network Quantization Convolution Layer</span>

<span class="sd">    During training, the weights are sequentially quantized to power-of-two</span>
<span class="sd">    values, which allows the training of a multiplierless network.</span>

<span class="sd">    Using `inq_iterations`, one can specify after how many forward passes</span>
<span class="sd">    half of the learnable weights are fixed and quantized to powers-of-two.</span>
<span class="sd">    After reaching the last value in `inq_iterations`, all weights are fixed.</span>

<span class="sd">    For more details, please refer to the reference.</span>

<span class="sd">    Reference:</span>
<span class="sd">    Zhou A, Yao A, Guo Y, Xu L, Chen Y. Incremental network quantization:</span>
<span class="sd">    Towards lossless CNNs with low-precision weights.</span>
<span class="sd">    &lt;https://arxiv.org/abs/1702.03044&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): Input N-D array with shape (:math:`M_0 \\times \ldots \\times M_{B-1} \\times D_B \\times \ldots \\times D_N`). Dimensions before and after base_axis are flattened as if it was a matrix.</span>
<span class="sd">        n_outmaps (int or :obj:`tuple` of :obj:`int`): Number of output neurons per data.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        num_bits (int): Number of bits per weight. Value has to be larger than 1 as one bit is already used to code the value &quot;0&quot;</span>
<span class="sd">        inq_iterations (tuple of int): Tuple of iteration numbers at which we fix half of the weights.</span>
<span class="sd">        selection_algorithm (str): Chooses algorithm that is used to decide which weights are fixed. (&quot;largest_abs&quot; ... fix weights with largest absolute value, &quot;random&quot; ... fix weights randomly)</span>
<span class="sd">        seed (int): Random seed for INQ algorithm</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for the weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`. </span>
<span class="sd">        i_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for the indicators (0 ... learnable, 1 ... fixed). By default, it is initialized with zeros.</span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for the bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weight and bias will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="n">outmaps</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">i_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">])</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;I&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">])</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
        <span class="n">i_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">inq_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">num_bits</span><span class="p">,</span> <span class="n">inq_iterations</span><span class="p">,</span> <span class="n">selection_algorithm</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>


<div class="viewcode-block" id="deformable_convolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.deformable_convolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;deformable_conv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps // group, *kernel)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">deformable_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">deformable_group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">channel_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                           <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                           <span class="n">apply_w</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">apply_b</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;2D Deformable Convolution with a bias term. If use mask, this function is Deformable Convolution v2.</span>

<span class="sd">    - Dai et al., Deformable Convolutional Networks. https://arxiv.org/abs/1703.06211</span>
<span class="sd">    - Zhu et al., Deformable ConvNets v2: More Deformable, Better Results. https://arxiv.org/abs/1811.11168</span>


<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array.</span>
<span class="sd">        outmaps (int): Number of convolution kernels (which is equal to the number of output channels). For example, to apply convolution on an input with 16 types of filters, specify 16.</span>
<span class="sd">        kernel (:obj:`tuple` of :obj:`int`): Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</span>
<span class="sd">        offset (~nnabla.Variable): Offsets for deformable convolutions. Shape is fixed to :math:`(N, deformable{\_}group \\times 2 \\times Kh \\times Kw, H, W)`. Offsets must be calculated externally through a separate convolution layer.</span>
<span class="sd">        mask (~nnabla.Variable): Normalized mask for deformable convolutions v2. Shape is fixed to :math:`(N, deformable{\_}group \\times Kh \\times Kw, H, W)`. Masks must be calculated externally together with the offsets through a separate convolution layer.</span>
<span class="sd">        pad (:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">        stride (:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">        dilation (:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">        group (int): Number of groups of channels. This makes connections across channels more sparse by grouping connections along map direction.</span>
<span class="sd">        deformable_group (int): Number of deformable groups of channels. This makes connections across channels more sparse by grouping connections along map direction.</span>
<span class="sd">        channel_last (bool): If True, the last dimension is considered as channel dimension, a.k.a. NHWC order.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.</span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>
<span class="sd">        apply_w (function): Lambda, function, or callable object applied to the weights.</span>
<span class="sd">        apply_b (function): Lambda, function, or callable object applied to the bias.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: N-D array. See :obj:`~nnabla.functions.convolution` for the output shape.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">channel_last</span><span class="p">:</span>
        <span class="n">channels</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">filter_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">channels</span> <span class="o">//</span> <span class="n">group</span><span class="p">,)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">channels</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span>
        <span class="n">filter_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">channels</span> <span class="o">//</span> <span class="n">group</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,)</span> <span class="o">+</span> <span class="n">filter_shape</span><span class="p">,</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">apply_w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">apply_w</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">apply_b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">apply_b</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">deformable_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">deformable_group</span><span class="p">,</span> <span class="n">channel_last</span><span class="p">)</span></div>


<div class="viewcode-block" id="depthwise_convolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.depthwise_convolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;depthwise_conv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps * multiplier, *kernel)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps * multiplier,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">depthwise_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">multiplier</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    N-D Depthwise Convolution with a bias term.</span>

<span class="sd">    Reference:</span>

<span class="sd">    - F. Chollet: Chollet, Francois. &quot;Xception: Deep Learning with Depthwise Separable Convolutions. https://arxiv.org/abs/1610.02357</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array.</span>
<span class="sd">        kernel (:obj:`tuple` of :obj:`int`): Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</span>
<span class="sd">        pad (:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">        stride (:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">        dilation (:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">        multiplier (:obj:`int`): Number of output feature maps per input feature map.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight.  By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`. </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: N-D array. See :obj:`~nnabla.functions.depthwise_convolution` for the output shape.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span>
                <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">multiplier</span><span class="p">,</span>
                <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span>
            <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">multiplier</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">multiplier</span><span class="p">,),</span>
            <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">depthwise_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
                                   <span class="n">multiplier</span><span class="p">)</span></div>


<div class="viewcode-block" id="deconvolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.deconvolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;deconv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps // group, *kernel)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">deconvolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">channel_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">output_padding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                  <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">apply_w</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">apply_b</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Deconvolution layer.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array.</span>
<span class="sd">        outmaps (int): Number of deconvolution kernels (which is equal to the number of output channels). For example, to apply deconvolution on an input with 16 types of filters, specify 16.</span>
<span class="sd">        kernel (:obj:`tuple` of :obj:`int`): Convolution kernel size. For example, to apply deconvolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</span>
<span class="sd">        pad (:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">        stride (:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">        dilation (:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">        group (int): Number of groups of channels. This makes connections across channels sparser by grouping connections along map direction.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.  </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>
<span class="sd">        apply_w (function): Lambda, function, or callable object applied to the weights.</span>
<span class="sd">        apply_b (function): Lambda, function, or callable object applied to the bias.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: N-D array. See :obj:`~nnabla.functions.deconvolution` for the output shape.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">channel_last</span><span class="p">:</span>
        <span class="n">channels</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">weights_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">channels</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">outmaps</span> <span class="o">//</span> <span class="n">group</span><span class="p">,)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">channels</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span>
        <span class="n">weights_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">outmaps</span> <span class="o">//</span> <span class="n">group</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="n">weights_shape</span><span class="p">,</span> <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">apply_w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">apply_w</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">apply_b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">apply_b</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">deconvolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span>
                           <span class="n">channel_last</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">)</span></div>


<div class="viewcode-block" id="depthwise_deconvolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.depthwise_deconvolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;depthwise_deconv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps,) + kernel&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps / divisor,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">depthwise_deconvolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                            <span class="n">divisor</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                            <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Depthwise deconvolution computes the transposed depthwise</span>
<span class="sd">    convolution for one-dimensional and two-dimensional input data.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array.</span>
<span class="sd">        kernel (:obj:`tuple` of :obj:`int`): Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</span>
<span class="sd">        pad (:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">        stride (:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">        dilation (:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">        divisor (:obj:`int`): Number of input feature maps per output feature map.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.  </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: N-D array. See :obj:`~nnabla.functions.depthwise_deconvolution` for the output shape.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span>
                <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span>
                <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span>
            <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span> <span class="o">//</span> <span class="n">divisor</span><span class="p">,),</span>
            <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">depthwise_deconvolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span>
                                     <span class="n">dilation</span><span class="p">,</span> <span class="n">divisor</span><span class="p">)</span></div>


<div class="viewcode-block" id="rnn"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.rnn">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;rnn&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;weight_l0&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights at 0-th layer&#39;</span><span class="p">,</span> <span class="s1">&#39;(D, H, I + H)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights at 1-st layer and above&#39;</span><span class="p">,</span> <span class="s1">&#39;(L-1, D, H, DH + H)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="s1">&#39;Biases&#39;</span><span class="p">,</span> <span class="s1">&#39;(L, D, H)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">rnn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w0_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;N-Step RNN (recurrent neural networks).</span>

<span class="sd">    N-Step RNN function implements Elman RNN with nonlinearity to input sequence.</span>
<span class="sd">    N-Step RNN function is defined as following:</span>

<span class="sd">    .. math::</span>
<span class="sd">        h_t = \\tanh(w_{ih}x_t+b_{ih}+w_{hh}h_{(t-1)}).</span>

<span class="sd">    We use the following notations to describe the inputs and outputs below.</span>
<span class="sd">    :math:`T`: sequcne length, :math:`B`: batch size, :math:`I`: input size, :math:`L`: number of layers, :math:`D`: number of directions, can be either 1 or 2, :math:`H`: hidden size.</span>

<span class="sd">    References:</span>

<span class="sd">        Jeffrey L. Elman. &quot;Finding Structure in Time.&quot;</span>
<span class="sd">        Cognitive Science. 1990.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (~nnabla.Variable): Input N-D array with shape :math:`(T, B, I)`.</span>
<span class="sd">        h (~nnabla.Variable): Input N-D array with shape :math:`(L, D, B, H)`.</span>
<span class="sd">        w0_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`, optional): Initializer for weight at the first layer. Shape is :math:`(D, H, I + H)`.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`, optional): Initializer for weights at the second layer and up. Shape is :math:`(L-1, D, H, D*H + H)`.</span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`, optional): Initializer for bias. Shape is :math:`(L, D, H)`.</span>
<span class="sd">        num_layers (int, optional): Number of layers in the network. If set to 1, only the weights for the first layer will be invoked. Default is 1.</span>
<span class="sd">        nonlinearity (str, optional): Type of nonlinearity applied to input sequcne. Must be either tanh or relu. Default is tanh.</span>
<span class="sd">        dropout (float, optional): Dropout ratio applied to parameters. Default is 0.0.</span>
<span class="sd">        bidirectional (bool, optional): If True, bidirectional computation will be performed in each layer. Default is False.</span>
<span class="sd">        training (bool, optional): Backpropagation will be performed only when it is true. Default is True.</span>
<span class="sd">        with_bias (bool, optional): Specify whether to include the bias term.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output :math:`y` with shape :math:`(T, B, D * H)`</span>
<span class="sd">        ~nnabla.Variable: Output :math:`h_n` with shape :math:`(L, D, B, H)`</span>

<span class="sd">    Example:</span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">            x = nn.Variable((seq_len, batch_size, input_size))</span>
<span class="sd">            h = nn.Variable((num_layers, num_directions, batch_size, hidden_size))</span>
<span class="sd">            y, hn = PF.rnn(x, h)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">input_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">num_directions</span> <span class="o">=</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">bidirectional</span> <span class="k">else</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="n">w0_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w0_init_ih</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span> <span class="n">rng</span><span class="p">)</span>
        <span class="n">w0_init_ih</span> <span class="o">=</span> <span class="n">w0_init_ih</span><span class="p">((</span><span class="n">num_directions</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">))</span>
        <span class="n">w0_init_hh</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span> <span class="n">rng</span><span class="p">)</span>
        <span class="n">w0_init_hh</span> <span class="o">=</span> <span class="n">w0_init_hh</span><span class="p">((</span><span class="n">num_directions</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>
        <span class="n">w0_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">w0_init_ih</span><span class="p">,</span> <span class="n">w0_init_hh</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_init_ih</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span><span class="n">calc_uniform_lim_glorot</span><span class="p">(</span>
            <span class="n">num_directions</span><span class="o">*</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span> <span class="n">rng</span><span class="p">)</span>
        <span class="n">w_init_ih</span> <span class="o">=</span> <span class="n">w_init_ih</span><span class="p">(</span>
            <span class="p">(</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_directions</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_directions</span><span class="o">*</span><span class="n">hidden_size</span><span class="p">))</span>
        <span class="n">w_init_hh</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span> <span class="n">rng</span><span class="p">)</span>
        <span class="n">w_init_hh</span> <span class="o">=</span> <span class="n">w_init_hh</span><span class="p">(</span>
            <span class="p">(</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_directions</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">w_init_ih</span><span class="p">,</span> <span class="n">w_init_hh</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">w0_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_directions</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">)</span>
    <span class="n">w0</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;weight_l0&quot;</span><span class="p">,</span> <span class="n">w0_shape</span><span class="p">,</span>
        <span class="n">w0_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">num_layers</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">w_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_directions</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span>
                   <span class="n">num_directions</span> <span class="o">*</span> <span class="n">hidden_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">,</span>
            <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">n_outmaps</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">num_directions</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">weight_l0</span><span class="o">=</span><span class="n">w0</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">w</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="n">nonlinearity</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span></div>


<div class="viewcode-block" id="lstm"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.lstm">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;weight_l0&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights at 0-th layer&#39;</span><span class="p">,</span> <span class="s1">&#39;(D, 4, H, I + H)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights at 1-st layer and above&#39;</span><span class="p">,</span>
     <span class="s1">&#39;(L-1, D, 4, H, DH + H)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="s1">&#39;Biases&#39;</span><span class="p">,</span> <span class="s1">&#39;(L, D, 4, H)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">w0_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;LSTM (long short-term memory).</span>

<span class="sd">    Long Short-Term Memory, or LSTM, is a building block for recurrent neural networks (RNN) layers.</span>
<span class="sd">    LSTM unit consists of a cell and input, output, forget gates whose functions are defined as following:</span>

<span class="sd">    .. math::</span>
<span class="sd">        f_t&amp;&amp;=\\sigma(W_fx_t+U_fh_{t-1}+b_f) \\\\</span>
<span class="sd">        i_t&amp;&amp;=\\sigma(W_ix_t+U_ih_{t-1}+b_i) \\\\</span>
<span class="sd">        o_t&amp;&amp;=\\sigma(W_ox_t+U_oh_{t-1}+b_o) \\\\</span>
<span class="sd">        c_t&amp;&amp;=f_t\\odot c_{t-1}+i_t\\odot\\tanh(W_cx_t+U_ch_{t-1}+b_c) \\\\</span>
<span class="sd">        h_t&amp;&amp;=o_t\\odot\\tanh(c_t).</span>

<span class="sd">    We use the following notations to describe the inputs and outputs below.</span>
<span class="sd">    :math:`T`: sequcne length, :math:`B`: batch size, :math:`I`: input size, :math:`L`: number of layers, :math:`D`: number of directions, can be either 1 or 2, :math:`H`: hidden size.</span>

<span class="sd">    References:</span>

<span class="sd">        S. Hochreiter, and J. Schmidhuber. &quot;Long Short-Term Memory.&quot;</span>
<span class="sd">        Neural Computation. 1997.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (~nnabla.Variable): Input N-D array with shape :math:`(T, B, I)`.</span>
<span class="sd">        h (~nnabla.Variable): Input N-D array with shape :math:`(L, D, B, H)`.</span>
<span class="sd">        c (~nnabla.Variable): Input N-D array with shape :math:`(L, D, B, H)` .</span>
<span class="sd">        w0_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`, optional): Initializer for weight at the first layer. Shape is :math:`(D, 4, H, I + H)`.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`, optional): Initializer for weights at the second layer and up. Shape is :math:`(L-1, D, 4, H, D * H + H)`.</span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`, optional): Initializer for bias. Shape is :math:`(L, D, 4, H)`.</span>
<span class="sd">        num_layers (int, optional): Number of layers in the network. If set to 1, only the weights for the first layer will be invoked. Default is 1.</span>
<span class="sd">        dropout (float, optional): Dropout ratio applied to parameters. Default is 0.0.</span>
<span class="sd">        bidirectional (bool, optional): If True, bidirectional computation will be performed in each layer. Default is False.</span>
<span class="sd">        training (bool, optional): Backpropagation will be performed only when it is true. Default is True.</span>
<span class="sd">        with_bias (bool, optional): Specify whether to include the bias term.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output :math:`y` with shape :math:`(T, B, D * H)`</span>
<span class="sd">        ~nnabla.Variable: Output :math:`h_n` with shape :math:`(L, D, B, H)`</span>
<span class="sd">        ~nnabla.Variable: Output :math:`c_n` with shape :math:`(L, D, B, H)`</span>

<span class="sd">    Example:</span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">            x = nn.Variable((seq_len, batch_size, input_size))</span>
<span class="sd">            h = nn.Variable((num_layers, num_directions, batch_size, hidden_size))</span>
<span class="sd">            c = nn.Variable((num_layers, num_directions, batch_size, hidden_size))</span>
<span class="sd">            y, hn, cn = PF.lstm(x, h, c)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">w0_init</span><span class="p">)</span> <span class="o">==</span> <span class="nb">int</span><span class="p">:</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;Arguments passed seem to be for previous LSTM function, which has been renamed to lstm_cell.&quot;</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span>

    <span class="n">input_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">num_directions</span> <span class="o">=</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">bidirectional</span> <span class="k">else</span> <span class="mi">1</span>

    <span class="n">w0</span> <span class="o">=</span> <span class="n">get_parameter</span><span class="p">(</span><span class="s1">&#39;weight_l0&#39;</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">w0</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">w0_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">w0_ih</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
                <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span> <span class="n">rng</span><span class="p">)</span>
            <span class="n">w0_ih</span> <span class="o">=</span> <span class="n">w0_ih</span><span class="p">((</span><span class="n">num_directions</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">))</span>
            <span class="n">w0_hh</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
                <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span> <span class="n">rng</span><span class="p">)</span>
            <span class="n">w0_hh</span> <span class="o">=</span> <span class="n">w0_hh</span><span class="p">((</span><span class="n">num_directions</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>
            <span class="n">w0_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">w0_ih</span><span class="p">,</span> <span class="n">w0_hh</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">w0_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_directions</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">w0</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;weight_l0&quot;</span><span class="p">,</span> <span class="n">w0_shape</span><span class="p">,</span>
            <span class="n">w0_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">num_layers</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">w</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">w_ih</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span><span class="n">calc_uniform_lim_glorot</span><span class="p">(</span>
                <span class="n">num_directions</span><span class="o">*</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span> <span class="n">rng</span><span class="p">)</span>
            <span class="n">w_ih</span> <span class="o">=</span> <span class="n">w_ih</span><span class="p">(</span>
                <span class="p">(</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_directions</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_directions</span><span class="o">*</span><span class="n">hidden_size</span><span class="p">))</span>
            <span class="n">w_hh</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
                <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span> <span class="n">rng</span><span class="p">)</span>
            <span class="n">w_hh</span> <span class="o">=</span> <span class="n">w_hh</span><span class="p">(</span>
                <span class="p">(</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_directions</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>
            <span class="n">w_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">w_ih</span><span class="p">,</span> <span class="n">w_hh</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
        <span class="n">w_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_directions</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span>
                   <span class="n">num_directions</span> <span class="o">*</span> <span class="n">hidden_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">,</span>
            <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
        <span class="n">n_outmaps</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">num_directions</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">w0</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">num_directions</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">input_size</span><span class="o">+</span><span class="n">hidden_size</span><span class="p">):</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;Parameters seem to have been saved prior to bug fix. It will be converted into the correct shape, but we highly recommend training again to obtain the correct parameters, as we will cease to support these parametetrs in future. We apologize for the inconveinences.&quot;</span><span class="p">)</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">w0</span><span class="o">.</span><span class="n">d</span>
        <span class="n">w0</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="o">.</span><span class="n">from_numpy_array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">tmp</span><span class="p">,</span> <span class="p">(</span><span class="n">num_directions</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">)),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">set_parameter</span><span class="p">(</span><span class="s1">&#39;weight_l0&#39;</span><span class="p">,</span> <span class="n">w0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">num_layers</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">num_layers</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_directions</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_directions</span><span class="o">*</span><span class="n">hidden_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">d</span>
        <span class="n">ww</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="o">.</span><span class="n">from_numpy_array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">tmp</span><span class="p">,</span> <span class="p">(</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_directions</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_directions</span><span class="o">*</span><span class="n">hidden_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">)),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">set_parameter</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

    <span class="n">w0</span> <span class="o">=</span> <span class="n">w0</span><span class="o">.</span><span class="n">get_unlinked_variable</span><span class="p">(</span><span class="n">need_grad</span><span class="o">=</span><span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">num_layers</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">get_unlinked_variable</span><span class="p">(</span><span class="n">need_grad</span><span class="o">=</span><span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">get_unlinked_variable</span><span class="p">(</span><span class="n">need_grad</span><span class="o">=</span><span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">weight_l0</span><span class="o">=</span><span class="n">w0</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">w</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span></div>


<div class="viewcode-block" id="gru"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.gru">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;gru&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;weight_l0&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights at 0-th layer&#39;</span><span class="p">,</span> <span class="s1">&#39;(D, 3, H, I + H)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights at 1-st layer and above&#39;</span><span class="p">,</span>
     <span class="s1">&#39;(L-1, D, 3, H, DH + H)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="s1">&#39;Biases&#39;</span><span class="p">,</span> <span class="s1">&#39;(L, D, 4, H)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">gru</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w0_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;GRU (gated recurrent units).</span>

<span class="sd">    GRU is defined as following:</span>

<span class="sd">    .. math::</span>
<span class="sd">        r_t&amp;&amp;=\\sigma(W_rx_t+U_rh_{t-1}+b_r) \\\\</span>
<span class="sd">        z_t&amp;&amp;=\\sigma(W_zx_t+U_zh_{t-1}+b_z) \\\\</span>
<span class="sd">        n_t&amp;&amp;=\\tanh(W_nx_t+b_{in}+r_n \odot (U_nh_{t-1}+b_{hn})) \\\\</span>
<span class="sd">        h_t&amp;&amp;=(1-z_t) \odot n_t+z_t \odot h_{t-1}.</span>

<span class="sd">    We use the following notations to describe the inputs and outputs below.</span>
<span class="sd">    :math:`T`: sequcne length, :math:`B`: batch size, :math:`I`: input size, :math:`L`: number of layers, :math:`D`: number of directions, can be either 1 or 2, :math:`H`: hidden size.</span>

<span class="sd">    References:</span>

<span class="sd">        K. Cho et al. &quot;Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation.&quot;</span>
<span class="sd">        Empirical Methods in Natural Language Processing. 2014.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (~nnabla.Variable): Input N-D array with shape :math:`(T, B, I)`.</span>
<span class="sd">        h (~nnabla.Variable): Input N-D array with shape :math:`(L, D, B, H)`.</span>
<span class="sd">        w0_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`, optional): Initializer for weight at the first layer. Shape is :math:`(D, 3, H, I + H)`.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`, optional): Initializer for weights at the second layer and up. Shape is :math:`(L-1, D, 3, H, D * H + H)`.</span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`, optional): Initializer for bias. Shape is :math:`(L, D, 4, H)`.</span>
<span class="sd">        num_layers (int, optional): Number of layers in the network. If set to 1, only the weights for the first layer will be invoked. Default is 1.</span>
<span class="sd">        dropout (float, optional): Dropout ratio applied to parameters. Default is 0.0.</span>
<span class="sd">        bidirectional (bool, optional): If True, bidirectional computation will be performed in each layer. Default is False.</span>
<span class="sd">        training (bool, optional): Backpropagation will be performed only when it is true. Default is True.</span>
<span class="sd">        with_bias (bool, optional): Specify whether to include the bias term.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output :math:`y` with shape :math:`(T, B, D * H)`</span>
<span class="sd">        ~nnabla.Variable: Output :math:`h_n` with shape :math:`(L, D, B, H)`</span>

<span class="sd">    Example:</span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">            x = nn.Variable((seq_len, batch_size, input_size))</span>
<span class="sd">            h = nn.Variable((num_layers, num_directions, batch_size, hidden_size))</span>
<span class="sd">            y, hn = PF.gru(x, h)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">input_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">num_directions</span> <span class="o">=</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">bidirectional</span> <span class="k">else</span> <span class="mi">1</span>

    <span class="n">w0</span> <span class="o">=</span> <span class="n">get_parameter</span><span class="p">(</span><span class="s1">&#39;weight_l0&#39;</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">w0</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">w0_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">w0_ih</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
                <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span> <span class="n">rng</span><span class="p">)</span>
            <span class="n">w0_ih</span> <span class="o">=</span> <span class="n">w0_ih</span><span class="p">((</span><span class="n">num_directions</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">))</span>
            <span class="n">w0_hh</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
                <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span> <span class="n">rng</span><span class="p">)</span>
            <span class="n">w0_hh</span> <span class="o">=</span> <span class="n">w0_hh</span><span class="p">((</span><span class="n">num_directions</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>
            <span class="n">w0_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">w0_ih</span><span class="p">,</span> <span class="n">w0_hh</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">w0_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_directions</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">w0</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;weight_l0&quot;</span><span class="p">,</span> <span class="n">w0_shape</span><span class="p">,</span>
            <span class="n">w0_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">num_layers</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">w</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">w_ih</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span><span class="n">calc_uniform_lim_glorot</span><span class="p">(</span>
                <span class="n">num_directions</span><span class="o">*</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span> <span class="n">rng</span><span class="p">)</span>
            <span class="n">w_ih</span> <span class="o">=</span> <span class="n">w_ih</span><span class="p">(</span>
                <span class="p">(</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_directions</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_directions</span><span class="o">*</span><span class="n">hidden_size</span><span class="p">))</span>
            <span class="n">w_hh</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
                <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span> <span class="n">rng</span><span class="p">)</span>
            <span class="n">w_hh</span> <span class="o">=</span> <span class="n">w_hh</span><span class="p">(</span>
                <span class="p">(</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_directions</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>
            <span class="n">w_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">w_ih</span><span class="p">,</span> <span class="n">w_hh</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
        <span class="n">w_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_directions</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span>
                   <span class="n">num_directions</span> <span class="o">*</span> <span class="n">hidden_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">,</span>
            <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
        <span class="n">n_outmaps</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">num_directions</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">w0</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">num_directions</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">input_size</span><span class="o">+</span><span class="n">hidden_size</span><span class="p">):</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;Parameters seem to have been saved prior to bug fix. It will be converted into the correct shape, but we highly recommend training again to obtain the correct parameters, as we will cease to support these parametetrs in future. We apologize for the inconveinences.&quot;</span><span class="p">)</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">w0</span><span class="o">.</span><span class="n">d</span>
        <span class="n">w0</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="o">.</span><span class="n">from_numpy_array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">tmp</span><span class="p">,</span> <span class="p">(</span><span class="n">num_directions</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">)),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">set_parameter</span><span class="p">(</span><span class="s1">&#39;weight_l0&#39;</span><span class="p">,</span> <span class="n">w0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">num_layers</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">num_layers</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_directions</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_directions</span><span class="o">*</span><span class="n">hidden_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">d</span>
        <span class="n">ww</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="o">.</span><span class="n">from_numpy_array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">tmp</span><span class="p">,</span> <span class="p">(</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_directions</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_directions</span><span class="o">*</span><span class="n">hidden_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">)),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">set_parameter</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
    <span class="n">w0</span> <span class="o">=</span> <span class="n">w0</span><span class="o">.</span><span class="n">get_unlinked_variable</span><span class="p">(</span><span class="n">need_grad</span><span class="o">=</span><span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">num_layers</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">get_unlinked_variable</span><span class="p">(</span><span class="n">need_grad</span><span class="o">=</span><span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">get_unlinked_variable</span><span class="p">(</span><span class="n">need_grad</span><span class="o">=</span><span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">weight_l0</span><span class="o">=</span><span class="n">w0</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">w</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span></div>


<div class="viewcode-block" id="fused_batch_normalization"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.fused_batch_normalization">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;bn&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="s1">&#39;Trainable bias :math:`</span><span class="se">\\</span><span class="s1">beta`&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;see above&gt;&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;gamma&#39;</span><span class="p">,</span> <span class="s1">&#39;Trainable scaling factor :math:`</span><span class="se">\\</span><span class="s1">gamma`&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;see above&gt;&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;Moving average of batch mean&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;see above&gt;&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;Moving average of batch variance&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;see above&gt;&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">fused_batch_normalization</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
                              <span class="n">batch_stat</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">output_stat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                              <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">param_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">no_scale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">no_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Batch normalization layer fused with the following add2 operation of a</span>
<span class="sd">    residual input and an nonlinear activation.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array of input.</span>
<span class="sd">        z (~nnabla.Variable, optional):</span>
<span class="sd">            A residual input. By specifying None, the activation function will</span>
<span class="sd">            follow immediately after BN operation.</span>
<span class="sd">        axes (:obj:`tuple` of :obj:`int`):</span>
<span class="sd">            Mean and variance for each element in ``axes`` are calculated using</span>
<span class="sd">            elements on the rest axes. For example, if an input is 4 dimensions,</span>
<span class="sd">            and ``axes`` is ``[1]``,  batch mean is calculated as</span>
<span class="sd">            ``np.mean(inp.d, axis=(0, 2, 3), keepdims=True)``</span>
<span class="sd">            (using numpy expression as an example).</span>
<span class="sd">        decay_rate (float): Decay rate of running mean and variance.</span>
<span class="sd">        eps (float): Tiny value to avoid zero division by std.</span>
<span class="sd">        batch_stat (bool): Use mini-batch statistics rather than running ones.</span>
<span class="sd">        nonlinearity (string): Activation function. The default is &#39;relu&#39;.</span>
<span class="sd">        output_stat (bool): Output batch mean and variance.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the beta and gamma will not be updated.</span>
<span class="sd">        no_scale (bool): If `True`, the scale term is omitted.</span>
<span class="sd">        no_bias (bool): If `True`, the bias term is omitted.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: N-D array.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">.normalization_functions</span> <span class="kn">import</span> <span class="n">_init_beta_gamma</span>

    <span class="n">shape_stat</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">axes</span><span class="p">)):</span>
        <span class="n">shape_stat</span><span class="p">[</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>

    <span class="k">if</span> <span class="n">param_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">param_init</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="n">_init_beta_gamma</span><span class="p">(</span>
        <span class="n">shape_stat</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="p">,</span> <span class="n">param_init</span><span class="p">,</span> <span class="n">no_bias</span><span class="p">,</span> <span class="n">no_scale</span><span class="p">)</span>

    <span class="n">mean_init</span> <span class="o">=</span> <span class="n">param_init</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">var_init</span> <span class="o">=</span> <span class="n">param_init</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">shape_stat</span><span class="p">,</span> <span class="n">mean_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;var&quot;</span><span class="p">,</span> <span class="n">shape_stat</span><span class="p">,</span> <span class="n">var_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">fused_batch_normalization</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span>
                                       <span class="n">decay_rate</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">batch_stat</span><span class="p">,</span>
                                       <span class="n">nonlinearity</span><span class="p">,</span> <span class="n">output_stat</span><span class="p">)</span></div>


<div class="viewcode-block" id="batch_normalization"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.batch_normalization">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;bn&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="s1">&#39;Trainable bias :math:`</span><span class="se">\\</span><span class="s1">beta`&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;see above&gt;&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;gamma&#39;</span><span class="p">,</span> <span class="s1">&#39;Trainable scaling factor :math:`</span><span class="se">\\</span><span class="s1">gamma`&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;see above&gt;&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;Moving average of batch mean&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;see above&gt;&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;Moving average of batch variance&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;see above&gt;&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">batch_normalization</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
                        <span class="n">batch_stat</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_stat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">param_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">no_scale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">no_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Batch normalization layer.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \\begin{array}{lcl}</span>
<span class="sd">        \\mu &amp;=&amp; \\frac{1}{M} \\sum x_i\\\\</span>
<span class="sd">        \\sigma^2 &amp;=&amp; \\frac{1}{M} \\sum \\left(x_i - \\mu\\right)^2\\\\</span>
<span class="sd">        \\hat{x}_i &amp;=&amp; \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon }}\\\\</span>
<span class="sd">        y_i &amp;= &amp; \\hat{x}_i \\gamma + \\beta.</span>
<span class="sd">        \\end{array}</span>

<span class="sd">    where :math:`x_i, y_i` are the inputs.</span>
<span class="sd">    In testing, the mean and variance computed by moving average calculated during training are used.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array of input.</span>
<span class="sd">        axes (:obj:`tuple` of :obj:`int`):</span>
<span class="sd">            Mean and variance for each element in ``axes`` are calculated using</span>
<span class="sd">            elements on the rest axes. For example, if an input is 4 dimensions,</span>
<span class="sd">            and ``axes`` is ``[1]``,  batch mean is calculated as</span>
<span class="sd">            ``np.mean(inp.d, axis=(0, 2, 3), keepdims=True)``</span>
<span class="sd">            (using numpy expression as an example).</span>
<span class="sd">        decay_rate (float): Decay rate of running mean and variance.</span>
<span class="sd">        eps (float): Tiny value to avoid zero division by std.</span>
<span class="sd">        batch_stat (bool): Use mini-batch statistics rather than running ones.</span>
<span class="sd">        output_stat (bool): Output batch mean and variance.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the beta and gamma will not be updated.</span>
<span class="sd">        param_init (dict):</span>
<span class="sd">            Parameter initializers can be set with a dict. A key of the dict must</span>
<span class="sd">            be ``&#39;beta&#39;``, ``&#39;gamma&#39;``, ``&#39;mean&#39;`` or ``&#39;var&#39;``.</span>
<span class="sd">            A value of the dict must be an :obj:`~nnabla.initializer.Initializer`</span>
<span class="sd">            or a :obj:`numpy.ndarray`.</span>
<span class="sd">            E.g. ``{&#39;beta&#39;: ConstantInitializer(0), &#39;gamma&#39;: np.ones(gamma_shape) * 2}``.</span>
<span class="sd">        no_scale (bool): If `True`, the scale term is omitted.</span>
<span class="sd">        no_bias (bool): If `True`, the bias term is omitted.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: N-D array.</span>

<span class="sd">    References:</span>

<span class="sd">        - Ioffe and Szegedy, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. https://arxiv.org/abs/1502.03167</span>

<span class="sd">    The shape of parameters has the same number of dimensions with the input</span>
<span class="sd">    data, and the shapes in ``axes`` has the same dimensions with the input, while the rest has ``1``.</span>
<span class="sd">    If an input is 4-dim and ``axes=[1]``, the parameter shape will be</span>
<span class="sd">    ``param_shape  = np.mean(inp.d, axis=(0, 2, 3), keepdims=True).shape``</span>
<span class="sd">    (using numpy expression as an example).</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">.normalization_functions</span> <span class="kn">import</span> <span class="n">_init_beta_gamma</span>

    <span class="n">shape_stat</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">axes</span><span class="p">)):</span>
        <span class="n">shape_stat</span><span class="p">[</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>

    <span class="k">if</span> <span class="n">param_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">param_init</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="n">_init_beta_gamma</span><span class="p">(</span>
        <span class="n">shape_stat</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="p">,</span> <span class="n">param_init</span><span class="p">,</span> <span class="n">no_bias</span><span class="p">,</span> <span class="n">no_scale</span><span class="p">)</span>

    <span class="n">mean_init</span> <span class="o">=</span> <span class="n">param_init</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">var_init</span> <span class="o">=</span> <span class="n">param_init</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">shape_stat</span><span class="p">,</span> <span class="n">mean_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;var&quot;</span><span class="p">,</span> <span class="n">shape_stat</span><span class="p">,</span> <span class="n">var_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span>
                                 <span class="n">decay_rate</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">batch_stat</span><span class="p">,</span> <span class="n">output_stat</span><span class="p">)</span></div>


<div class="viewcode-block" id="sync_batch_normalization"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.sync_batch_normalization">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;bn&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="s1">&#39;Trainable bias :math:`</span><span class="se">\\</span><span class="s1">beta`&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;see above&gt;&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;gamma&#39;</span><span class="p">,</span> <span class="s1">&#39;Trainable scaling factor :math:`</span><span class="se">\\</span><span class="s1">gamma`&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;see above&gt;&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;Moving average of batch mean&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;see above&gt;&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;Moving average of batch variance&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;see above&gt;&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">sync_batch_normalization</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">comm</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="s2">&quot;world&quot;</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">batch_stat</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                             <span class="n">output_stat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                             <span class="n">param_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">no_scale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">no_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Synchronized batch normalization layer.</span>

<span class="sd">    For some tasks (e.g., semantic segmentation), batch size will be too small and BatchNormalization layer might not work well.</span>
<span class="sd">    SyncBatchNorlization layer solves these problems by synchronizing batch stats (mean and var) between multiple processes.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \\begin{array}{lcl}</span>
<span class="sd">        \\mu &amp;=&amp; \\frac{1}{M} \\sum x_i\\\\</span>
<span class="sd">        \\sigma^2 &amp;=&amp; \\frac{1}{M} \\left(\\sum x_i - \\mu\\right)^2\\\\</span>
<span class="sd">        \\hat{x}_i &amp;=&amp; \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon }}\\\\</span>
<span class="sd">        y_i &amp;= &amp; \\hat{x}_i \\gamma + \\beta.</span>
<span class="sd">        \\end{array}</span>

<span class="sd">    where :math:`x_i, y_i` are the inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array of input.</span>
<span class="sd">        comm (~nnabla.communicators.Communicator): The communicator</span>
<span class="sd">        group (string): The name of the communicator group</span>
<span class="sd">        axes (:obj:`tuple` of :obj:`int`):</span>
<span class="sd">            Mean and variance for each element in ``axes`` are calculated using</span>
<span class="sd">            elements on the rest axes. For example, if an input is 4 dimensions,</span>
<span class="sd">            and ``axes`` is ``[1]``,  batch mean is calculated as</span>
<span class="sd">            ``np.mean(inp.d, axis=(0, 2, 3), keepdims=True)``</span>
<span class="sd">            (using numpy expression as an example).</span>
<span class="sd">        decay_rate (float): Decay rate of running mean and variance.</span>
<span class="sd">        eps (float): Tiny value to avoid zero division by std.</span>
<span class="sd">        batch_stat (bool): Use mini-batch statistics rather than running ones.</span>
<span class="sd">        output_stat (bool): Output batch mean and variance.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the beta and gamma will not be updated.</span>
<span class="sd">        param_init (dict):</span>
<span class="sd">            Parameter initializers can be set with a dict. A key of the dict must</span>
<span class="sd">            be ``&#39;beta&#39;``, ``&#39;gamma&#39;``, ``&#39;mean&#39;`` or ``&#39;var&#39;``.</span>
<span class="sd">            A value of the dict must be an :obj:`~nnabla.initializer.Initializer`</span>
<span class="sd">            or a :obj:`numpy.ndarray`.</span>
<span class="sd">            E.g. ``{&#39;beta&#39;: ConstantInitializer(0), &#39;gamma&#39;: np.ones(gamma_shape) * 2}``.</span>
<span class="sd">        no_scale (bool): If `True`, the scale term is omitted.</span>
<span class="sd">        no_bias (bool): If `True`, the bias term is omitted.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: N-D array.</span>

<span class="sd">    References:</span>
<span class="sd">        - Ioffe and Szegedy, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, https://arxiv.org/abs/1502.03167</span>
<span class="sd">        - Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, Amit Agrawal, Context Encoding for Semantic Segmentation, https://arxiv.org/abs/1803.08904 </span>
<span class="sd">        - Implementing Synchronized Multi-GPU Batch Normalization https://hangzhang.org/PyTorch-Encoding/notes/syncbn.html</span>

<span class="sd">    The shape of parameters has the same number of dimensions with the input</span>
<span class="sd">    data, and the shapes in ``axes`` has the same dimensions with the input, while the rest has ``1``.</span>
<span class="sd">    If an input is 4-dim and ``axes=[1]``, the parameter shape will be</span>
<span class="sd">    ``param_shape  = np.mean(inp.d, axis=(0, 2, 3), keepdims=True).shape``</span>
<span class="sd">    (using numpy expression as an example).</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">.normalization_functions</span> <span class="kn">import</span> <span class="n">_init_beta_gamma</span>

    <span class="n">shape_stat</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">axes</span><span class="p">)):</span>
        <span class="n">shape_stat</span><span class="p">[</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>

    <span class="k">if</span> <span class="n">param_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">param_init</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="n">_init_beta_gamma</span><span class="p">(</span>
        <span class="n">shape_stat</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="p">,</span> <span class="n">param_init</span><span class="p">,</span> <span class="n">no_bias</span><span class="p">,</span> <span class="n">no_scale</span><span class="p">)</span>

    <span class="n">mean_init</span> <span class="o">=</span> <span class="n">param_init</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">var_init</span> <span class="o">=</span> <span class="n">param_init</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">shape_stat</span><span class="p">,</span> <span class="n">mean_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;var&quot;</span><span class="p">,</span> <span class="n">shape_stat</span><span class="p">,</span> <span class="n">var_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">sync_batch_normalization</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">comm</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span>
                                      <span class="n">axes</span><span class="p">,</span> <span class="n">decay_rate</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">batch_stat</span><span class="p">,</span> <span class="n">output_stat</span><span class="p">)</span></div>


<div class="viewcode-block" id="mean_subtraction"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.mean_subtraction">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;mean_subtraction&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;Moving average&#39;</span><span class="p">,</span> <span class="s1">&#39;inp.shape[base_axis:]&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;t&#39;</span><span class="p">,</span> <span class="s1">&#39;Minibatch counter used in forward pass&#39;</span><span class="p">,</span> <span class="s1">&#39;(1,)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">mean_subtraction</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">update_running_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Mean subtraction layer.</span>

<span class="sd">    It subtracts the mean of the elements of the input array,</span>
<span class="sd">    and normalizes it to :math:`0`. Preprocessing arrays with this function has the effect of improving accuracy</span>
<span class="sd">    in various tasks such as image classification.</span>

<span class="sd">    At training time, this function is defined as</span>

<span class="sd">    .. math::</span>

<span class="sd">        \\begin{array}{lcl}</span>
<span class="sd">        \\mu &amp;=&amp; \\frac{1}{M} \\sum x_i \\\\</span>
<span class="sd">        y_i &amp;=&amp; x_i - \\mu</span>
<span class="sd">        \\end{array}</span>

<span class="sd">    At testing time, the mean values used are those that were computed during training by moving average.</span>

<span class="sd">    Note:</span>
<span class="sd">        The backward performs an approximated differentiation that takes into account only the latest mini-batch.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array of input.</span>
<span class="sd">        base_axis (int): Base axis of Mean Subtraction operation. Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">        update_running_mean (bool): When set to `True`, the running mean will not be updated.</span>
<span class="sd">        fix_parameters (bool): dummy parameter. This argument dose not affect anything.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">base_axis</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;t&quot;</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">mean_subtraction</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="n">base_axis</span><span class="p">,</span> <span class="n">update_running_mean</span><span class="o">=</span><span class="n">update_running_mean</span><span class="p">)</span></div>


<div class="viewcode-block" id="layer_normalization"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.layer_normalization">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;layer_normalization&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="s1">&#39;Trainable bias :math:`</span><span class="se">\\</span><span class="s1">beta`&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;see above&gt;&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;gamma&#39;</span><span class="p">,</span> <span class="s1">&#39;Trainable scaling factor :math:`</span><span class="se">\\</span><span class="s1">gamma`&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;see above&gt;&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">layer_normalization</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">output_stat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">param_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">no_scale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">no_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies Layer Normalization over an input variable, which is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">      \begin{eqnarray}</span>
<span class="sd">        \mu^l &amp;=&amp; \frac{1}{H} \sum_{i=1}^{H} x_i^l \\</span>
<span class="sd">        \sigma^l &amp;=&amp; \sqrt{\frac{1}{H} \sum_{i=1}^{H} \left(x_i^l - \mu^l\right)^2} \\</span>
<span class="sd">        y &amp;=&amp; \frac{x - \mu^l}{\sigma^l + \epsilon} \gamma + \beta</span>
<span class="sd">      \end{eqnarray}</span>

<span class="sd">    where :math:`x` and :math:`y` are input and output variable,</span>
<span class="sd">    :math:`\mu^l` and :math:`\sigma^l` are the mean and std of each layer along batch axis,</span>
<span class="sd">    and :math:`\alpha` and :math:`\beta` are trainable parameter.</span>

<span class="sd">    .. note::</span>
<span class="sd">        Unlike other normalization,</span>
<span class="sd">        which applies scalar scale and bias for each entire channel/plane,</span>
<span class="sd">        Layer Normalization applies per-element scale and bias.</span>

<span class="sd">    References:</span>

<span class="sd">        * `Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton, Layer Normalization.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1607.06450&gt;`_</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (Variable): An input variable.</span>
<span class="sd">        batch_axis (int or repeated int): Axes mean and variance are taken.</span>
<span class="sd">        eps (float): Tiny value to avoid zero division by std.</span>
<span class="sd">        output_stat(bool): It `True`, calculated mean and variance are also returned.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the beta and gamma will not be updated.</span>
<span class="sd">        param_init (dict):</span>
<span class="sd">            Parameter initializers can be set with a dict. A key of the dict must</span>
<span class="sd">            be ``&#39;gamma&#39;``, ``&#39;beta&#39;``.</span>
<span class="sd">            A value of the dict must be an :obj:`~nnabla.initializer.Initializer`</span>
<span class="sd">            or a :obj:`numpy.ndarray`.</span>
<span class="sd">            E.g. ``{&#39;gamma&#39;: np.ones(...) * 2, &#39;beta&#39;: ConstantInitializer(0)}``.</span>
<span class="sd">        no_scale (bool): If `True`, the scale term is omitted.</span>
<span class="sd">        no_bias (bool): If `True`, the bias term is omitted.</span>

<span class="sd">    Returns:</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Normalized output variable.</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Mean (if `output_stat=True`).</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Std (if `output_stat=True`)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">nnabla.normalization_functions</span> <span class="kn">import</span> <span class="n">_force_list</span><span class="p">,</span> <span class="n">_init_beta_gamma</span>

    <span class="n">batch_axis</span> <span class="o">=</span> <span class="n">_force_list</span><span class="p">(</span><span class="n">batch_axis</span><span class="p">)</span>

    <span class="n">shape_stat</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">baxis</span> <span class="ow">in</span> <span class="n">batch_axis</span><span class="p">:</span>
        <span class="n">shape_stat</span><span class="p">[</span><span class="n">baxis</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="n">param_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">param_init</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="n">_init_beta_gamma</span><span class="p">(</span>
        <span class="n">shape_stat</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="p">,</span> <span class="n">param_init</span><span class="p">,</span> <span class="n">no_bias</span><span class="p">,</span> <span class="n">no_scale</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">layer_normalization</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span>
                                 <span class="n">batch_axis</span><span class="o">=</span><span class="n">batch_axis</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">output_stat</span><span class="o">=</span><span class="n">output_stat</span><span class="p">)</span></div>


<div class="viewcode-block" id="instance_normalization"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.instance_normalization">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;instance_normalization&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="s1">&#39;Trainable bias :math:`</span><span class="se">\\</span><span class="s1">beta`&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;see above&gt;&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;gamma&#39;</span><span class="p">,</span> <span class="s1">&#39;Trainable scaling factor :math:`</span><span class="se">\\</span><span class="s1">gamma`&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;see above&gt;&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">instance_normalization</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">channel_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">output_stat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                           <span class="n">param_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">no_scale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">no_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies Instance Normalization over an input variable, which is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">      \begin{eqnarray}</span>
<span class="sd">        \mu^i &amp;=&amp; \frac{1}{H} \sum_{i=1}^{H} x_i^i \\</span>
<span class="sd">        \sigma^i &amp;=&amp; \sqrt{\frac{1}{H} \sum_{i=1}^{H} \left(x_i^i - \mu^i\right)^2} \\</span>
<span class="sd">        y &amp;=&amp; \frac{x - \mu^i}{\sigma^ + \epsilon} \gamma + \beta</span>
<span class="sd">      \end{eqnarray}</span>

<span class="sd">    where :math:`x` and :math:`y` are input and output variable,</span>
<span class="sd">    :math:`\mu^i` and :math:`\sigma^i` are the mean and std of each instance which is separately calculated for each batch and channel,</span>
<span class="sd">    and :math:`\gamma` and :math:`\beta` are adaptive gains and biases.</span>

<span class="sd">    If the input shape is [B, C, H, W] (= channel_axis=1, batch_axis=0), the shape of calculated mean and std are [B, C, 1, 1]</span>

<span class="sd">    References:</span>

<span class="sd">        * `Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky, Instance Normalization: The Missing Ingredient for Fast Stylization.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1607.08022&gt;`_</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (Variable): An input variable.</span>
<span class="sd">        channel_axis (int or repeated int): Channel axes.</span>
<span class="sd">        batch_axis (int or repeated int): Batch axes.</span>
<span class="sd">        eps (float): Tiny value to avoid zero division by std.</span>
<span class="sd">        output_stat(bool): It `True`, the batch statistics of mean and variance.</span>
<span class="sd">        fix_parameters (bool): If `True`, the beta and gamma will not be updated.</span>
<span class="sd">        param_init (dict):</span>
<span class="sd">            Parameter initializers can be set with a dict. A key of the dict must</span>
<span class="sd">            be ``&#39;gamma&#39;``, ``&#39;beta&#39;``.</span>
<span class="sd">            A value of the dict must be an :obj:`~nnabla.initializer.Initializer`</span>
<span class="sd">            or a :obj:`numpy.ndarray`.</span>
<span class="sd">            E.g. ``{&#39;gamma&#39;: np.ones(...) * 2, &#39;beta&#39;: ConstantInitializer(0)}``.</span>
<span class="sd">        no_scale (bool): If `True`, the scale term is omitted.</span>
<span class="sd">        no_bias (bool): If `True`, the bias term is omitted.</span>

<span class="sd">        Returns:</span>
<span class="sd">            * :obj:`~nnabla.Variable`: Normalized output variable.</span>
<span class="sd">            * :obj:`~nnabla.Variable`: Mean (if `output_stat=True`)</span>
<span class="sd">            * :obj:`~nnabla.Variable`: Std (if `output_stat=True`)</span>
<span class="sd">        &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">nnabla.normalization_functions</span> <span class="kn">import</span> <span class="n">_init_beta_gamma</span>

    <span class="n">shape_stat</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">))]</span>
    <span class="n">shape_stat</span><span class="p">[</span><span class="n">channel_axis</span><span class="p">]</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">channel_axis</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">param_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">param_init</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="n">_init_beta_gamma</span><span class="p">(</span>
        <span class="n">shape_stat</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="p">,</span> <span class="n">param_init</span><span class="p">,</span> <span class="n">no_bias</span><span class="p">,</span> <span class="n">no_scale</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">instance_normalization</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span>
                                    <span class="n">channel_axis</span><span class="o">=</span><span class="n">channel_axis</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="n">batch_axis</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">output_stat</span><span class="o">=</span><span class="n">output_stat</span><span class="p">)</span></div>


<div class="viewcode-block" id="group_normalization"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.group_normalization">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;group_normalization&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="s1">&#39;Trainable bias :math:`</span><span class="se">\\</span><span class="s1">beta`&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;see above&gt;&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;gamma&#39;</span><span class="p">,</span> <span class="s1">&#39;Trainable scaling factor :math:`</span><span class="se">\\</span><span class="s1">gamma`&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;see above&gt;&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">group_normalization</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="n">channel_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">output_stat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">param_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">no_scale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">no_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies Group Normalization over an input tensor, which is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">      \begin{eqnarray}</span>
<span class="sd">        \mu^g &amp;=&amp; \frac{1}{H} \sum_{i=1}^{H} x_i^g \\</span>
<span class="sd">        \sigma^g &amp;=&amp; \sqrt{\frac{1}{H} \sum_{i=1}^{H} \left(x_i^g - \mu^g\right)^2} \\</span>
<span class="sd">        y &amp;=&amp; \frac{x - \mu^g}{\sigma^g + \epsilon} \gamma + \beta</span>
<span class="sd">      \end{eqnarray}</span>

<span class="sd">    where :math:`x` and :math:`y` are input and output variable,</span>
<span class="sd">    :math:`\mu^g` and :math:`\sigma^g` are the mean and std of each group which contains `num_channels / num_groups` channels,</span>
<span class="sd">    and :math:`\gamma` and :math:`\beta` are adaptive gains and biases.</span>

<span class="sd">    The input channels, specified by :attr:`channel_axis`, are separeted into :attr:`num_groups` groups,</span>
<span class="sd">    and the mean and std are calculated over the each group.</span>
<span class="sd">    For example, if the input shape is [B, C, H, W] (= channel_axis=1, batch_axis=0),</span>
<span class="sd">    an input variable is once reshaped to [B, num_groups, C / num_groups, H, W]</span>
<span class="sd">    and standardize by its mean and std whose shapes are [B, num_groups, C / num_groups, 1, 1].</span>
<span class="sd">    Before returning, an output variable is reshaped again to the original input shape (= [B, C, H, W] in the case above).</span>

<span class="sd">    References:</span>

<span class="sd">        * `Yuxin Wu, Kaiming He, Group Normalization.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1803.08494&gt;`_</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (Variable): An input variable.</span>
<span class="sd">        num_groups (int): A number of groups. The channel dim of &#39;x&#39; must be integer multiple of `num_groups`.</span>
<span class="sd">        channel_axis (int): Channel axis.</span>
<span class="sd">        batch_axis (int or repeated int): Axes mean and variance are taken.</span>
<span class="sd">        eps (float): Tiny value to avoid zero division by std.</span>
<span class="sd">        output_stat(bool): It true, the batch statistics of mean and variance.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the beta and gamma will not be updated.</span>
<span class="sd">        param_init (dict):</span>
<span class="sd">            Parameter initializers can be set with a dict. A key of the dict must</span>
<span class="sd">            be ``&#39;gamma&#39;``, ``&#39;beta&#39;``.</span>
<span class="sd">            A value of the dict must be an :obj:`~nnabla.initializer.Initializer`</span>
<span class="sd">            or a :obj:`numpy.ndarray`.</span>
<span class="sd">            E.g. ``{&#39;gamma&#39;: np.ones(...) * 2, &#39;beta&#39;: ConstantInitializer(0)}``.</span>
<span class="sd">        no_scale (bool): If `True`, the scale term is omitted.</span>
<span class="sd">        no_bias (bool): If `True`, the bias term is omitted.</span>


<span class="sd">    Returns:</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Normalized output variable.</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Mean (if `output_stat=True`)</span>
<span class="sd">        * :obj:`~nnabla.Variable`: Std (if `output_stat=True`)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">nnabla.normalization_functions</span> <span class="kn">import</span> <span class="n">_force_list</span><span class="p">,</span> <span class="n">_init_beta_gamma</span>

    <span class="n">batch_axis</span> <span class="o">=</span> <span class="n">_force_list</span><span class="p">(</span><span class="n">batch_axis</span><span class="p">)</span>

    <span class="n">shape_stat</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">))]</span>
    <span class="n">shape_stat</span><span class="p">[</span><span class="n">channel_axis</span><span class="p">]</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">channel_axis</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">param_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">param_init</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="n">_init_beta_gamma</span><span class="p">(</span>
        <span class="n">shape_stat</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="p">,</span> <span class="n">param_init</span><span class="p">,</span> <span class="n">no_bias</span><span class="p">,</span> <span class="n">no_scale</span><span class="p">)</span>

    <span class="c1"># we dont have to broadcast beta and gamma here in this case because adaptive operation in bn is not used.</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">group_normalization</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span>
                                 <span class="n">num_groups</span><span class="o">=</span><span class="n">num_groups</span><span class="p">,</span> <span class="n">channel_axis</span><span class="o">=</span><span class="n">channel_axis</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="n">batch_axis</span><span class="p">,</span>
                                 <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">output_stat</span><span class="o">=</span><span class="n">output_stat</span><span class="p">)</span></div>


<div class="viewcode-block" id="embed"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.embed">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;embed&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Embedding matrix&#39;</span><span class="p">,</span> <span class="s1">&#39;(n_inputs, n_features)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">embed</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
          <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">apply_w</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Embed.</span>

<span class="sd">    Embed slices a matrix/tensor with indexing array/tensor. Weights are initialized with :obj:`nnabla.initializer.UniformInitializer` within the range of :math:`-\\sqrt{3}` and :math:`\\sqrt{3}`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): [Integer] Indices with shape :math:`(I_0, ..., I_N)`</span>
<span class="sd">        n_inputs : number of possible inputs, words or vocabraries</span>
<span class="sd">        n_features : number of embedding features</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the embedding weight matrix</span>
<span class="sd">            will not be updated.</span>
<span class="sd">        apply_w (function): Lambda, function, or callable object applied to the weights.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output with shape :math:`(I_0, ..., I_N, W_1, ..., W_M)`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">initializer</span><span class="p">:</span>
        <span class="n">initializer</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">((</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">3.</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">3</span><span class="p">)))</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span><span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_features</span><span class="p">],</span>
                                <span class="n">initializer</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">apply_w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">apply_w</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span></div>


<div class="viewcode-block" id="prelu"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.prelu">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;prelu&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;slope&#39;</span><span class="p">,</span> <span class="s1">&#39;Negative slope&#39;</span><span class="p">,</span>
     <span class="s1">&#39;tuple() if shared else (inp.shape[base_axis],)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">prelu</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shared</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">slope_init</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parametrized Rectified Linear Unit function defined as</span>

<span class="sd">    .. math::</span>
<span class="sd">        y_i = \max(0, x_i) + w_i \min(0, x_i)</span>

<span class="sd">    where negative slope :math:`w` is learned and can vary across channels (an</span>
<span class="sd">    axis specified with base_axis). Weights are initialized with :math:`-1`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array as input</span>
<span class="sd">        base_axis(int): Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">        shared(bool): Use shared weight value or not</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the negative slope values</span>
<span class="sd">            will not be updated.</span>
<span class="sd">        slope_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`):</span>
<span class="sd">            Initializer of negative slopes. By default, they are initialized with `0.25`.</span>
<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">()</span> <span class="k">if</span> <span class="n">shared</span> <span class="k">else</span> <span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],)</span>
    <span class="k">if</span> <span class="n">slope_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">slope_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span><span class="s2">&quot;slope&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span>
                                <span class="n">slope_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">prelu</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="fixed_point_quantized_affine"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.fixed_point_quantized_affine">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;fp_quantized_affine&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Weight matrix in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;W_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Quantized weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Quantized biases&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">fixed_point_quantized_affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span>
                                 <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                 <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                 <span class="n">quantize_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sign_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_w</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">delta_w</span><span class="o">=</span><span class="mi">2</span><span class="o">**-</span><span class="mi">4</span><span class="p">,</span> <span class="n">ste_fine_grained_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                 <span class="n">quantize_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sign_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_b</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">delta_b</span><span class="o">=</span><span class="mi">2</span><span class="o">**-</span><span class="mi">4</span><span class="p">,</span> <span class="n">ste_fine_grained_b</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Fixed-Point Quantized Affine.</span>

<span class="sd">    Fixed-Point Quantized Affine is the affine function,</span>
<span class="sd">    except the definition of the inner product is modified.</span>
<span class="sd">    The input-output relation of this function is as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_j = \sum_{i} Q(w_{ji}) x_i,</span>

<span class="sd">    where :math:`Q(w_{ji})` is the fixed-point quantization function.</span>

<span class="sd">    .. note::</span>

<span class="sd">        1) if you would like to share weights between some layers, please</span>
<span class="sd">        make sure to share the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the quantized weights (`quantized weight`)</span>

<span class="sd">        2) The weights and the quantized weights become synced only after :func:`~nnabla._variable.Variable.forward` is called,</span>
<span class="sd">        and not after a call to :func:`~nnabla._variable.Variable.backward`.</span>
<span class="sd">        To access the parameters of the network, remember to call :func:`~nnabla._variable.Variable.forward` once before doing so, otherwise the</span>
<span class="sd">        float weights and the quantized weights will not be in sync.</span>

<span class="sd">        3) CPU and GPU implementations now use float value for `quantized weight`,</span>
<span class="sd">        since this function is only for simulation purposes.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): Input N-D array with shape (:math:`M_0 \\times \ldots \\times M_{B-1} \\times D_B \\times \ldots \\times D_N`). Dimensions before and after base_axis are flattened as if it is a matrix.</span>
<span class="sd">        n_outmaps (:obj:`int` or :obj:`tuple` of :obj:`int`): Number of output neurons per data.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`. </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>
<span class="sd">        quantize_w (bool): Quantize weights if `True`.</span>
<span class="sd">        sign_w (bool): Use signed quantization if `True`.</span>
<span class="sd">        n_w (int): Bit width used for weight.</span>
<span class="sd">        delta_w (float): Step size for weight.</span>
<span class="sd">        ste_fine_grained_w (bool): STE is fine-grained if `True`.</span>
<span class="sd">        quantize_b (bool): Quantize bias if `True`.</span>
<span class="sd">        n_b (int): Bit width used for bias.</span>
<span class="sd">        delta_w (float): Step size for bias.</span>
<span class="sd">        ste_fine_grained_b (bool): STE is fine-grained if `True`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: :math:`(B + 1)`-D array. (:math:`M_0 \\times \ldots \\times M_{B-1} \\times L`)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">,</span> <span class="s1">&#39;__iter__&#39;</span><span class="p">):</span>
        <span class="n">n_outmaps</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_outmaps</span><span class="p">]</span>
    <span class="n">n_outmaps</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">)</span>
    <span class="n">n_outmap</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inmaps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inmaps</span><span class="p">,</span> <span class="n">n_outmap</span><span class="p">),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>

    <span class="c1"># Floating Weight</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="c1"># Quantized Weight</span>
    <span class="k">if</span> <span class="n">quantize_w</span><span class="p">:</span>
        <span class="n">w_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;W_q&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
            <span class="n">w_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="c1"># Link computation graph</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fixed_point_quantize</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">quantize</span><span class="o">=</span><span class="n">quantize_w</span><span class="p">,</span>
                                          <span class="n">sign</span><span class="o">=</span><span class="n">sign_w</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n_w</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="n">delta_w</span><span class="p">,</span>
                                          <span class="n">ste_fine_grained</span><span class="o">=</span><span class="n">ste_fine_grained_w</span><span class="p">,</span>
                                          <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">w_q</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
        <span class="n">real_w_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">w</span>

    <span class="c1"># Bias</span>
    <span class="c1"># Floating</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">b_q</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">real_b_q</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">quantize_b</span><span class="p">:</span>
            <span class="n">b_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
                <span class="s2">&quot;b_q&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="c1"># Link computation graph</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fixed_point_quantize</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">quantize</span><span class="o">=</span><span class="n">quantize_b</span><span class="p">,</span>
                                              <span class="n">sign</span><span class="o">=</span><span class="n">sign_b</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n_b</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="n">delta_b</span><span class="p">,</span>
                                              <span class="n">ste_fine_grained</span><span class="o">=</span><span class="n">ste_fine_grained_b</span><span class="p">,</span>
                                              <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">b_q</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
            <span class="n">real_b_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">b</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">real_w_q</span><span class="p">,</span> <span class="n">real_b_q</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="fixed_point_quantized_convolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.fixed_point_quantized_convolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;fp_quantized_conv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps // group, *kernel)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;W_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Quantized weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps // group, *kernel)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Quantized biases&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">fixed_point_quantized_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span>
                                      <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">channel_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                      <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                      <span class="n">quantize_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sign_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_w</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">delta_w</span><span class="o">=</span><span class="mi">2</span><span class="o">**-</span><span class="mi">4</span><span class="p">,</span> <span class="n">ste_fine_grained_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                      <span class="n">quantize_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sign_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_b</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">delta_b</span><span class="o">=</span><span class="mi">2</span><span class="o">**-</span><span class="mi">4</span><span class="p">,</span> <span class="n">ste_fine_grained_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Fixed-Point Quantized Convolution.</span>

<span class="sd">    Fixed-Point Quantized Convolution is the convolution function,</span>
<span class="sd">    except the definition of the inner product is modified.</span>
<span class="sd">    The input-output relation of this function is as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_{n, a, b} = \sum_{m} \sum_{i} \sum_{j} Q(w_{n, m, i, j}) x_{m, a + i, b + j},</span>

<span class="sd">    where :math:`Q(w_{n, m, i, j})` is the fixed-point quantization function.</span>

<span class="sd">    .. note::</span>

<span class="sd">        1) if you would like to share weights between some layers, please</span>
<span class="sd">        make sure to share the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the quantized weights (`quantized weight`)</span>

<span class="sd">        2) The weights and the quantized weights become synced only after :func:`~nnabla._variable.Variable.forward` is called,</span>
<span class="sd">        and not after a call to :func:`~nnabla._variable.Variable.backward`.</span>
<span class="sd">        To access the parameters of the network, remember to call :func:`~nnabla._variable.Variable.forward` once before doing so, otherwise the</span>
<span class="sd">        float weights and the quantized weights will not be in sync.</span>

<span class="sd">        3) CPU and GPU implementations now use float value for `quantized weight`,</span>
<span class="sd">        since this function is only for simulation purposes.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array.</span>
<span class="sd">        outmaps (int): Number of convolution kernels (which is equal to the number of output channels). For example, to apply convolution on an input with 16 types of filters, specify 16.</span>
<span class="sd">        kernel (:obj:`tuple` of :obj:`int`): Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</span>
<span class="sd">        pad (:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">        stride (:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">        dilation (:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">        group (int): Number of groups of channels. This makes connections across channels more sparse by grouping connections along map direction.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.  </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>
<span class="sd">        quantize_w (bool): Quantize weights if `True`.</span>
<span class="sd">        quantize_bias (bool): Quantize bias if `True`.</span>
<span class="sd">        sign_w (bool): Use signed quantization if `True`.</span>
<span class="sd">        n_w (int): Bit width used for weight.</span>
<span class="sd">        delta_w (float): Step size for weight.</span>
<span class="sd">        ste_fine_grained_w (bool): STE is fine-grained if `True`.</span>
<span class="sd">        quantize_b (bool): Quantize bias if `True`.</span>
<span class="sd">        n_b (int): Bit width used for bias.</span>
<span class="sd">        delta_w (float): Step size for bias.</span>
<span class="sd">        ste_fine_grained_b (bool): STE is fine-grained if `True`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: N-D array.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">channel_last</span><span class="p">:</span>
        <span class="n">channels</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">filter_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">channels</span> <span class="o">//</span> <span class="n">group</span><span class="p">,)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">channels</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span>
        <span class="n">filter_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">channels</span> <span class="o">//</span> <span class="n">group</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>

    <span class="c1"># Floating Weight</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span> <span class="o">//</span> <span class="n">group</span><span class="p">)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="c1"># Quantized Weight</span>
    <span class="k">if</span> <span class="n">quantize_w</span><span class="p">:</span>
        <span class="n">w_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;W_q&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,)</span> <span class="o">+</span> <span class="n">filter_shape</span><span class="p">,</span>
            <span class="n">w_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="c1"># Link computation graph</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fixed_point_quantize</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">quantize</span><span class="o">=</span><span class="n">quantize_w</span><span class="p">,</span>
                                          <span class="n">sign</span><span class="o">=</span><span class="n">sign_w</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n_w</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="n">delta_w</span><span class="p">,</span>
                                          <span class="n">ste_fine_grained</span><span class="o">=</span><span class="n">ste_fine_grained_w</span><span class="p">,</span>
                                          <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">w_q</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
        <span class="n">real_w_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">w</span>

    <span class="c1"># Bias</span>
    <span class="c1"># Floating</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">b_q</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">real_b_q</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">quantize_b</span><span class="p">:</span>
            <span class="n">b_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
                <span class="s2">&quot;b_q&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="c1"># Link computation graph</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fixed_point_quantize</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">quantize</span><span class="o">=</span><span class="n">quantize_b</span><span class="p">,</span>
                                              <span class="n">sign</span><span class="o">=</span><span class="n">sign_b</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n_b</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="n">delta_b</span><span class="p">,</span>
                                              <span class="n">ste_fine_grained</span><span class="o">=</span><span class="n">ste_fine_grained_b</span><span class="p">,</span>
                                              <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">b_q</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
            <span class="n">real_b_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">b</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">real_w_q</span><span class="p">,</span> <span class="n">real_b_q</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="pow2_quantized_affine"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.pow2_quantized_affine">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;pow2_quantized_affine&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Weight matrix in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;W_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Quantized weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Quantized biases&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">pow2_quantized_affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span>
                          <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                          <span class="n">quantize_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sign_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">with_zero_w</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_w</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">m_w</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ste_fine_grained_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                          <span class="n">quantize_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sign_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">with_zero_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_b</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">m_b</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ste_fine_grained_b</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Pow2 Quantized Affine.</span>

<span class="sd">    Pow2 Quantized Affine is the affine function,</span>
<span class="sd">    except the definition of the inner product is modified.</span>
<span class="sd">    The input-output relation of this function is as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_j = \sum_{i} Q(w_{ji}) x_i,</span>

<span class="sd">    where :math:`Q(w_{ji})` is the power-of-2 quantization function.</span>

<span class="sd">    .. note::</span>

<span class="sd">        1) if you would like to share weights between some layers, please</span>
<span class="sd">        make sure to share the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the quantized weights (`quantized weight`)</span>

<span class="sd">        2) The weights and the quantized weights become synced only after :func:`~nnabla._variable.Variable.forward` is called,</span>
<span class="sd">        and not after a call to :func:`~nnabla._variable.Variable.backward`.</span>
<span class="sd">        To access the parameters of the network, remember to call :func:`~nnabla._variable.Variable.forward` once before doing so, otherwise the</span>
<span class="sd">        float weights and the quantized weights will not be in sync.</span>

<span class="sd">        3) Quantized values are stored as floating point number for `quantized weight`,</span>
<span class="sd">        since this function is only for simulation purposes.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): Input N-D array with shape (:math:`M_0 \\times \ldots \\times M_{B-1} \\times D_B \\times \ldots \\times D_N`). Dimensions before and after base_axis are flattened as if it is a matrix.</span>
<span class="sd">        n_outmaps (:obj:`int` or :obj:`tuple` of :obj:`int`): Number of output neurons per data.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.  </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>
<span class="sd">        quantize_w (bool): Quantize weights if `True`.</span>
<span class="sd">        sign_w (bool): Use signed quantization if `True`.</span>
<span class="sd">        with_zero_w (bool): Indicate using zero as a quantized value. Default is false.</span>
<span class="sd">        n_w (int): Bit width used for weight.</span>
<span class="sd">        m_w (int): :math:`2^m` is upper bound and :math:`-2^m` is lower bound for weights. Default is 2.</span>
<span class="sd">        ste_fine_grained_w (bool): STE is fine-grained if `True`.</span>
<span class="sd">        quantize_b (bool): Quantize bias if `True`.</span>
<span class="sd">        with_zero_b (bool): Indicate using zero as a quantized value. Default is false.</span>
<span class="sd">        n_b (int): Bit width used for bias.</span>
<span class="sd">        m_b (int): :math:`2^m` is upper bound and :math:`-2^m` is lower bound for bias. Default is 2.</span>
<span class="sd">        ste_fine_grained_b (bool): STE is fine-grained if `True`.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: :math:`(B + 1)`-D array. (:math:`M_0 \\times \ldots \\times M_{B-1} \\times L`)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">,</span> <span class="s1">&#39;__iter__&#39;</span><span class="p">):</span>
        <span class="n">n_outmaps</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_outmaps</span><span class="p">]</span>
    <span class="n">n_outmaps</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">)</span>
    <span class="n">n_outmap</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inmaps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inmaps</span><span class="p">,</span> <span class="n">n_outmap</span><span class="p">),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>

    <span class="c1"># Floating Weight</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="c1"># Quantized Weight</span>
    <span class="k">if</span> <span class="n">quantize_w</span><span class="p">:</span>
        <span class="n">w_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;W_q&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
            <span class="n">w_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="c1"># Link computation graph</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pow2_quantize</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">quantize</span><span class="o">=</span><span class="n">quantize_w</span><span class="p">,</span>
                                   <span class="n">sign</span><span class="o">=</span><span class="n">sign_w</span><span class="p">,</span> <span class="n">with_zero</span><span class="o">=</span><span class="n">with_zero_w</span><span class="p">,</span>
                                   <span class="n">n</span><span class="o">=</span><span class="n">n_w</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="n">m_w</span><span class="p">,</span> <span class="n">ste_fine_grained</span><span class="o">=</span><span class="n">ste_fine_grained_w</span><span class="p">,</span>
                                   <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">w_q</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
        <span class="n">real_w_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">w</span>

    <span class="c1"># Bias</span>
    <span class="c1"># Floating</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">b_q</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">real_b_q</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">quantize_b</span><span class="p">:</span>
            <span class="n">b_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
                <span class="s2">&quot;b_q&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pow2_quantize</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">quantize</span><span class="o">=</span><span class="n">quantize_b</span><span class="p">,</span>
                                       <span class="n">sign</span><span class="o">=</span><span class="n">sign_b</span><span class="p">,</span> <span class="n">with_zero</span><span class="o">=</span><span class="n">with_zero_b</span><span class="p">,</span>
                                       <span class="n">n</span><span class="o">=</span><span class="n">n_b</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="n">m_b</span><span class="p">,</span> <span class="n">ste_fine_grained</span><span class="o">=</span><span class="n">ste_fine_grained_b</span><span class="p">,</span>
                                       <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">b_q</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
            <span class="n">real_b_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">b</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">real_w_q</span><span class="p">,</span> <span class="n">real_b_q</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="pow2_quantized_convolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.pow2_quantized_convolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;pow2_quantized_conv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps // group, *kernel)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;W_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Quantized weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps // group, *kernel)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Quantized biases&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">pow2_quantized_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span>
                               <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                               <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                               <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                               <span class="n">quantize_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">with_zero_w</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sign_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_w</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">m_w</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ste_fine_grained_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                               <span class="n">quantize_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">with_zero_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sign_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_b</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">m_b</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ste_fine_grained_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Pow2 Quantized Convolution.</span>

<span class="sd">    Pow2 Quantized Convolution is the convolution function,</span>
<span class="sd">    except the definition of the inner product is modified.</span>
<span class="sd">    The input-output relation of this function is as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_{n, a, b} = \sum_{m} \sum_{i} \sum_{j} Q(w_{n, m, i, j}) x_{m, a + i, b + j},</span>

<span class="sd">    where :math:`Q(w_{n, m, i, j})` is the power-of-2 quantization function.</span>

<span class="sd">    .. note::</span>

<span class="sd">        1) if you would like to share weights between some layers, please</span>
<span class="sd">        make sure to share the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the quantized weights (`quantized weight`)</span>

<span class="sd">        2) The weights and the quantized weights become synced only after :func:`~nnabla._variable.Variable.forward` is called,</span>
<span class="sd">        and not after a call to :func:`~nnabla._variable.Variable.backward`.</span>
<span class="sd">        To access the parameters of the network, remember to call :func:`~nnabla._variable.Variable.forward` once before doing so, otherwise the</span>
<span class="sd">        float weights and the quantized weights will not be in sync.</span>

<span class="sd">        3) Quantized values are stored as floating point number for `quantized weight`,</span>
<span class="sd">        since this function is only for simulation purposes.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array.</span>
<span class="sd">        outmaps (int): Number of convolution kernels (which is equal to the number of output channels). For example, to apply convolution on an input with 16 types of filters, specify 16.</span>
<span class="sd">        kernel (:obj:`tuple` of :obj:`int`): Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</span>
<span class="sd">        pad (:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">        stride (:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">        dilation (:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">        group (int): Number of groups of channels. This makes connections across channels more sparse by grouping connections along map direction.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.  </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>
<span class="sd">        quantize_w (bool): Quantize weights if `True`.</span>
<span class="sd">        sign_w (bool): Use signed quantization if `True`.</span>
<span class="sd">        n_w (int): Bit width used for weight.</span>
<span class="sd">        m_w (int): :math:`2^m` is upper bound and :math:`-2^m` is lower bound for weights. Default is 2.</span>
<span class="sd">        ste_fine_grained_w (bool): STE is fine-grained if `True`.</span>
<span class="sd">        quantize_b (bool): Quantize bias if `True`.</span>
<span class="sd">        sign_b (bool): Use signed quantization if `True`.</span>
<span class="sd">        n_b (int): Bit width used for bias.</span>
<span class="sd">        m_b (int): :math:`2^m` is upper bound and :math:`-2^m` is lower bound for bias. Default is 2.</span>
<span class="sd">        ste_fine_grained_b (bool): STE is fine-grained if `True`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: N-D array.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="n">outmaps</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>

    <span class="c1"># Floating Weight</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span> <span class="o">//</span> <span class="n">group</span><span class="p">)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="c1"># Quantized Weight</span>
    <span class="k">if</span> <span class="n">quantize_w</span><span class="p">:</span>
        <span class="n">w_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;W_q&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span> <span class="o">//</span> <span class="n">group</span><span class="p">)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
            <span class="n">w_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Link computation graph</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pow2_quantize</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">quantize</span><span class="o">=</span><span class="n">quantize_w</span><span class="p">,</span>
                                   <span class="n">sign</span><span class="o">=</span><span class="n">sign_w</span><span class="p">,</span> <span class="n">with_zero</span><span class="o">=</span><span class="n">with_zero_w</span><span class="p">,</span>
                                   <span class="n">n</span><span class="o">=</span><span class="n">n_w</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="n">m_w</span><span class="p">,</span> <span class="n">ste_fine_grained</span><span class="o">=</span><span class="n">ste_fine_grained_w</span><span class="p">,</span>
                                   <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">w_q</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
        <span class="n">real_w_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">w</span>

    <span class="c1"># Bias</span>
    <span class="c1"># Floating</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">b_q</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">real_b_q</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">quantize_b</span><span class="p">:</span>
            <span class="n">b_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
                <span class="s2">&quot;b_q&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="c1"># Link computation graph</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pow2_quantize</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">quantize</span><span class="o">=</span><span class="n">quantize_b</span><span class="p">,</span>
                                       <span class="n">sign</span><span class="o">=</span><span class="n">sign_b</span><span class="p">,</span> <span class="n">with_zero</span><span class="o">=</span><span class="n">with_zero_b</span><span class="p">,</span>
                                       <span class="n">n</span><span class="o">=</span><span class="n">n_b</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="n">m_b</span><span class="p">,</span> <span class="n">ste_fine_grained</span><span class="o">=</span><span class="n">ste_fine_grained_b</span><span class="p">,</span>
                                       <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">b_q</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
            <span class="n">real_b_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">b</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">real_w_q</span><span class="p">,</span> <span class="n">real_b_q</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="pruned_affine"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.pruned_affine">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;pruned_affine&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Weight matrix in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;W_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Qunatized weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Quantized biases&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">pruned_affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span>
                  <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                  <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">prune_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rate_w</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">prune_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rate_b</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Pruned Affine.</span>

<span class="sd">    Pruned Affine is the affine function, </span>
<span class="sd">    except the definition of the inner product is modified.</span>
<span class="sd">    The input-output relation of this function is as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_j = \sum_{i} Q(w_{ji}) x_i, </span>

<span class="sd">    where :math:`Q(w_{ji})` is the pruning function, i.e., `F.prune`.</span>

<span class="sd">    .. note::</span>

<span class="sd">        1) if you would like to share weights between some layers, please</span>
<span class="sd">        make sure to share the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the quantized weights (`quantized weight`)</span>

<span class="sd">        2) The weights and the quantized weights become synced only after :func:`~nnabla._variable.Variable.forward` is called,</span>
<span class="sd">        and not after a call to :func:`~nnabla._variable.Variable.backward`.</span>
<span class="sd">        To access the parameters of the network, remember to call :func:`~nnabla._variable.Variable.forward` once before doing so, otherwise the</span>
<span class="sd">        float weights and the quantized weights will not be in sync.</span>

<span class="sd">        3) CPU and GPU implementations now use float value for `quantized weight`,</span>
<span class="sd">        since this function is only for simulation purposes.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): Input N-D array with shape (:math:`M_0 \\times \ldots \\times M_{B-1} \\times D_B \\times \ldots \\times D_N`). Dimensions before and after base_axis are flattened as if it is a matrix.</span>
<span class="sd">        n_outmaps (:obj:`int` or :obj:`tuple` of :obj:`int`): Number of output neurons per data.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight.</span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>
<span class="sd">        prune_w (bool): Quantize weights if `True`.</span>
<span class="sd">        rate_w (float): Pruning rate for weights.</span>
<span class="sd">        prune_b (bool): Quantize bias if `True`.</span>
<span class="sd">        rate_b (float): Pruning rate for bias.</span>


<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: :math:`(B + 1)`-D array. (:math:`M_0 \\times \ldots \\times M_{B-1} \\times L`)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">,</span> <span class="s1">&#39;__iter__&#39;</span><span class="p">):</span>
        <span class="n">n_outmaps</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_outmaps</span><span class="p">]</span>
    <span class="n">n_outmaps</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">)</span>
    <span class="n">n_outmap</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inmaps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inmaps</span><span class="p">,</span> <span class="n">n_outmap</span><span class="p">),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>

    <span class="c1"># Floating Weight</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="c1"># sparse Weight</span>
    <span class="k">if</span> <span class="n">prune_w</span><span class="p">:</span>
        <span class="n">w_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;W_q&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
            <span class="n">w_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="c1"># Link computation graph</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">prune</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="n">rate_w</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">w_q</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
        <span class="n">real_w_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">w</span>

    <span class="c1"># Bias</span>
    <span class="c1"># Floating</span>
    <span class="n">real_b_q</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">prune_b</span><span class="p">:</span>
            <span class="n">b_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
                <span class="s2">&quot;b_q&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="c1"># Link computation graph</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">prune</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="n">rate_b</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">b_q</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
            <span class="n">real_b_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">b</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">real_w_q</span><span class="p">,</span> <span class="n">real_b_q</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="pruned_convolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.pruned_convolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;pruned_conv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps // group, *kernel)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;W_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Qunatized weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps // group, *kernel)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Quantized biases&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">pruned_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span>
                       <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">channel_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                       <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                       <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                       <span class="n">prune_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rate_w</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">prune_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rate_b</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Pruned Convolution.</span>

<span class="sd">    Pruned Convolution is the convolution function, </span>
<span class="sd">    except the definition of the inner product is modified.</span>
<span class="sd">    The input-output relation of this function is as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_{n, a, b} = \sum_{m} \sum_{i} \sum_{j} Q(w_{n, m, i, j}) x_{m, a + i, b + j}, </span>

<span class="sd">    where :math:`Q(w_{ji})` is the pruning function, i.e., `F.prune`.</span>

<span class="sd">    .. note::</span>

<span class="sd">        1) if you would like to share weights between some layers, please</span>
<span class="sd">        make sure to share the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the quantized weights (`quantized weight`)</span>

<span class="sd">        2) The weights and the quantized weights become synced only after :func:`~nnabla._variable.Variable.forward` is called,</span>
<span class="sd">        and not after a call to :func:`~nnabla._variable.Variable.backward`.</span>
<span class="sd">        To access the parameters of the network, remember to call :func:`~nnabla._variable.Variable.forward` once before doing so, otherwise the</span>
<span class="sd">        float weights and the quantized weights will not be in sync.</span>

<span class="sd">        3) CPU and GPU implementations now use float value for `quantized weight`,</span>
<span class="sd">        since this function is only for simulation purposes.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array.</span>
<span class="sd">        outmaps (int): Number of convolution kernels (which is equal to the number of output channels). For example, to apply convolution on an input with 16 types of filters, specify 16.</span>
<span class="sd">        kernel (:obj:`tuple` of :obj:`int`): Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</span>
<span class="sd">        pad (:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">        stride (:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">        dilation (:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">        group (int): Number of groups of channels. This makes connections across channels more sparse by grouping connections along map direction.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight.</span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>
<span class="sd">        prune_w (bool): Quantize weights if `True`.</span>
<span class="sd">        rate_w (float): Pruning rate for weights.</span>
<span class="sd">        prune_b (bool): Quantize bias if `True`.</span>
<span class="sd">        rate_b (float): Pruning rate for bias.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: N-D array.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">channel_last</span><span class="p">:</span>
        <span class="n">channels</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">filter_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">channels</span> <span class="o">//</span> <span class="n">group</span><span class="p">,)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">channels</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span>
        <span class="n">filter_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">channels</span> <span class="o">//</span> <span class="n">group</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>

    <span class="c1"># Floating Weight</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,)</span> <span class="o">+</span> <span class="n">filter_shape</span><span class="p">,</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="c1"># Quantized Weight</span>
    <span class="k">if</span> <span class="n">prune_w</span><span class="p">:</span>
        <span class="n">w_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;W_q&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span> <span class="o">//</span> <span class="n">group</span><span class="p">)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
            <span class="n">w_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="c1"># Link computation graph</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">prune</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="n">rate_w</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">w_q</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
        <span class="n">real_w_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">w</span>

    <span class="c1"># Bias</span>
    <span class="c1"># Floating</span>
    <span class="n">real_b_q</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">prune_b</span><span class="p">:</span>
            <span class="n">b_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
                <span class="s2">&quot;b_q&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="c1"># Link computation graph</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">prune</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="n">rate_b</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">b_q</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
            <span class="n">real_b_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">b</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">real_w_q</span><span class="p">,</span> <span class="n">real_b_q</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">channel_last</span><span class="p">)</span></div>


<div class="viewcode-block" id="min_max_quantize"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.min_max_quantize">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;min_max_quantize&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;qr_min&#39;</span><span class="p">,</span> <span class="s1">&#39;Minimum quantization range, the exponential movining average of min values of inputs initialized with -6.0 if ema is True&#39;</span><span class="p">,</span> <span class="s1">&#39;ql_min.shape&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;qr_max&#39;</span><span class="p">,</span> <span class="s1">&#39;Maximum quantization range, the exponential movining average of max values of inputs initialized with 6.0 if ema is True&#39;</span><span class="p">,</span> <span class="s1">&#39;ql_max.shape&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">min_max_quantize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ql_min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ql_max</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">x_min_max</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ema</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                     <span class="n">ste_fine_grained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                     <span class="n">qr_min_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">qr_max_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                     <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Min-max quantization.</span>

<span class="sd">    This function uniformly quantizes values in the range of min and max quantization levels.</span>

<span class="sd">    Min-max quantization is defined as the following equation</span>

<span class="sd">    .. math::</span>

<span class="sd">        y = round \left(\frac{\min(\max(x, m), M) - m}{scale} \right) \times scale + m, </span>

<span class="sd">    where the :math:`scale` is defined as </span>

<span class="sd">    .. math::</span>

<span class="sd">        scale = \frac{M - m}{M_q - m_q}, </span>

<span class="sd">    and </span>

<span class="sd">    .. math::</span>

<span class="sd">        m_q = ql_{min}, \\</span>
<span class="sd">        M_q = ql_{max}, \\</span>
<span class="sd">        m = qr_{min}, \\</span>
<span class="sd">        M = qr_{max}.</span>

<span class="sd">    In the backward pass when using `ste_fine_grained` as false,</span>

<span class="sd">        .. math::</span>

<span class="sd">          \frac{\partial q_i}{\partial x_i} = 1.</span>


<span class="sd">    In the backward pass when using `ste_fine_grained` as true,</span>

<span class="sd">        .. math::</span>

<span class="sd">           \frac{\partial q_i}{\partial x_i}= \left\{</span>
<span class="sd">         \begin{array}{ll}</span>
<span class="sd">           0 &amp; if \ \ \ x_i &gt; M \\</span>
<span class="sd">           1 &amp; if \ \ m \le x_i \le M \\</span>
<span class="sd">           0 &amp; if \ \ x_i &lt; m \\</span>
<span class="sd">         \end{array} \right..</span>

<span class="sd">    :math:`qr_{min}` and :math:`qr_{max}` are treaded as follows.</span>

<span class="sd">        * `x_min_max` is `True` and `ema` is `True`: </span>
<span class="sd">          Exponential moving average are computed for each :math:`min(x)` and :math:`max(x)` </span>
<span class="sd">          then stored in :math:`qr_{min}` and :math:`qr_{max}`.</span>
<span class="sd">        * `x_min_max` is `True` and `ema` is `False`:</span>
<span class="sd">          :math:`min(x)` and :math:`max(x)` are computed then stored in :math:`qr_{min}` and :math:`qr_{max}`.</span>
<span class="sd">        * `x_min_max` is `False` and `ema` is `True`:</span>
<span class="sd">          Exponential moving average stored in :math:`qr_{min}` and :math:`qr_{max}` are used.</span>
<span class="sd">        * `x_min_max` is `False` and `ema` is `False`</span>
<span class="sd">          Gradients of :math:`qr_{min}` and :math:`qr_{max}` are computed in the backward pass.</span>

<span class="sd">    More precisely, in inference of the min-max quantization, one has to consider *zero-point (zp)*</span>
<span class="sd">    which corresponds</span>
<span class="sd">    to the real value 0, and its data type is an integer. *zero-point* is defined as </span>

<span class="sd">        .. math::</span>

<span class="sd">           &amp;&amp; zp_f = ql_{min} -\frac{qr_{min}}{scale}, \\</span>
<span class="sd">           &amp;&amp; zp = \left\{</span>
<span class="sd">         \begin{array}{ll}</span>
<span class="sd">           ql_{max} &amp; if \ \ \ zp_f &gt;= ql_{max} \\</span>
<span class="sd">           round(zp_f) &amp; if \ \ otherwise \\</span>
<span class="sd">           ql_{min}  &amp; if \ \ zp_f &lt;= ql_{min} \\</span>
<span class="sd">         \end{array} \right..</span>

<span class="sd">    Accordingly, in order to simulate quantization effect of *zero-point*, </span>
<span class="sd">    during both forward and backward pass, :math:`qr_{min}` and :math:`qr_{max}` are adjusted as follows,</span>

<span class="sd">        .. math::</span>

<span class="sd">           qr_{min}^{adj} = ql_{min} - zp * scale, \\</span>
<span class="sd">           qr_{max}^{adj} = ql_{max} - zp * scale.</span>

<span class="sd">    These operations are often called *nudge*. </span>

<span class="sd">    Finally, in the formulas of the min-max quantization, :math:`m` and :math:`M` are replaced by</span>
<span class="sd">    :math:`qr_{min}^{adj}` and :math:`qr_{max}^{adj}` respectively.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (~nnabla.Variable): Input N-D array.</span>
<span class="sd">        ql_min (int, float, or ~nnabla.Variable): Minimum quantization level. Default is 0.</span>
<span class="sd">        ql_max (int, float, or ~nnabla.Variable): Maximum quantization level. Default is 255.</span>
<span class="sd">        decay (float): The decay rate for the exponential moving average.</span>
<span class="sd">        x_min_max (bool): Use the min and max of x to compute quantization ranges. Default is `False`.</span>
<span class="sd">        ema (bool): Use the exponential moving average for the min and max quantization ranges.</span>
<span class="sd">                    Default is `False`.</span>
<span class="sd">        ste_fine_grained (bool): If true, STE is not 1, the {0, 1}-mask computed from the min-max is applied to the gradient in the backward; otherwise, STE is 1.</span>
<span class="sd">        eps (float): Epsilon, or small value to ensure :math:`qr_{max} - qr_{min}` must be greater</span>
<span class="sd">                     than the epsilon for both weights and bias.</span>
<span class="sd">        qr_min_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for the minimum quantization range, qr_min. Default is :obj:`nnabla.initializer.ConstantInitializer` (-6.0).</span>
<span class="sd">        qr_max_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for the maximum quantization range, qr_max Default is :obj:`nnabla.initializer.ConstantInitializer` (6.0).</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>

<span class="sd">    References:</span>
<span class="sd">        Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko, &quot;Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference&quot;, https://arxiv.org/abs/1712.05877</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># ql_min and ql_max</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ql_min</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
        <span class="n">reshape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)]</span>
        <span class="n">ql_min</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ql_min</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">reshape</span><span class="p">)</span>
        <span class="n">ql_min</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;ql_min&quot;</span><span class="p">,</span> <span class="n">reshape</span><span class="p">,</span> <span class="n">ql_min</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ql_max</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
        <span class="n">reshape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)]</span>
        <span class="n">ql_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ql_max</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">reshape</span><span class="p">)</span>
        <span class="n">ql_max</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;ql_max&quot;</span><span class="p">,</span> <span class="n">reshape</span><span class="p">,</span> <span class="n">ql_max</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="c1"># qr_min and qr_max</span>
    <span class="n">qr_min_init</span> <span class="o">=</span> <span class="n">qr_min_init</span> <span class="k">if</span> <span class="n">qr_min_init</span> <span class="k">else</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="o">-</span><span class="mf">6.0</span><span class="p">)</span>
    <span class="n">qr_max_init</span> <span class="o">=</span> <span class="n">qr_max_init</span> <span class="k">if</span> <span class="n">qr_max_init</span> <span class="k">else</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mf">6.0</span><span class="p">)</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">ql_min</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">qr_min</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;qr_min&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">qr_min_init</span><span class="p">,</span> <span class="ow">not</span> <span class="p">(</span><span class="n">x_min_max</span> <span class="ow">and</span> <span class="n">ema</span><span class="p">),</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">qr_max</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;qr_max&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">qr_max_init</span><span class="p">,</span> <span class="ow">not</span> <span class="p">(</span><span class="n">x_min_max</span> <span class="ow">and</span> <span class="n">ema</span><span class="p">),</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">x_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">min_max_quantize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">qr_min</span><span class="p">,</span> <span class="n">qr_max</span><span class="p">,</span> <span class="n">ql_min</span><span class="p">,</span> <span class="n">ql_max</span><span class="p">,</span>
                             <span class="n">decay</span><span class="p">,</span> <span class="n">x_min_max</span><span class="p">,</span> <span class="n">ema</span><span class="p">,</span> <span class="n">ste_fine_grained</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_q</span></div>


<div class="viewcode-block" id="min_max_quantized_affine"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.min_max_quantized_affine">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;min_max_quantized_affine&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Weight matrix in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;W_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Quantized weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Quantized biases&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;qr_min&#39;</span><span class="p">,</span> <span class="s1">&#39;Minimum quantization range. Minimum values of inputs or trainable range.&#39;</span><span class="p">,</span>
     <span class="s1">&#39;ql_min.shape&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;qr_max&#39;</span><span class="p">,</span> <span class="s1">&#39;Maximum quantization range. Maximum values of inputs or trainable range.&#39;</span><span class="p">,</span>
     <span class="s1">&#39;ql_max.shape&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">min_max_quantized_affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span>
                             <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                             <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                             <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                             <span class="n">quantize_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ql_min_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ql_max_w</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span> <span class="n">w_min_max</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                             <span class="n">qr_min_w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">qr_max_w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                             <span class="n">ste_fine_grained_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                             <span class="n">quantize_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ql_min_b</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ql_max_b</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span> <span class="n">b_min_max</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                             <span class="n">qr_min_b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">qr_max_b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                             <span class="n">ste_fine_grained_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                             <span class="n">eps</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Min-max Quantized Affine.</span>

<span class="sd">    Min-max Quantized Affine is the affine function,</span>
<span class="sd">    except the definition of the inner product is modified.</span>
<span class="sd">    The input-output relation of this function is as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_j = \sum_{i} Q(w_{ji}) x_i,</span>

<span class="sd">    where :math:`Q(w_{ji})` is the min-max quantization function.</span>

<span class="sd">    In the min_max_quantized affine, the exponential moving average is not used. the min and max quantization</span>
<span class="sd">    ranges are either the min-max of weights and bias or trained.</span>

<span class="sd">    Notice that the min and max values of inputs are always used instead of the exponential moving average.</span>

<span class="sd">    .. note::</span>

<span class="sd">        1) if you would like to share weights between some layers, please</span>
<span class="sd">        make sure to share the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the quantized weights (`quantized weight`)</span>

<span class="sd">        2) The weights and the quantized weights become synced only after :func:`~nnabla._variable.Variable.forward` is called,</span>
<span class="sd">        and not after a call to :func:`~nnabla._variable.Variable.backward`.</span>
<span class="sd">        To access the parameters of the network, remember to call :func:`~nnabla._variable.Variable.forward` once before doing so, otherwise the</span>
<span class="sd">        float weights and the quantized weights will not be in sync.</span>

<span class="sd">        3) CPU and GPU implementations now use float value for `quantized weight`,</span>
<span class="sd">        since this function is only for simulation purposes.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): Input N-D array with shape (:math:`M_0 \times \ldots \times M_{B-1} \times D_B \times \ldots \times D_N`). Dimensions before and after base_axis are flattened as if it is a matrix.</span>
<span class="sd">        n_outmaps (:obj:`int` or :obj:`tuple` of :obj:`int`): Number of output neurons per data.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`. </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>
<span class="sd">        quantize_w (bool): Quantize weights if `True`.</span>
<span class="sd">        ql_min_w (int, float, or ~nnabla.Variable): Minimum quantization level for weights. Default is 0.</span>
<span class="sd">        ql_max_w (int, float, or ~nnabla.Variable): Maximum quantization level for weights. Default is 255.</span>
<span class="sd">        w_min_max (bool): Use the min and max of weights to compute quantization ranges. Default is `False`.</span>
<span class="sd">        qr_min_w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for the minimum quantization range, qr_min. Default is :obj:`nnabla.initializer.ConstantInitializer` (-2.0).</span>
<span class="sd">        qr_max_w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for the maximum quantization range, qr_max. Default is :obj:`nnabla.initializer.ConstantInitializer` (2.0).</span>
<span class="sd">        ste_fine_grained_w (bool): If true, STE is not 1, the {0, 1}-mask computed from the min-max is applied to the gradient in the backward; otherwise, STE is 1.</span>
<span class="sd">        quantize_b (bool): Quantize bias if `True`.</span>
<span class="sd">        ql_min_b (int, float, or ~nnabla.Variable): Minimum quantization level for bias. Default is 0.</span>
<span class="sd">        ql_max_b (int, float, or ~nnabla.Variable): Maximum quantization level for bias. Default is 255.</span>
<span class="sd">        b_min_max (bool): Use the min and max of bias to compute quantization ranges. Default is `False`.</span>
<span class="sd">        qr_min_b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for the minimum quantization range, qr_min. Default is :obj:`nnabla.initializer.ConstantInitializer` (-6.0).</span>
<span class="sd">        qr_max_b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for the maximum quantization range, qr_max. Default is :obj:`nnabla.initializer.ConstantInitializer` (6.0).</span>
<span class="sd">        ste_fine_grained_b (bool): If true, STE is not 1, the {0, 1}-mask computed from the min-max is applied to the gradient in the backward; otherwise, STE is 1.</span>
<span class="sd">        eps (float): Epsilon, or small value to ensure :math:`qr_{max} - qr_{min}` must be greater</span>
<span class="sd">                     than the epsilon for both weights and bias.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: :math:`(B + 1)`-D array. (:math:`M_0 \times \ldots \times M_{B-1} \times L`)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">,</span> <span class="s1">&#39;__iter__&#39;</span><span class="p">):</span>
        <span class="n">n_outmaps</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_outmaps</span><span class="p">]</span>
    <span class="n">n_outmaps</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">)</span>
    <span class="n">n_outmap</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inmaps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inmaps</span><span class="p">,</span> <span class="n">n_outmap</span><span class="p">),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">qr_min_w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">qr_min_w_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">qr_max_w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">qr_max_w_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">qr_min_b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">qr_min_b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="o">-</span><span class="mf">6.0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">qr_max_b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">qr_max_b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mf">6.0</span><span class="p">)</span>

    <span class="c1"># Floating Weight</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="c1"># Quantized Weight</span>
    <span class="k">if</span> <span class="n">quantize_w</span><span class="p">:</span>
        <span class="n">w_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;W_q&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
            <span class="n">w_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="c1"># Link computation graph</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">min_max_quantize</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">ql_min_w</span><span class="p">,</span> <span class="n">ql_max_w</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="n">w_min_max</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
                                    <span class="n">qr_min_init</span><span class="o">=</span><span class="n">qr_min_w_init</span><span class="p">,</span> <span class="n">qr_max_init</span><span class="o">=</span><span class="n">qr_max_w_init</span><span class="p">,</span>
                                    <span class="n">ste_fine_grained</span><span class="o">=</span><span class="n">ste_fine_grained_w</span><span class="p">,</span>
                                    <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
                                    <span class="n">fix_parameters</span><span class="o">=</span><span class="n">fix_parameters</span><span class="p">,</span>
                                    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">w_q</span><span class="o">.</span><span class="n">data</span><span class="p">],</span>
                                    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;min_max_quantize_w&quot;</span><span class="p">)</span>
        <span class="n">real_w_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">w</span>

    <span class="c1"># Bias</span>
    <span class="c1"># Floating</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">b_q</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">real_b_q</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">quantize_b</span><span class="p">:</span>
            <span class="n">b_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
                <span class="s2">&quot;b_q&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="c1"># Link computation graph</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">min_max_quantize</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">ql_min_b</span><span class="p">,</span> <span class="n">ql_max_b</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="n">b_min_max</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
                                        <span class="n">qr_min_init</span><span class="o">=</span><span class="n">qr_min_b_init</span><span class="p">,</span> <span class="n">qr_max_init</span><span class="o">=</span><span class="n">qr_max_b_init</span><span class="p">,</span>
                                        <span class="n">ste_fine_grained</span><span class="o">=</span><span class="n">ste_fine_grained_b</span><span class="p">,</span>
                                        <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
                                        <span class="n">fix_parameters</span><span class="o">=</span><span class="n">fix_parameters</span><span class="p">,</span>
                                        <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">b_q</span><span class="o">.</span><span class="n">data</span><span class="p">],</span>
                                        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;min_max_quantize_b&quot;</span><span class="p">)</span>
            <span class="n">real_b_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">b</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">real_w_q</span><span class="p">,</span> <span class="n">real_b_q</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="min_max_quantized_convolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.min_max_quantized_convolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;min_max_quantized_conv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps // group, *kernel)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;W_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Quantized weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps // group, *kernel)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Quantized biases&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;qr_min&#39;</span><span class="p">,</span> <span class="s1">&#39;Minimum quantization range. Minimum values of inputs or trainable range.&#39;</span><span class="p">,</span>
     <span class="s1">&#39;ql_min.shape&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;qr_max&#39;</span><span class="p">,</span> <span class="s1">&#39;Maximum quantization range. Maximum values of inputs or trainable range.&#39;</span><span class="p">,</span>
     <span class="s1">&#39;ql_max.shape&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">min_max_quantized_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span>
                                  <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">channel_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                  <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                  <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                  <span class="n">quantize_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ql_min_w</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ql_max_w</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span> <span class="n">w_min_max</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                  <span class="n">qr_min_w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">qr_max_w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                  <span class="n">ste_fine_grained_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                  <span class="n">quantize_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ql_min_b</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ql_max_b</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span> <span class="n">b_min_max</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                  <span class="n">qr_min_b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">qr_max_b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                  <span class="n">ste_fine_grained_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                  <span class="n">eps</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Min-max Quantized Convolution.</span>

<span class="sd">    Min-max Quantized Convolution is the convolution function,</span>
<span class="sd">    except the definition of the inner product is modified.</span>
<span class="sd">    The input-output relation of this function is as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_{n, a, b} = \sum_{m} \sum_{i} \sum_{j} Q(w_{n, m, i, j}) x_{m, a + i, b + j},</span>

<span class="sd">    where :math:`Q(w_{n, m, i, j})` is the min-max quantization function.</span>

<span class="sd">    In the min_max_quantized convolution, the exponential moving average is not used.</span>
<span class="sd">    the min and max quantization ranges are either the min-max of weights and bias or trained.</span>

<span class="sd">    Notice that the min and max values of inputs are always used instead of the exponential moving average.</span>

<span class="sd">    .. note::</span>

<span class="sd">        1) if you would like to share weights between some layers, please</span>
<span class="sd">        make sure to share the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the quantized weights (`quantized weight`)</span>

<span class="sd">        2) The weights and the quantized weights become synced only after :func:`~nnabla._variable.Variable.forward` is called,</span>
<span class="sd">        and not after a call to :func:`~nnabla._variable.Variable.backward`.</span>
<span class="sd">        To access the parameters of the network, remember to call :func:`~nnabla._variable.Variable.forward` once before doing so, otherwise the</span>
<span class="sd">        float weights and the quantized weights will not be in sync.</span>

<span class="sd">        3) CPU and GPU implementations now use float value for `quantized weight`,</span>
<span class="sd">        since this function is only for simulation purposes.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array.</span>
<span class="sd">        outmaps (int): Number of convolution kernels (which is equal to the number of output channels). For example, to apply convolution on an input with 16 types of filters, specify 16.</span>
<span class="sd">        kernel (:obj:`tuple` of :obj:`int`): Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</span>
<span class="sd">        pad (:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">        stride (:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">        dilation (:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">        group (int): Number of groups of channels. This makes connections across channels more sparse by grouping connections along map direction.</span>
<span class="sd">        channel_last (bool): If True, the last dimension is considered as channel dimension, a.k.a. NHWC order.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.  </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>
<span class="sd">        quantize_w (bool): Quantize weights if `True`.</span>
<span class="sd">        ql_min_w (int, float, or ~nnabla.Variable): Minimum quantization level for weights. Default is 0.</span>
<span class="sd">        ql_max_w (int, float, or ~nnabla.Variable): Maximum quantization level for weights. Default is 255.</span>
<span class="sd">        w_min_max (bool): Use the min and max of weights to compute quantization ranges. Default is `False`.</span>
<span class="sd">        qr_min_w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for the minimum quantization range, qr_min. Default is :obj:`nnabla.initializer.ConstantInitializer` (-2.0).</span>
<span class="sd">        qr_max_w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for the maximum quantization range, qr_max Default is :obj:`nnabla.initializer.ConstantInitializer` (2.0).</span>
<span class="sd">        ste_fine_grained_w (bool): If true, STE is not 1, the {0, 1}-mask computed from the min-max is applied to the gradient in the backward; otherwise, STE is 1.</span>
<span class="sd">        quantize_b (bool): Quantize bias if `True`.</span>
<span class="sd">        ql_min_b (int, float, or ~nnabla.Variable): Minimum quantization level for bias. Default is 0.</span>
<span class="sd">        ql_max_b (int, float, or ~nnabla.Variable): Maximum quantization level for bias. Default is 255.</span>
<span class="sd">        b_min_max (bool): Use the min and max of bias to compute quantization ranges. Default is `False`.</span>
<span class="sd">        qr_min_b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for the minimum quantization range, qr_min. Default is :obj:`nnabla.initializer.ConstantInitializer` (-6.0).</span>
<span class="sd">        qr_max_b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for the maximum quantization range, qr_max Default is :obj:`nnabla.initializer.ConstantInitializer` (6.0).</span>
<span class="sd">        ste_fine_grained_b (bool): If true, STE is not 1, the {0, 1}-mask computed from the min-max is applied to the gradient in the backward; otherwise, STE is 1.</span>
<span class="sd">        eps (float): Epsilon, or small value to ensure :math:`qr_{max} - qr_{min}` must be greater</span>
<span class="sd">                     than the epsilon for both weights and bias.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: N-D array.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">channel_last</span><span class="p">:</span>
        <span class="n">channels</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">filter_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">channels</span> <span class="o">//</span> <span class="n">group</span><span class="p">,)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">channels</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span>
        <span class="n">filter_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">channels</span> <span class="o">//</span> <span class="n">group</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">qr_min_w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">qr_min_w_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">qr_max_w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">qr_max_w_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">qr_min_b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">qr_min_b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="o">-</span><span class="mf">6.0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">qr_max_b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">qr_max_b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mf">6.0</span><span class="p">)</span>

    <span class="c1"># Floating Weight</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,)</span> <span class="o">+</span> <span class="n">filter_shape</span><span class="p">,</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="c1"># Quantized Weight</span>
    <span class="k">if</span> <span class="n">quantize_w</span><span class="p">:</span>
        <span class="n">w_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;W_q&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,)</span> <span class="o">+</span> <span class="n">filter_shape</span><span class="p">,</span>
            <span class="n">w_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="c1"># Link computation graph</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">min_max_quantize</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">ql_min_w</span><span class="p">,</span> <span class="n">ql_max_w</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="n">w_min_max</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
                                    <span class="n">qr_min_init</span><span class="o">=</span><span class="n">qr_min_w_init</span><span class="p">,</span> <span class="n">qr_max_init</span><span class="o">=</span><span class="n">qr_max_w_init</span><span class="p">,</span>
                                    <span class="n">ste_fine_grained</span><span class="o">=</span><span class="n">ste_fine_grained_w</span><span class="p">,</span>
                                    <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
                                    <span class="n">fix_parameters</span><span class="o">=</span><span class="n">fix_parameters</span><span class="p">,</span>
                                    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">w_q</span><span class="o">.</span><span class="n">data</span><span class="p">],</span>
                                    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;min_max_quantize_w&quot;</span><span class="p">)</span>
        <span class="n">real_w_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">w</span>

    <span class="c1"># Bias</span>
    <span class="c1"># Floating</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">b_q</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">real_b_q</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">quantize_b</span><span class="p">:</span>
            <span class="n">b_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
                <span class="s2">&quot;b_q&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="c1"># Link computation graph</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">min_max_quantize</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">ql_min_b</span><span class="p">,</span> <span class="n">ql_max_b</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="n">b_min_max</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span>
                                        <span class="n">qr_min_init</span><span class="o">=</span><span class="n">qr_min_b_init</span><span class="p">,</span> <span class="n">qr_max_init</span><span class="o">=</span><span class="n">qr_max_b_init</span><span class="p">,</span>
                                        <span class="n">ste_fine_grained</span><span class="o">=</span><span class="n">ste_fine_grained_b</span><span class="p">,</span>
                                        <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
                                        <span class="n">fix_parameters</span><span class="o">=</span><span class="n">fix_parameters</span><span class="p">,</span>
                                        <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">b_q</span><span class="o">.</span><span class="n">data</span><span class="p">],</span>
                                        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;min_max_quantize_b&quot;</span><span class="p">)</span>
            <span class="n">real_b_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">b</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">real_w_q</span><span class="p">,</span> <span class="n">real_b_q</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">channel_last</span><span class="p">)</span></div>


<div class="viewcode-block" id="lstm_cell"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.lstm_cell">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;affine/W&#39;</span><span class="p">,</span> <span class="s1">&#39;Stacked weight matrixes of LSTM block&#39;</span><span class="p">,</span>
     <span class="s1">&#39;(inmaps, 4, state_size)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;affine/b&#39;</span><span class="p">,</span> <span class="s1">&#39;Stacked bias vectors of LSTM block&#39;</span><span class="p">,</span> <span class="s1">&#39;(4, state_size,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">lstm_cell</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">state_size</span><span class="p">,</span> <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Long Short-Term Memory.</span>

<span class="sd">    Long Short-Term Memory, or LSTM, is a building block for recurrent neural networks (RNN) layers.</span>
<span class="sd">    LSTM unit consists of a cell and input, output, forget gates whose functions are defined as following:</span>

<span class="sd">    .. math::</span>
<span class="sd">        f_t&amp;&amp;=\\sigma(W_fx_t+U_fh_{t-1}+b_f) \\\\</span>
<span class="sd">        i_t&amp;&amp;=\\sigma(W_ix_t+U_ih_{t-1}+b_i) \\\\</span>
<span class="sd">        o_t&amp;&amp;=\\sigma(W_ox_t+U_oh_{t-1}+b_o) \\\\</span>
<span class="sd">        c_t&amp;&amp;=f_t\\odot c_{t-1}+i_t\\odot\\tanh(W_cx_t+U_ch_{t-1}+b_c) \\\\</span>
<span class="sd">        h_t&amp;&amp;=o_t\\odot\\tanh(c_t).</span>

<span class="sd">    References:</span>

<span class="sd">        S. Hochreiter, and J. Schmidhuber. &quot;Long Short-Term Memory.&quot;</span>
<span class="sd">        Neural Computation. 1997.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (~nnabla.Variable): Input N-D array with shape (batch_size, input_size).</span>
<span class="sd">        h (~nnabla.Variable): Input N-D array with shape (batch_size, state_size).</span>
<span class="sd">        c (~nnabla.Variable): Input N-D array with shape (batch_size, state_size).</span>
<span class="sd">        state_size (int): Internal state size is set to `state_size`.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`, optional): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.  </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`, optional): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">xh</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">iofc</span> <span class="o">=</span> <span class="n">affine</span><span class="p">(</span><span class="n">xh</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">state_size</span><span class="p">),</span> <span class="n">w_init</span><span class="o">=</span><span class="n">w_init</span><span class="p">,</span>
                  <span class="n">b_init</span><span class="o">=</span><span class="n">b_init</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">i_t</span><span class="p">,</span> <span class="n">o_t</span><span class="p">,</span> <span class="n">f_t</span><span class="p">,</span> <span class="n">gate</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">iofc</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">c_t</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">f_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">c</span> <span class="o">+</span> <span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">i_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">F</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">gate</span><span class="p">)</span>
    <span class="n">h_t</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">o_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">F</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">c_t</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">h_t</span><span class="p">,</span> <span class="n">c_t</span></div>


<div class="viewcode-block" id="LSTMCell"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.LSTMCell">[docs]</a><span class="k">class</span> <span class="nc">LSTMCell</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">state_size</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an LSTM cell.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_size (int): Internal batch size is set to `batch_size`.</span>
<span class="sd">            state_size (int): Internal state size is set to `state_size`.</span>
<span class="sd">            h (~nnabla.Variable): Input N-D array with shape (batch_size, state_size). If not specified, it is initialized to zero by default.</span>
<span class="sd">            c (~nnabla.Variable): Input N-D array with shape (batch_size, state_size). If not specified, it is initialized to zero by default.</span>
<span class="sd">            name (str): Name for this LSTM Cell.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state_size</span> <span class="o">=</span> <span class="n">state_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="k">if</span> <span class="n">h</span><span class="p">:</span>  <span class="c1"># when user defines h</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">h</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_size</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">c</span><span class="p">:</span>  <span class="c1"># when user defines c</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">c</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_size</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">c</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Resets states h and c to zero.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>

<div class="viewcode-block" id="LSTMCell.__call__"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.LSTMCell.__call__">[docs]</a>    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Updates h and c by calling lstm function.</span>

<span class="sd">        Args:</span>
<span class="sd">            x (~nnabla.Variable): Input N-D array with shape (batch_size, input_size).</span>
<span class="sd">            w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`, optional): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.  </span>
<span class="sd">            b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`, optional): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">            fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">lstm_cell</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_size</span><span class="p">,</span>
            <span class="n">w_init</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span>
            <span class="n">fix_parameters</span><span class="o">=</span><span class="n">fix_parameters</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span></div></div>


<div class="viewcode-block" id="spectral_norm"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.spectral_norm">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;spectral-norm&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;u&#39;</span><span class="p">,</span> <span class="s1">&#39;singular vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(w.shape[dim], )&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">spectral_norm</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">itr</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">u_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Spectral Normalization.</span>

<span class="sd">    .. math::</span>

<span class="sd">        W_{sn} = \\frac{W}{\\sigma(W)}.</span>

<span class="sd">    where :math:`W` is the input matrix, and the :math:`\\sigma(W)` is the spectral norm of :math:`W`. The spectral norm is approximately computed by the power iteration.</span>

<span class="sd">    References:</span>

<span class="sd">        Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida, </span>
<span class="sd">        &quot;Spectral Normalization for Generative Adversarial Networks&quot;, </span>
<span class="sd">        International Conference on Learning Representations. 2018.</span>

<span class="sd">    Args:</span>
<span class="sd">        W (~nnabla.Variable): Input N-D array with shape. This is normally network parameter.</span>
<span class="sd">        dim (`int`): Output dimension. Default is 0. If the dimension is not 0, then the specified dimension becomes the most-left dimension by transposing.</span>
<span class="sd">        itr (`int`): Number of iterations. Default is 1.</span>
<span class="sd">        eps (`float`): Epsilon for the normalization. Default is 1e-12.</span>
<span class="sd">        test (`bool`): Use test mode. Default is False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Spectrally normalized :math:`W_{sn}` with the same shape as :math:`W`.</span>

<span class="sd">    Example:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            import nnabla as nn</span>
<span class="sd">            import nnabla.parametric_functions as PF</span>

<span class="sd">            b, c, h, w = 4, 64, 32, 32</span>

<span class="sd">            # Spectrally normalized convolution</span>
<span class="sd">            apply_w = lambda w: PF.spectral_norm(w, dim=0)</span>
<span class="sd">            h = nn.Variable.from_numpy_array(np.random.randn(b, c, h, w))</span>
<span class="sd">            h = PF.convolution(h, with_bias=False, apply_w=apply_w)</span>

<span class="sd">            # Spectrally normalized affine</span>
<span class="sd">            apply_w = lambda w: PF.spectral_norm(w, dim=1)</span>
<span class="sd">            h = nn.Variable.from_numpy_array(np.random.randn(b, c))</span>
<span class="sd">            h = PF.affine(h, with_bias=False, apply_w=apply_w)</span>

<span class="sd">            # Spectrally normalized embed</span>
<span class="sd">            apply_w = lambda w: PF.spectral_norm(w, dim=1)</span>
<span class="sd">            h = nn.Variable.from_numpy_array(np.random.randn(b, c))</span>
<span class="sd">            h = PF.embed(h, c, apply_w=apply_w)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">assert</span> <span class="p">(</span><span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">dim</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="p">),</span> <span class="s2">&quot;`dim` must be `0 &lt;= dim and dim &lt; len(w.shape)`.&quot;</span>

    <span class="k">if</span> <span class="n">u_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">u_init</span> <span class="o">=</span> <span class="n">NormalInitializer</span><span class="p">()</span>
    <span class="n">u_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">],)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span><span class="s2">&quot;u&quot;</span><span class="p">,</span> <span class="n">u_shape</span><span class="p">,</span> <span class="n">u_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">spectral_norm</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">itr</span><span class="o">=</span><span class="n">itr</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="n">test</span><span class="p">)</span></div>


<span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;spectral-norm&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W_sn&#39;</span><span class="p">,</span> <span class="s1">&#39;Spectral Normalized Weight matrix&#39;</span><span class="p">,</span> <span class="s1">&#39;w.shape&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;u&#39;</span><span class="p">,</span> <span class="s1">&#39;singular vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(w.shape[dim], )&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">_spectral_norm_v1</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">itr</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">u_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Spectral Normalization.</span>

<span class="sd">    .. math::</span>

<span class="sd">        W_{sn} = \\frac{W}{\\sigma(W)}.</span>

<span class="sd">    where :math:`W` is the input matrix, and the :math:`\\sigma(W)` is the spectral norm of :math:`W`. The spectral norm is approximately computed by the power iteration.</span>

<span class="sd">    References:</span>

<span class="sd">        Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida, </span>
<span class="sd">        &quot;Spectral Normalization for Generative Adversarial Networks&quot;, </span>
<span class="sd">        International Conference on Learning Representations. 2018.</span>

<span class="sd">    Args:</span>
<span class="sd">        W (~nnabla.Variable): Input N-D array with shape. This is normally network parameter.</span>
<span class="sd">        dim (`int`): Output dimension. Default is 0. If the dimension is not 0, then the specified dimension becomes the most-left dimension by transposing.</span>
<span class="sd">        itr (`int`): Number of iterations. Default is 1.</span>
<span class="sd">        eps (`float`): Epsilon for the normalization. Default is 1e-12.</span>
<span class="sd">        test (`bool`): Use test mode. Default is False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Spectrally normalized :math:`W_{sn}` with the same shape as :math:`W`.</span>

<span class="sd">    Example:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            import nnabla as nn</span>
<span class="sd">            import nnabla.parametric_functions as PF</span>

<span class="sd">            b, c, h, w = 4, 64, 32, 32</span>

<span class="sd">            # Spectrally normalized convolution</span>
<span class="sd">            apply_w = lambda w: PF.spectral_norm(w, dim=0)</span>
<span class="sd">            h = nn.Variable.from_numpy_array(np.random.randn(b, c, h, w))</span>
<span class="sd">            h = PF.convolution(h, with_bias=False, apply_w=apply_w)</span>

<span class="sd">            # Spectrally normalized affine</span>
<span class="sd">            apply_w = lambda w: PF.spectral_norm(w, dim=1)</span>
<span class="sd">            h = nn.Variable.from_numpy_array(np.random.randn(b, c))</span>
<span class="sd">            h = PF.affine(h, with_bias=False, apply_w=apply_w)</span>

<span class="sd">            # Spectrally normalized embed</span>
<span class="sd">            apply_w = lambda w: PF.spectral_norm(w, dim=1)</span>
<span class="sd">            h = nn.Variable.from_numpy_array(np.random.randn(b, c))</span>
<span class="sd">            h = PF.embed(h, c, apply_w=apply_w)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">assert</span> <span class="p">(</span><span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">dim</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="p">),</span> <span class="s2">&quot;`dim` must be `0 &lt;= dim and dim &lt; len(w.shape)`.&quot;</span>
    <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">itr</span><span class="p">,</span> <span class="s2">&quot;`itr` must be greater than 0.&quot;</span>
    <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">,</span> <span class="s2">&quot;`eps` must be greater than 0.&quot;</span>

    <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">w_sn</span> <span class="o">=</span> <span class="n">_spectral_norm_outer_most_dim</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">itr</span><span class="o">=</span><span class="n">itr</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="n">test</span><span class="p">,</span>
                                             <span class="n">u_init</span><span class="o">=</span><span class="n">u_init</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">w_sn</span> <span class="o">=</span> <span class="n">_spectral_norm</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">itr</span><span class="o">=</span><span class="n">itr</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="n">test</span><span class="p">,</span>
                              <span class="n">u_init</span><span class="o">=</span><span class="n">u_init</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">w_sn</span>


<span class="k">def</span> <span class="nf">_spectral_norm</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">itr</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">u_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="c1"># Use the original shape for W_sn</span>
    <span class="n">w_shape</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">W_sn</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W_sn&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">,</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="kc">False</span><span class="p">)</span>
    <span class="c1"># Transpose if the output dimension is not the most-left dimension.</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">dims_transpose</span> <span class="o">=</span> <span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w_shape</span><span class="p">))</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">dim</span><span class="p">]</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dims_transpose</span><span class="p">)</span>
        <span class="n">w_shape</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">d0</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>            <span class="c1"># Out</span>
    <span class="n">d1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>  <span class="c1"># In</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="p">[</span><span class="n">d0</span><span class="p">,</span> <span class="n">d1</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">u_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">u_init</span> <span class="o">=</span> <span class="n">NormalInitializer</span><span class="p">()</span>
    <span class="n">u0</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span><span class="s2">&quot;u&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">d0</span><span class="p">],</span> <span class="n">u_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">u0</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">d0</span><span class="p">])</span>

    <span class="c1"># Ensure both parameters (W_sn and u) exist when the test is called fast.</span>
    <span class="k">if</span> <span class="n">test</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">W_sn</span>
    <span class="c1"># Power method</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">itr</span><span class="p">):</span>
        <span class="c1"># v</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span> <span class="o">/</span> <span class="p">((</span><span class="n">F</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">v</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="p">[</span><span class="n">d1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="c1"># u</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">u</span> <span class="o">/</span> <span class="p">((</span><span class="n">F</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">u</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">d0</span><span class="p">])</span>
    <span class="c1"># Iterate</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">u0</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
    <span class="n">u</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="c1"># No grad</span>
    <span class="n">u</span><span class="o">.</span><span class="n">need_grad</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">v</span><span class="o">.</span><span class="n">need_grad</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># Spectral normalization</span>
    <span class="n">wv</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">wv</span><span class="p">)</span>
    <span class="n">w_sn</span> <span class="o">=</span> <span class="n">w</span> <span class="o">/</span> <span class="n">sigma</span>
    <span class="n">w_sn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">w_sn</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">)</span>
    <span class="c1"># Transpose again if the output dimension is not the most-left dimension.</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">dims_transpose</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span> \
                         <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">w_shape</span><span class="p">))]</span>
        <span class="n">w_sn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">w_sn</span><span class="p">,</span> <span class="n">dims_transpose</span><span class="p">)</span>
    <span class="n">w_sn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">w_sn</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">W_sn</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
    <span class="n">w_sn</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">w_sn</span>


<span class="k">def</span> <span class="nf">_spectral_norm_outer_most_dim</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">itr</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                  <span class="n">u_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">w_shape</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">W_sn</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W_sn&quot;</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">d0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># In</span>
    <span class="n">d1</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>             <span class="c1"># Out</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="p">[</span><span class="n">d0</span><span class="p">,</span> <span class="n">d1</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">u_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">u_init</span> <span class="o">=</span> <span class="n">NormalInitializer</span><span class="p">()</span>
    <span class="n">u0</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span><span class="s2">&quot;u&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">d1</span><span class="p">],</span> <span class="n">u_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">u0</span><span class="p">,</span> <span class="p">[</span><span class="n">d1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Ensure both parameters (W_sn and u) exist when the test is called fast.</span>
    <span class="k">if</span> <span class="n">test</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">W_sn</span>

    <span class="c1"># Power method</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">itr</span><span class="p">):</span>
        <span class="c1"># v</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span> <span class="o">/</span> <span class="p">((</span><span class="n">F</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">v</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">d0</span><span class="p">])</span>
        <span class="c1"># u</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">u</span> <span class="o">/</span> <span class="p">((</span><span class="n">F</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">u</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="p">[</span><span class="n">d1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="c1"># Iterate</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">u0</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
    <span class="n">u</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="c1"># No grad</span>
    <span class="n">u</span><span class="o">.</span><span class="n">need_grad</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">v</span><span class="o">.</span><span class="n">need_grad</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># Spectral normalization</span>
    <span class="n">wv</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">wv</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span>
    <span class="n">w_sn</span> <span class="o">=</span> <span class="n">w</span> <span class="o">/</span> <span class="n">sigma</span>
    <span class="n">w_sn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">w_sn</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">)</span>
    <span class="n">w_sn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">w_sn</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">W_sn</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
    <span class="n">w_sn</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">w_sn</span>


<div class="viewcode-block" id="weight_normalization"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.weight_normalization">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;wn&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="s1">&#39;Weight Normalization adaptive scale scalar.&#39;</span><span class="p">,</span> <span class="s1">&#39;w.shape[dim]&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">weight_normalization</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span> <span class="n">g_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Weight Normalization.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \mathbf{w}_{WN} = g \dfrac{\mathbf{w}}{\|\mathbf{w}\|}</span>

<span class="sd">    where :math:`\mathbf{w}` is the input weights to be normalized, </span>
<span class="sd">    and :math:`g` is learnable multiplication factors each of which is applied to each input weights at `dim`.</span>
<span class="sd">    This function is in general used as callback passed to apply_w for PF.convolution, PF.affine and so on.</span>
<span class="sd">    According to the author`s `original implementation &lt;https://github.com/TimSalimans/weight_norm&gt;`_, :math:`v` should be initialized by :math:`N(0, 0.05)`.</span>
<span class="sd">    To meet this condition, initializer should be passed to convolution which Weight Normalization is applied, like an example below.</span>


<span class="sd">    References:</span>
<span class="sd">        * `Tim Salimans, Diederik P. Kingma, Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1602.07868&gt;`_</span>

<span class="sd">    Args:</span>
<span class="sd">        W (~nnabla.Variable): Input N-D array with shape. This is normally network parameter.</span>
<span class="sd">        dim (`int`):</span>
<span class="sd">            Output dimension. Default is 0.</span>
<span class="sd">            If the dimension is not 0, then the specified dimension becomes the most-left dimension by transposing.</span>
<span class="sd">        eps (`float`): Epsilon for the normalization. Default is 1e-12.</span>
<span class="sd">        g_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for the scale. By default, L2-norm of weights corresponding to `dim` are used.</span>


<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable:  :math:`W` with the same shape as :math:`v`.</span>

<span class="sd">    Example:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            import nnabla as nn</span>
<span class="sd">            import nnabla.parametric_functions as PF</span>
<span class="sd">            import nnabla.initializer as I</span>

<span class="sd">            # h is nn.Variable.</span>

<span class="sd">            # convolution</span>
<span class="sd">            # according to the original implementation, w should be initialized by N(0, 0.05).</span>
<span class="sd">            h = PF.convolution(h, ..., apply_w=PF.weight_normalization, w_init=I.NormalInitializer(0.05))</span>

<span class="sd">            # affine</span>
<span class="sd">            h = PF.affine(h, ..., apply_w=lambda w: PF.weight_normalization(w, dim=1), w_init=I.NormalInitializer(0.05))</span>

<span class="sd">    .. warning::</span>
<span class="sd">       Up to the version 1.10.0, this had been implemented as the composite functions.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">outmaps</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">g_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">g_init</span> <span class="o">=</span> <span class="n">WeightNormalizationScaleInitializer</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span><span class="s2">&quot;g&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span>
                                <span class="n">initializer</span><span class="o">=</span><span class="n">g_init</span><span class="p">,</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">as_need_grad</span><span class="o">=</span><span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">weight_normalization</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span></div>


<span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;wn&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="s1">&#39;Weight Normalization adaptive scale scalar.&#39;</span><span class="p">,</span> <span class="s1">&#39;w.shape[dim]&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">_weight_normalization_v1</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Weight Normalization.</span>

<span class="sd">    This functions is of the composite functions. It takes a lots of memories since the intermediate results</span>
<span class="sd">    are stored as a part of the computation graph.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|}</span>

<span class="sd">    where :math:`v` is the input matrix,</span>
<span class="sd">    and :math:`g` is learnable multiplication factors each of which is applied to each output map at `dim`.</span>
<span class="sd">    This function is in general used as callback passed to apply_w for PF.convolution, PF.affine and so on.</span>
<span class="sd">    According to the author`s original implementation (https://github.com/TimSalimans/weight_norm), :math:`v` should be initialized by :math:`N(0, 0.05)`.</span>
<span class="sd">    To meet this condition, initializer should be passed to convolution which Weight Normalization is applied, like an example below.</span>


<span class="sd">    References:</span>
<span class="sd">        * `Tim Salimans, Diederik P. Kingma, Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1602.07868&gt;`_</span>

<span class="sd">    Args:</span>
<span class="sd">        W (~nnabla.Variable): Input N-D array with shape. This is normally network parameter.</span>
<span class="sd">        dim (`int`):</span>
<span class="sd">            Output dimension. Default is 0.</span>
<span class="sd">            If the dimension is not 0, then the specified dimension becomes the most-left dimension by transposing.</span>
<span class="sd">        eps (`float`): Epsilon for the normalization. Default is 1e-12.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable:  :math:`W` with the same shape as :math:`v`.</span>

<span class="sd">    Example:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            import nnabla as nn</span>
<span class="sd">            import nnabla.parametric_functions as PF</span>
<span class="sd">            import nnabla.initializer as I</span>

<span class="sd">            # h is nn.Variable.</span>

<span class="sd">            # convolution</span>
<span class="sd">            # according to the original implementation, w should be initialized by N(0, 0.05).</span>
<span class="sd">            h = PF.convolution(h, ..., apply_w=PF.weight_normalization, w_init=I.NormalInitializer(0.05))</span>

<span class="sd">            # affine</span>
<span class="sd">            h = PF.affine(h, ..., apply_w=lambda w: PF.weight_normalization(w, dim=1), w_init=I.NormalInitializer(0.05))</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="o">-</span> \
        <span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">dim</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span>
            <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="s2">&quot;`dim` must be `-len(w.shape) &lt;= dim &lt; len(w.shape)`.&quot;</span>
    <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">,</span> <span class="s2">&quot;`eps` must be greater than 0.&quot;</span>

    <span class="c1"># consider w as v.</span>

    <span class="n">outmaps</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span><span class="s2">&quot;g&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span>
                                <span class="n">initializer</span><span class="o">=</span><span class="n">ConstantInitializer</span><span class="p">(</span><span class="mf">1.</span><span class="p">),</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">as_need_grad</span><span class="o">=</span><span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="n">sh</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">outmaps</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">dim</span> <span class="k">else</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">))])</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">dim</span><span class="p">])</span>

    <span class="n">normalized_v</span> <span class="o">=</span> <span class="n">v</span> <span class="o">/</span> <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">v</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">sh</span><span class="p">)</span> <span class="o">*</span> <span class="n">normalized_v</span>


<div class="viewcode-block" id="multi_head_attention"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.multi_head_attention">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;multi_head_attention&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;q_weight&#39;</span><span class="p">,</span> <span class="s1">&#39;weights for query&#39;</span><span class="p">,</span> <span class="s1">&#39;(E, E)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;k_weight&#39;</span><span class="p">,</span> <span class="s1">&#39;weights for key&#39;</span><span class="p">,</span> <span class="s1">&#39;(E_k, E)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;v_weight&#39;</span><span class="p">,</span> <span class="s1">&#39;weights for value&#39;</span><span class="p">,</span> <span class="s1">&#39;(E_v, E)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;out_weight&#39;</span><span class="p">,</span> <span class="s1">&#39;weigths for out projection&#39;</span><span class="p">,</span> <span class="s1">&#39;(E, E)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;q_bias&#39;</span><span class="p">,</span> <span class="s1">&#39;bias for query&#39;</span><span class="p">,</span> <span class="s1">&#39;(E, )&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;k_bias&#39;</span><span class="p">,</span> <span class="s1">&#39;bias for key&#39;</span><span class="p">,</span> <span class="s1">&#39;(E, )&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;v_bias&#39;</span><span class="p">,</span> <span class="s1">&#39;bais for value&#39;</span><span class="p">,</span> <span class="s1">&#39;(E, )&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;out_bias&#39;</span><span class="p">,</span> <span class="s1">&#39;bias for out projection&#39;</span><span class="p">,</span> <span class="s1">&#39;(E, )&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;attn_bias_k&#39;</span><span class="p">,</span> <span class="s1">&#39;attnetion bias for k&#39;</span><span class="p">,</span> <span class="s1">&#39;(E, 1)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;attn_bias_v&#39;</span><span class="p">,</span> <span class="s1">&#39;attnetion bias for v&#39;</span><span class="p">,</span> <span class="s1">&#39;(E, 1)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">multi_head_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">k_embed_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">v_embed_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">out_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_attn_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">additive_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">param_init</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;MultiHeadAttention.</span>

<span class="sd">    Computes multi-headed attention with query, key, and value.</span>
<span class="sd">    We use the following notations to describe the inputs and outputs below.</span>
<span class="sd">    :math:`L_T`: target sequence length, :math:`L_S`: source sequence length, :math:`B`: batch size, :math:`D`: input dimension, :math:`E`: embedding dimension.</span>

<span class="sd">    References:</span>

<span class="sd">        A. Vaswani et al. &quot;Attention is All You Need.&quot;</span>
<span class="sd">        NIPS. 2017.</span>
<span class="sd">        &lt;https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&gt;</span>

<span class="sd">    Example:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        q = nn.Variable((tgt_len, batch_size, q_input_dim))</span>
<span class="sd">        k = nn.Variable((src_len, batch_size, k_input_dim))</span>
<span class="sd">        v = nn.Variable((src_len, batch_size, v_input_dim))</span>

<span class="sd">        out, w = PF.multi_head_attention(q, k, v)</span>
<span class="sd">        out.forward()</span>

<span class="sd">    Args:</span>
<span class="sd">        query (~nnabla.Variable): Input N-D array with shape :math:`(L_T, B, D_q)`.</span>
<span class="sd">        key (~nnabla.Variable): Input N-D array with shape :math:`(L_S, B, D_k)`.</span>
<span class="sd">        value (~nnabla.Variable): Input N-D array with shape :math:`(L_S, B, D_v)`.</span>
<span class="sd">        num_heads (int, optional): Number of attention heads. Note that embedding dimensoin E must be divisible by the number of heads. Default is 12 which is conventional.</span>
<span class="sd">        dropout (float, optional): Dropout ratio applied to parameters. Default is 0.</span>
<span class="sd">        k_embed_dim (int, optional): Embedding dimension for key. If specified, embedding dimensions for both query and key are set as that value. Otherwise, k_embed_dim is set as the same alue as embedding dimension for query.</span>
<span class="sd">        v_embed_dim (int, optional): Embedding dimension for value. If not specified, it is defaulted as the same value as embedding dimension for query.</span>
<span class="sd">        out_dim (int, optional): Embedding dimension for output weight. If not spefied, it is defaulted as the same value as embedding dimension for value.</span>
<span class="sd">        rng (numpy.random.RandomState, optional): Random generator for Initializer. Default is None.</span>
<span class="sd">        with_bias (bool, optional): Specify whether to include the bias parameters. Default is True.</span>
<span class="sd">        add_attn_bias (bool, optional): Specify whether to add attention bias parameters for key and value. Default is False.</span>
<span class="sd">        additive_mask (~nnabla.Variable, optional): Input N-D array with shape :math:`(L_T, L_S)`. Values will be added to the attention layer to prevent attention to certain positions.</span>
<span class="sd">        key_padding_mask (~nnabla.Variable, optional): Input N-D array with shape :math:`(B, L_S)`. Specified padding elements will be ignored by the attention layer. Values must be either 1 or 0.</span>
<span class="sd">        fix_parameters (bool, optional): When set to `True`, the weights and biases will not be updated. Default is False.</span>
<span class="sd">        param_init (dict, optional):</span>
<span class="sd">            Parameter initializers can be set with a dict. Possible keys of the dict include q_weight, k_weight, v_weight, q_bias, k_bias, v_bias, out_weight, out_bias, attn_bias_k, attn_bias_v.</span>
<span class="sd">            A value of the dict must be an :obj:`~nnabla.initializer.Initializer`</span>
<span class="sd">            or a :obj:`numpy.ndarray`.</span>
<span class="sd">            E.g. ``{&#39;q_bias&#39;: ConstantInitializer(0)}``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output :math:`y` with shape :math:`(L_T, B, E)`</span>
<span class="sd">        ~nnabla.Variable: Output :math:`h_n` with shape :math:`(B, L_T, L_S)`</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">if</span> <span class="n">k_embed_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">q_embed_dim</span> <span class="o">=</span> <span class="n">k_embed_dim</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">q_embed_dim</span> <span class="o">=</span> <span class="n">k_embed_dim</span>
    <span class="k">if</span> <span class="n">v_embed_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">v_embed_dim</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">out_dim</span> <span class="o">==</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">out_dim</span> <span class="o">=</span> <span class="n">v_embed_dim</span>

    <span class="k">if</span> <span class="n">param_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">param_init</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">q_weight</span> <span class="o">=</span> <span class="n">param_init</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;q_weight&#39;</span><span class="p">,</span> <span class="n">UniformInitializer</span><span class="p">(</span>
        <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">q_embed_dim</span><span class="p">),</span> <span class="n">rng</span><span class="p">))</span>
    <span class="n">k_weight</span> <span class="o">=</span> <span class="n">param_init</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;k_weight&#39;</span><span class="p">,</span> <span class="n">UniformInitializer</span><span class="p">(</span>
        <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">k_embed_dim</span><span class="p">),</span> <span class="n">rng</span><span class="p">))</span>
    <span class="n">v_weight</span> <span class="o">=</span> <span class="n">param_init</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;v_weight&#39;</span><span class="p">,</span> <span class="n">UniformInitializer</span><span class="p">(</span>
        <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">v_embed_dim</span><span class="p">),</span> <span class="n">rng</span><span class="p">))</span>

    <span class="n">qw</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;q_weight&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">q_embed_dim</span><span class="p">),</span> <span class="n">q_weight</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">kw</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;k_weight&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">k_embed_dim</span><span class="p">),</span> <span class="n">k_weight</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">vw</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;v_weight&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">v_embed_dim</span><span class="p">),</span> <span class="n">v_weight</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="n">out_weight</span> <span class="o">=</span> <span class="n">param_init</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;out_weight&#39;</span><span class="p">,</span> <span class="n">UniformInitializer</span><span class="p">(</span>
        <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">v_embed_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">),</span> <span class="n">rng</span><span class="p">))</span>

    <span class="n">ow</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span><span class="s2">&quot;out_weight&quot;</span><span class="p">,</span> <span class="p">(</span>
        <span class="n">v_embed_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">),</span> <span class="n">out_weight</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="n">qb</span> <span class="o">=</span> <span class="n">kb</span> <span class="o">=</span> <span class="n">vb</span> <span class="o">=</span> <span class="n">ob</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">q_bias</span> <span class="o">=</span> <span class="n">param_init</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;q_bias&#39;</span><span class="p">,</span> <span class="n">ConstantInitializer</span><span class="p">())</span>
        <span class="n">k_bias</span> <span class="o">=</span> <span class="n">param_init</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;k_bias&#39;</span><span class="p">,</span> <span class="n">ConstantInitializer</span><span class="p">())</span>
        <span class="n">v_bias</span> <span class="o">=</span> <span class="n">param_init</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;v_bias&#39;</span><span class="p">,</span> <span class="n">ConstantInitializer</span><span class="p">())</span>
        <span class="n">out_bias</span> <span class="o">=</span> <span class="n">param_init</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;out_bias&#39;</span><span class="p">,</span> <span class="n">ConstantInitializer</span><span class="p">())</span>

        <span class="n">qb</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;q_bias&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">q_embed_dim</span><span class="p">,</span> <span class="p">),</span> <span class="n">q_bias</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
        <span class="n">kb</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;k_bias&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">k_embed_dim</span><span class="p">,</span> <span class="p">),</span> <span class="n">k_bias</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
        <span class="n">vb</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;v_bias&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">v_embed_dim</span><span class="p">,</span> <span class="p">),</span> <span class="n">v_bias</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
        <span class="n">ob</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;out_bias&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">out_dim</span><span class="p">,</span> <span class="p">),</span> <span class="n">out_bias</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="n">abk</span> <span class="o">=</span> <span class="n">abv</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">add_attn_bias</span><span class="p">:</span>
        <span class="n">attn_bias_k</span> <span class="o">=</span> <span class="n">param_init</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;attn_bias_k&#39;</span><span class="p">,</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">k_embed_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">rng</span><span class="p">))</span>
        <span class="n">attn_bias_v</span> <span class="o">=</span> <span class="n">param_init</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;attn_bias_v&#39;</span><span class="p">,</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">v_embed_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">rng</span><span class="p">))</span>

        <span class="n">abk</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;attn_bias_k&quot;</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">k_embed_dim</span><span class="p">),</span> <span class="n">attn_bias_k</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
        <span class="n">abv</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;attn_bias_v&quot;</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">v_embed_dim</span><span class="p">),</span> <span class="n">attn_bias_v</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">multi_head_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">qw</span><span class="p">,</span> <span class="n">kw</span><span class="p">,</span> <span class="n">vw</span><span class="p">,</span> <span class="n">ow</span><span class="p">,</span> <span class="n">qb</span><span class="p">,</span> <span class="n">kb</span><span class="p">,</span> <span class="n">vb</span><span class="p">,</span> <span class="n">ob</span><span class="p">,</span> <span class="n">abk</span><span class="p">,</span> <span class="n">abv</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">additive_mask</span><span class="o">=</span><span class="n">additive_mask</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">key_padding_mask</span><span class="p">)</span></div>


<div class="viewcode-block" id="transformer"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.transformer">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;transformer&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;encoder{layer#}&#39;</span><span class="p">,</span> <span class="s1">&#39;parameters for the n</span><span class="se">\&#39;</span><span class="s1">th encoder layer&#39;</span><span class="p">,</span>
     <span class="s1">&#39;Refer to transformer_encode for details&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;decoder{layer#}&#39;</span><span class="p">,</span> <span class="s1">&#39;parameters for the n</span><span class="se">\&#39;</span><span class="s1">th decoder layer&#39;</span><span class="p">,</span>
     <span class="s1">&#39;Refer to transformer_decode for details&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">transformer</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_encoder_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">num_decoder_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">dim_feedforward</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">src_additive_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_additive_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">memory_additive_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">src_key_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_key_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">memory_key_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">add_attn_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Transformer.</span>

<span class="sd">    We use the following notations to describe the inputs and outputs below.</span>
<span class="sd">    :math:`L_T`: target sequence length, :math:`L_S`: source sequence length, :math:`B`: batch size, :math:`E`: embedding dimension.</span>

<span class="sd">    References:</span>

<span class="sd">        A. Vaswani et al. &quot;Attention is All You Need.&quot;</span>
<span class="sd">        NIPS. 2017.</span>
<span class="sd">        &lt;https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&gt;</span>

<span class="sd">    Examples:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        src = nn.Variable((src_len, batch_size, embed_dim),need_grad=True)</span>
<span class="sd">        tgt = nn.Variable((tgt_len, batch_size, embed_dim),need_grad=True)</span>
<span class="sd">        out = PF.transformer(src, tgt, num_heads=16, num_encoder_layers=12)</span>
<span class="sd">        out.forward()</span>

<span class="sd">    Args:</span>
<span class="sd">        src (~nnabla.Variable): Input source sequence to the encoder with shape:math:`(L_S, B, E)`.</span>
<span class="sd">        tgt (~nnabla.Variable): Input target sequence to the decoder with shape :math:`(L_T, B, E)`.</span>
<span class="sd">        embed_dim (int, optional): Embedding dimension to be used. Default is 512.</span>
<span class="sd">        num_heads (int, optional): Number of attention heads. Default is 12.</span>
<span class="sd">        num_encoder_layers (int, optional): Number of encoder layers to stack. Default is 6.</span>
<span class="sd">        num_decoder_layers (int, optional): Number of decoder layers to stack. Default is 6.</span>
<span class="sd">        dim_feedforward (int, optional): Dimension of the feedforward network model. Default is 2048.</span>
<span class="sd">        dropout (float, optional): Dropout ratio applied. Default is 0.1.</span>
<span class="sd">        activation (function, optional): Non-linear activation function to be used. Default is None, which is set as F.relu in the code.</span>
<span class="sd">        src_additive_mask (~nnabla.Variable, optional): Additive mask for the src sequence (optional). :math:`(L_S, L_S)`.</span>
<span class="sd">        tgt_additive_mask (~nnabla.Variable, optional): Additive mask for the tgt sequence (optional). :math:`(L_T, L_T)`.</span>
<span class="sd">        memory_additive_mask (~nnabla.Variable, optional): Additive mask for the encoder output (optional). :math:`(L_T, L_S)`.</span>
<span class="sd">        src_key_padding_mask (~nnabla.Variable, optional): Key padding mask for src keys per batch (optional). :math:`(B, L_S)`. Specified padding elements will be ignored by the attention layer. Values must be either 1 or 0.</span>
<span class="sd">        tgt_key_padding_mask (~nnabla.Variable, optional): Key padding mask for tgt keys per batch (optional). :math:`(B, L_T)`. Specified padding elements will be ignored by the attention layer. Values must be either 1 or 0.</span>
<span class="sd">        memory_key_padding_mask (~nnabla.Variable, optional): Key padding mask for memory keys per batch (optional). :math:`(B, L_S)`. Specified padding elements will be ignored by the attention layer. Values must be either 1 or 0.</span>
<span class="sd">        rng (numpy.random.RandomState, optional): Random generator for Initializer. Default is None.</span>
<span class="sd">        add_attn_bias (bool, optional): Specify whether to add attention bias parameters for key and value. Default is False.</span>
<span class="sd">        fix_parameters (bool, optional): When set to `True`, the weights and biases will not be updated. Default is False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output :math:`y` with shape :math:`(L_T, B, E)`</span>
<span class="sd">   &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">src</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">tgt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;the batch number of source and target must be equal&quot;</span>
    <span class="k">assert</span> <span class="n">src</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">embed_dim</span> <span class="ow">and</span> <span class="n">tgt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="s2">&quot;the feature dimension of source and target must be equal to embed_dim&quot;</span>

    <span class="n">memory</span> <span class="o">=</span> <span class="n">src</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_encoder_layers</span><span class="p">):</span>
        <span class="n">memory</span> <span class="o">=</span> <span class="n">transformer_encode</span><span class="p">(</span>
            <span class="n">memory</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_feedforward</span><span class="o">=</span><span class="n">dim_feedforward</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span> <span class="n">src_additive_mask</span><span class="o">=</span><span class="n">src_additive_mask</span><span class="p">,</span> <span class="n">src_key_padding_mask</span><span class="o">=</span><span class="n">src_key_padding_mask</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">,</span> <span class="n">add_attn_bias</span><span class="o">=</span><span class="n">add_attn_bias</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="n">fix_parameters</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;encoder</span><span class="si">{:02d}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">tgt</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_decoder_layers</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">transformer_decode</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_feedforward</span><span class="o">=</span><span class="n">dim_feedforward</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span> <span class="n">tgt_additive_mask</span><span class="o">=</span><span class="n">tgt_additive_mask</span><span class="p">,</span> <span class="n">memory_additive_mask</span><span class="o">=</span><span class="n">memory_additive_mask</span><span class="p">,</span>
                                    <span class="n">tgt_key_padding_mask</span><span class="o">=</span><span class="n">tgt_key_padding_mask</span><span class="p">,</span> <span class="n">memory_key_padding_mask</span><span class="o">=</span><span class="n">memory_key_padding_mask</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">,</span> <span class="n">add_attn_bias</span><span class="o">=</span><span class="n">add_attn_bias</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="n">fix_parameters</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;decoder</span><span class="si">{:02d}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="transformer_encode"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.transformer_encode">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;transformer_encode&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;src_self_attn&#39;</span><span class="p">,</span> <span class="s1">&#39;self-attention parameters for source sequence&#39;</span><span class="p">,</span>
     <span class="s1">&#39;Refer to multi_head_attention for details&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;enc_affine1&#39;</span><span class="p">,</span> <span class="s1">&#39;first affine used in encoder&#39;</span><span class="p">,</span>
     <span class="s1">&#39;Refer to affine for details&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;enc_affine2&#39;</span><span class="p">,</span> <span class="s1">&#39;second affine used in encoder&#39;</span><span class="p">,</span>
     <span class="s1">&#39;Refer to affine for details&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;enc_layer_norm1&#39;</span><span class="p">,</span> <span class="s1">&#39;fist layer normalization used in encoder&#39;</span><span class="p">,</span>
     <span class="s1">&#39;Refer to layer_normalization for details&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;enc_layer_norm2&#39;</span><span class="p">,</span> <span class="s1">&#39;second layer normalization used in encoder&#39;</span><span class="p">,</span>
     <span class="s1">&#39;Refer to layer_normalization for details&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">transformer_encode</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_feedforward</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">src_additive_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">src_key_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">add_attn_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Transformer Encoder.</span>

<span class="sd">    Args:</span>
<span class="sd">        src (~nnabla.Variable): Input sequnce to the encoder layer with shape :math:`(L_S, B, E)`.</span>
<span class="sd">        embed_dim (int): Number of embedding dimension.</span>
<span class="sd">        num_heads (int): Number of attention heads.</span>
<span class="sd">        dim_feedforward (int, optional): Dimension of the feedforward network model. Default is 2048.</span>
<span class="sd">        dropout (float, optional): Dropout ratio. Default is 0.1.</span>
<span class="sd">        activation (function, optional): Non-linear activation function to be used. Default is None, which is set as F.relu in the code.</span>
<span class="sd">        src_additive_mask (~nnabla.Variable, optional): Additive mask for the source sequence with shape :math:`(L_S, L_S)`</span>
<span class="sd">        src_key_padding_mask (~nnabla.Variable, optional): Padding mask for the source sequence with shape :math:`(B, L_S)`. Specified padding elements will be ignored by the attention layer. Values must be either 1 or 0.</span>
<span class="sd">        rng (numpy.random.RandomState, optional): Random generator for Initializer. Defalut is None.</span>
<span class="sd">        add_attn_bias (bool, optional): Specify whether to add attention bias parameters for key and value. Default is False.</span>
<span class="sd">        fix_parameters (bool, optional): When set to `True`, the weights and biases will not be updated. Default is False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output :math:`y` with shape :math:`(L_S, B, E)`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">activation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">activation</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span>
    <span class="n">src_self_attn</span> <span class="o">=</span> <span class="n">multi_head_attention</span><span class="p">(</span>
        <span class="n">src</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">add_attn_bias</span><span class="o">=</span><span class="n">add_attn_bias</span><span class="p">,</span> <span class="n">additive_mask</span><span class="o">=</span><span class="n">src_additive_mask</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">src_key_padding_mask</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;src_self_attn&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">src_self_attn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">src_self_attn</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
    <span class="n">src_self_attn</span> <span class="o">=</span> <span class="n">src</span> <span class="o">+</span> <span class="n">src_self_attn</span>
    <span class="n">src</span> <span class="o">=</span> <span class="n">layer_normalization</span><span class="p">(</span>
        <span class="n">src_self_attn</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;enc_layer_norm1&#39;</span><span class="p">)</span>
    <span class="n">src_affine</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">affine</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">dim_feedforward</span><span class="p">,</span>
                                   <span class="n">base_axis</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;enc_affine1&#39;</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">src_affine</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">src_affine</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
    <span class="n">src_affine</span> <span class="o">=</span> <span class="n">affine</span><span class="p">(</span><span class="n">src_affine</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;enc_affine2&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">src_affine</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">src_affine</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
    <span class="n">src_affine</span> <span class="o">=</span> <span class="n">src</span> <span class="o">+</span> <span class="n">src_affine</span>
    <span class="n">src</span> <span class="o">=</span> <span class="n">layer_normalization</span><span class="p">(</span>
        <span class="n">src_affine</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;enc_layer_norm2&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">src</span></div>


<div class="viewcode-block" id="transformer_decode"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.transformer_decode">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;transformer_decode&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;tgt_self_attn&#39;</span><span class="p">,</span> <span class="s1">&#39;self-attention parameters for target sequence&#39;</span><span class="p">,</span>
     <span class="s1">&#39;Refer to multi_head_attention for details&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;tgt_memory_attn&#39;</span><span class="p">,</span> <span class="s1">&#39;attention parameters for target sequence with output from encoder as key&#39;</span><span class="p">,</span>
     <span class="s1">&#39;Refer to multi_head_attention for details&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;dec_affine1&#39;</span><span class="p">,</span> <span class="s1">&#39;first affine used in decoder&#39;</span><span class="p">,</span>
     <span class="s1">&#39;Refer to affine for details&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;dec_affine2&#39;</span><span class="p">,</span> <span class="s1">&#39;second affine used in decoder&#39;</span><span class="p">,</span>
     <span class="s1">&#39;Refer to affine for details&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;dec_layer_norm1&#39;</span><span class="p">,</span> <span class="s1">&#39;fist layer normalization used in decoder&#39;</span><span class="p">,</span>
     <span class="s1">&#39;Refer to layer_normalization for details&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;dec_layer_norm2&#39;</span><span class="p">,</span> <span class="s1">&#39;second layer normalization used in decoder&#39;</span><span class="p">,</span>
     <span class="s1">&#39;Refer to layer_normalization for details&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;dec_layer_norm3&#39;</span><span class="p">,</span> <span class="s1">&#39;third layer normalization used in decoder&#39;</span><span class="p">,</span>
     <span class="s1">&#39;Refer to layer_normalization for details&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">transformer_decode</span><span class="p">(</span><span class="n">tgt</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_feedforward</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_additive_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">memory_additive_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_key_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">memory_key_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">add_attn_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Transformer Decoder.</span>

<span class="sd">    Args:</span>
<span class="sd">        tgt (~nnabla.Variable): Input sequnce to the decoder layer with shape :math:`(L_T, B, E)`.</span>
<span class="sd">        memory (~nnabla.Variable): Output sequnce from the last layer of the encoder with shape :math:`(L_T, B, E)`.</span>
<span class="sd">        embed_dim (int): Number of embedding dimension.</span>
<span class="sd">        num_heads (int): Number of attention heads.</span>
<span class="sd">        dim_feedforward (int, optional): Dimension of the feedforward network model. Default is 2048.</span>
<span class="sd">        dropout (float, optional): Dropout ratio. Default is 0.1.</span>
<span class="sd">        activation (function, optional): Non-linear activation function to be used. Default is None, which is set as F.relu in the code.</span>
<span class="sd">        tgt_additive_mask (~nnabla.Variable, optional): Additive mask for the target sequence with shape :math:`(L_T, L_T)`.</span>
<span class="sd">        memory_additive_mask (~nnabla.Variable, optional): Additive mask for the memory sequcne with shape :math:`(L_T, L_S)`.</span>
<span class="sd">        tgt_key_padding_mask (~nnabla.Variable, optional): Padding mask for the target sequence with shape :math:`(B, L_T)`. Specified padding elements will be ignored by the attention layer. Values must be either 1 or 0.</span>
<span class="sd">        memory_key_padding_mask (~nnabla.Variable, optional): Padding mask for the mask sequence with shape :math:`(B, L_S)`. Specified padding elements will be ignored by the attention layer. Values must be either 1 or 0.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer. Default is None.</span>
<span class="sd">        add_attn_bias (bool, optional): Specify whether to add attention bias parameters for key and value. Default is False.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated. Default is False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output :math:`y` with shape :math:`(L_T, B, E)`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">activation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">activation</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span>
    <span class="n">tgt_self_attn</span> <span class="o">=</span> <span class="n">multi_head_attention</span><span class="p">(</span>
        <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">add_attn_bias</span><span class="o">=</span><span class="n">add_attn_bias</span><span class="p">,</span> <span class="n">additive_mask</span><span class="o">=</span><span class="n">tgt_additive_mask</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">tgt_key_padding_mask</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;tgt_self_attn&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">tgt_self_attn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">tgt_self_attn</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
    <span class="n">tgt_self_attn</span> <span class="o">=</span> <span class="n">tgt</span> <span class="o">+</span> <span class="n">tgt_self_attn</span>
    <span class="n">tgt</span> <span class="o">=</span> <span class="n">layer_normalization</span><span class="p">(</span>
        <span class="n">tgt_self_attn</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;dec_layer_norm1&#39;</span><span class="p">)</span>
    <span class="n">tgt_multi_attn</span> <span class="o">=</span> <span class="n">multi_head_attention</span><span class="p">(</span>
        <span class="n">tgt</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">add_attn_bias</span><span class="o">=</span><span class="n">add_attn_bias</span><span class="p">,</span> <span class="n">additive_mask</span><span class="o">=</span><span class="n">memory_additive_mask</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">memory_key_padding_mask</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;tgt_memory_attn&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">tgt_multi_attn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">tgt_multi_attn</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
    <span class="n">tgt_multi_attn</span> <span class="o">=</span> <span class="n">tgt</span> <span class="o">+</span> <span class="n">tgt_multi_attn</span>
    <span class="n">tgt</span> <span class="o">=</span> <span class="n">layer_normalization</span><span class="p">(</span>
        <span class="n">tgt_multi_attn</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;dec_layer_norm2&#39;</span><span class="p">)</span>
    <span class="n">tgt_affine</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">affine</span><span class="p">(</span><span class="n">tgt</span><span class="p">,</span> <span class="n">dim_feedforward</span><span class="p">,</span>
                                   <span class="n">base_axis</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;dec_affine1&#39;</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">tgt_affine</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">tgt_affine</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
    <span class="n">tgt_affine</span> <span class="o">=</span> <span class="n">affine</span><span class="p">(</span><span class="n">tgt_affine</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;dec_affine2&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">tgt_affine</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">tgt_affine</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
    <span class="n">tgt_affine</span> <span class="o">=</span> <span class="n">tgt</span> <span class="o">+</span> <span class="n">tgt_affine</span>
    <span class="n">tgt</span> <span class="o">=</span> <span class="n">layer_normalization</span><span class="p">(</span>
        <span class="n">tgt_affine</span><span class="p">,</span> <span class="n">batch_axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;dec_layer_norm3&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tgt</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2017, Sony Corporation.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>