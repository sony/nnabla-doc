

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>nnabla.function_bases &mdash; Neural Network Libraries 1.0.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> Neural Network Libraries
          

          
          </a>

          
            
            
              <div class="version">
                1.0.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../python.html">Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cpp.html">C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../format.html">Data Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python/file_format_converter/file_format_converter.html">File format converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">Contributing Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../license.html">License</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Neural Network Libraries</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>nnabla.function_bases</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for nnabla.function_bases</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) 2017 Sony Corporation. All Rights Reserved.</span>
<span class="c1"># </span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1"># </span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1"># </span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>

<span class="c1">#</span>
<span class="c1"># *WARNING*</span>
<span class="c1"># THIS FILE IS AUTO-GENERATED BY CODE GENERATOR.</span>
<span class="c1"># PLEASE DO NOT EDIT THIS FILE BY HAND!</span>
<span class="c1"># If you want to modify this file, edit following files.</span>
<span class="c1"># - python/src/nnabla/function_bases.py.tmpl</span>
<span class="c1"># - build-tools/code_generator/generate.py</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>

<span class="kn">from</span> <span class="nn">.context</span> <span class="k">import</span> <span class="n">get_current_context</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="k">import</span> <span class="n">function</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">.auto_forward</span> <span class="k">import</span> <span class="n">get_auto_forward</span>

<span class="kn">import</span> <span class="nn">inspect</span>


<span class="c1"># Templates for function_api source building.</span>
<span class="n">FUNCTION_API_HEADER</span> <span class="o">=</span> <span class="s2">&quot;def </span><span class="si">{name}{signature}</span><span class="s2">:&quot;</span>
<span class="n">FUNCTION_API_BODY</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;ctx = get_current_context()</span>
<span class="s1">return _func_(ctx, </span><span class="si">{shortsignature}</span><span class="s1">)&#39;&#39;&#39;</span>


<span class="k">def</span> <span class="nf">function_api</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Decorator for making function called with current context.</span>
<span class="sd">    Some tricky things are done here so that signature and docstring are available.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="vm">__name__</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="vm">__doc__</span>
    <span class="k">if</span> <span class="n">doc</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">doc</span> <span class="o">=</span> <span class="s2">&quot;No docstring.&quot;</span>

    <span class="c1"># Parsing argspecs</span>
    <span class="n">spec</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getargspec</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
    <span class="n">defaults</span> <span class="o">=</span> <span class="n">spec</span><span class="o">.</span><span class="n">defaults</span>
    <span class="k">if</span> <span class="n">spec</span><span class="o">.</span><span class="n">defaults</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">defaults</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">defaults</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">spec</span><span class="o">.</span><span class="n">args</span><span class="p">):</span>
        <span class="n">defaults</span> <span class="o">=</span> <span class="n">defaults</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="c1"># Creating signature</span>
    <span class="c1"># e.g. (x, weights, biases=None, n_outputs=None)</span>
    <span class="n">signature</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">formatargspec</span><span class="p">(</span>
        <span class="n">spec</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">spec</span><span class="o">.</span><span class="n">varargs</span><span class="p">,</span> <span class="n">spec</span><span class="o">.</span><span class="n">keywords</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>
    <span class="c1"># Creating signature without parans and defaults</span>
    <span class="c1"># e.g. x, weights, biases, n_outputs</span>
    <span class="n">shortsignature</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">formatargspec</span><span class="p">(</span>
        <span class="n">spec</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">spec</span><span class="o">.</span><span class="n">varargs</span><span class="p">,</span> <span class="n">spec</span><span class="o">.</span><span class="n">keywords</span><span class="p">,</span> <span class="kc">None</span><span class="p">)[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Create code by string</span>
    <span class="n">src</span> <span class="o">=</span> <span class="p">(</span><span class="n">FUNCTION_API_HEADER</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;    &#39;</span> <span class="o">+</span>
                                                      <span class="n">x</span><span class="p">,</span> <span class="n">FUNCTION_API_BODY</span><span class="o">.</span><span class="n">splitlines</span><span class="p">())))</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="nb">locals</span><span class="p">())</span>

    <span class="c1"># Evaluate source code from string</span>
    <span class="n">code</span> <span class="o">=</span> <span class="nb">compile</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="s2">&quot;&lt;</span><span class="si">{name}</span><span class="s2">&gt;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="nb">locals</span><span class="p">()),</span> <span class="s1">&#39;single&#39;</span><span class="p">)</span>
    <span class="n">execdict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">_func_</span><span class="o">=</span><span class="n">func</span><span class="p">,</span> <span class="n">get_current_context</span><span class="o">=</span><span class="n">get_current_context</span><span class="p">)</span>
    <span class="n">exec</span><span class="p">(</span><span class="n">code</span><span class="p">,</span> <span class="n">execdict</span><span class="p">)</span>

    <span class="c1"># Get created function.</span>
    <span class="n">newfunc</span> <span class="o">=</span> <span class="n">execdict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
    <span class="c1"># DOC newfunc.__doc__ = FUNCTION_API_DOC.format(**locals())</span>
    <span class="n">doc</span> <span class="o">+=</span> <span class="s1">&#39;&#39;&#39;</span>

<span class="s1">    Note:</span>
<span class="s1">        All nnabla functions in :obj:`nnabla.functions` are decorated with the :obj:`nnabla.function_bases.function_api` decorator,</span>
<span class="s1">        which queries the current context and passes it into the first argument of the</span>
<span class="s1">        original function. The original function always takes a context as the first argument.</span>

<span class="s1">    &#39;&#39;&#39;</span>
    <span class="n">newfunc</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">doc</span>
    <span class="n">newfunc</span><span class="o">.</span><span class="n">__source__</span> <span class="o">=</span> <span class="n">src</span>
    <span class="n">newfunc</span><span class="o">.</span><span class="n">__function_api_base__</span> <span class="o">=</span> <span class="n">func</span>
    <span class="n">newfunc</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">=</span> <span class="vm">__name__</span>
    <span class="k">return</span> <span class="n">newfunc</span>





<div class="viewcode-block" id="affine"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.affine">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">affine</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Affine layer, also called as the fully connected layer. It calculates:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        {\mathbf y} = {\mathbf A} {\mathbf x} + {\mathbf b}.</span>
<span class="sd">    </span>
<span class="sd">    where :math:`{\mathbf x}` is the input and :math:`{\mathbf y}` is the output.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input N-D array with shape (:math:`M_0 \times ... \times M_{B-1} \times D_B \times ... \times D_N`). Dimensions before and after base_axis are flattened as if it is a matrix.</span>
<span class="sd">        weight(~nnabla.Variable): Weight matrix with shape (:math:`(D_B \times ... \times D_N) \times L`)</span>
<span class="sd">            [parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Bias vector (:math:`L`)</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        base_axis(int): Base axis of Affine operation. Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: :math:`(B + 1)`-D array. (:math:`M_0 \times ... \times M_{B-1} \times L`)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Affine</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="convolution"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.convolution">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">convolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    N-D Convolution with bias.</span>
<span class="sd">    </span>
<span class="sd">    See references for dilated convolution (a.k.a. atrous convolution).</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">        * `Chen et al., DeepLab: Semantic Image Segmentation with Deep Convolutional</span>
<span class="sd">          Nets, Atrous Convolution, and Fully Connected CRFs.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1606.00915&gt;`_</span>
<span class="sd">    </span>
<span class="sd">        * `Yu et al., Multi-Scale Context Aggregation by Dilated Convolutions.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1511.07122&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): :math:`(B + 1 + N)`-D array (:math:`M_1 \times ... \times M_B \times C \times L_1 \times ... \times L_N`).</span>
<span class="sd">        weight(~nnabla.Variable): :math:`(2 + N)`-D array (:math:`C&#39; \times C \times K_1 \times ... \times K_N`).</span>
<span class="sd">            [parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Bias vector (:math:`C&#39;`).</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        base_axis(int): base axis :math:`B`.</span>
<span class="sd">            [default=``1``]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">            [default=``(0,) * (len(x.shape) - (base_axis+1))``]</span>
<span class="sd">        stride(:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">            [default=``(1,) * (len(x.shape) - (base_axis+1))``]</span>
<span class="sd">        dilation(:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">            [default=``(1,) * (len(x.shape) - (base_axis+1))``]</span>
<span class="sd">        group(int): Number of groups of channels. This makes the connection across channels sparser, by grouping connections along the mapping direction.</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: :math:`(B + 1 + N)`-D array (:math:`M_1 \times ... \times M_B \times C&#39; \times L&#39;_1 \times ... \times L&#39;_N`).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">pad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">dilation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Convolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="depthwise_convolution"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.depthwise_convolution">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">depthwise_convolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">multiplier</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    N-D Depthwise Convolution with bias.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">        * `F. Chollet: Chollet, Francois. &quot;Xception: Deep Learning with Depthwise Separable Convolutions.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1610.02357&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): :math:`(B + 1 + N)`-D array (:math:`M_1 \times ... \times M_B \times C \times L_1 \times ... \times L_N`).</span>
<span class="sd">        weight(~nnabla.Variable): :math:`(1 + N)`-D array (:math:`C \times K_1 \times ... \times K_N`).</span>
<span class="sd">            [parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Bias vector (:math:`C`).</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        base_axis(int): base axis :math:`B`.</span>
<span class="sd">            [default=``1``]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">            [default=``(0,) * (len(x.shape) - (base_axis+1))``]</span>
<span class="sd">        stride(:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">            [default=``(1,) * (len(x.shape) - (base_axis+1))``]</span>
<span class="sd">        dilation(:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">            [default=``(1,) * (len(x.shape) - (base_axis+1))``]</span>
<span class="sd">        multiplier(int): Number of output feature maps per input feature map.</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: :math:`(B + 1 + N)`-D array (:math:`M_1 \times ... \times M_B \times C \times L&#39;_1 \times ... \times L&#39;_N`).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">pad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">dilation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">DepthwiseConvolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">multiplier</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="deconvolution"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.deconvolution">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">deconvolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    N-D deconvolution, also known as transposed convolution, with bias operates backward convolution (derivative of the output w.r.t. the input) plus channel-wise learned bias.</span>
<span class="sd">    </span>
<span class="sd">    The weights are specified in the same manner as :meth:`~nnabla.functions.convolution` , as if it was an ordinary convolution function.</span>
<span class="sd">    The forward operation of :meth:`~nnabla.functions.deconvolution` will then be operationally equivalent to the backward pass of :meth:`~nnabla.functions.convolution` .</span>
<span class="sd">    Therefore, the number of input channels (can be seen as output channels of forward convolution) is specified in the first dimension, and the number of the output channels divided by the number of groups is specified in the second dimension.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): :math:`(B + 1 + N)`-D array (:math:`M_1 \times ... \times M_B \times C \times L_1 \times ... \times L_N`).</span>
<span class="sd">        weight(~nnabla.Variable): :math:`(2 + N)`-D array (:math:`C&#39; \times C \times K_1 \times ... \times K_N`).</span>
<span class="sd">            [parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Bias vector (:math:`C&#39;`).</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        base_axis(int): base axis :math:`B`.</span>
<span class="sd">            [default=``1``]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">            [default=``(0,) * (len(x.shape) - (base_axis+1))``]</span>
<span class="sd">        stride(:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">            [default=``(1,) * (len(x.shape) - (base_axis+1))``]</span>
<span class="sd">        dilation(:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">            [default=``(1,) * (len(x.shape) - (base_axis+1))``]</span>
<span class="sd">        group(int): Number of groups of channels. This makes the connection across channels sparser, by grouping connections along the mapping direction.</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: :math:`(B + 1 + N)`-D array (:math:`M_1 \times ... \times M_B \times C&#39; \times L&#39;_1 \times ... \times L&#39;_N`).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">pad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">dilation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Deconvolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="depthwise_deconvolution"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.depthwise_deconvolution">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">depthwise_deconvolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">divisor</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Depthwise deconvolution computes the transposed depthwise convolution with bias for one-dimensional and two-dimensional input data.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): :math:`(B + 1 + N)`-D array (:math:`M_1 \times ... \times M_B \times C \times L_1 \times ... \times L_N`).</span>
<span class="sd">        weight(~nnabla.Variable): :math:`(1 + N)`-D array (:math:`C \times K_1 \times ... \times K_N`).</span>
<span class="sd">            [parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Bias vector (:math:`C`).</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        base_axis(int): base axis :math:`B`.</span>
<span class="sd">            [default=``1``]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">            [default=``(0,) * (len(x.shape) - (base_axis+1))``]</span>
<span class="sd">        stride(:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">            [default=``(1,) * (len(x.shape) - (base_axis+1))``]</span>
<span class="sd">        dilation(:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">            [default=``(1,) * (len(x.shape) - (base_axis+1))``]</span>
<span class="sd">        divisor(int): Number of input feature maps per output feature map.</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: :math:`(B + 1 + N)`-D array (:math:`M_1 \times ... \times M_B \times C \times L&#39;_1 \times ... \times L&#39;_N`).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">pad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">dilation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">DepthwiseDeconvolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">divisor</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="max_pooling"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.max_pooling">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">max_pooling</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_border</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Max pooling. It pools the maximum values inside the scanning kernel:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_{i_1, i_2} = \max_{k_1, k_2 \in K} (x_{i_1 + k_1, i_2 + k_2})</span>
<span class="sd">    </span>
<span class="sd">    where :math:`x_{i_1 + k_1, i_2 + k_2}` is the input and :math:`y_{i_1, i_2}` is the output.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable.</span>
<span class="sd">        kernel(:obj:`tuple` of :obj:`int`): Kernel sizes for each spatial axis.</span>
<span class="sd">        stride(:obj:`tuple` of :obj:`int`): Subsampling factors for each spatial axis.</span>
<span class="sd">            [default=``kernel``]</span>
<span class="sd">        ignore_border(bool): If false, kernels covering borders are also considered for the output.</span>
<span class="sd">            [default=``True``]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): Border padding values for each spatial axis. Padding will be added both sides of the dimension.</span>
<span class="sd">            [default=``(0,) * len(kernel)``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Maximum values variable</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">kernel</span>
    <span class="k">if</span> <span class="n">pad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">MaxPooling</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">ignore_border</span><span class="p">,</span> <span class="n">pad</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="average_pooling"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.average_pooling">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">average_pooling</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_border</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">including_pad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Average pooling. It pools the averaged values inside the scanning kernel:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_{i_1, i_2} = \frac{1}{K_1 K_2} \sum_{k1} \sum_{k2} x_{i_1 + k_1, i_2 + k_2}</span>
<span class="sd">    </span>
<span class="sd">    where :math:`x_{i_1 + k_1, i_2 + k_2}` is the input and :math:`y_{i_1, i_2}` is the output.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable.</span>
<span class="sd">        kernel(:obj:`tuple` of :obj:`int`): Kernel sizes for each spatial axis.</span>
<span class="sd">        stride(:obj:`tuple` of :obj:`int`): Subsampling factors for each spatial axis.</span>
<span class="sd">            [default=``kernel``]</span>
<span class="sd">        ignore_border(bool): If false, kernels covering borders are also considered for the output.</span>
<span class="sd">            [default=``True``]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): Border padding values for each spatial axis. Padding will be added both sides of the dimension.</span>
<span class="sd">            [default=``(0,) * len(kernel)``]</span>
<span class="sd">        including_pad(bool): If true, border padding values are considered for the output.</span>
<span class="sd">            [default=``True``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Average values variable</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">kernel</span>
    <span class="k">if</span> <span class="n">pad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">AveragePooling</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">ignore_border</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">including_pad</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="global_average_pooling"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.global_average_pooling">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">global_average_pooling</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;.. WARNING::</span>
<span class="sd">      This function is experimental suppport, so please do not actively use it.</span>
<span class="sd">    </span>
<span class="sd">    Global average pooling. It pools an averaged value from the whole image</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Average values variable</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">GlobalAveragePooling</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="sum_pooling"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.sum_pooling">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">sum_pooling</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_border</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sum pooling. It pools the summed values inside the scanning kernel:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_{i_1, i_2} = \sum_{k1} \sum_{k2} x_{i_1 + k_1, i_2 + k_2}</span>
<span class="sd">    </span>
<span class="sd">    where :math:`x_{i_1 + k_1, i_2 + k_2}` is the input and :math:`y_{i_1, i_2}` is the output.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable.</span>
<span class="sd">        kernel(:obj:`tuple` of :obj:`int`): Kernel sizes for each spatial axis.</span>
<span class="sd">        stride(:obj:`tuple` of :obj:`int`): Subsampling factors for each spatial axis.</span>
<span class="sd">            [default=``kernel``]</span>
<span class="sd">        ignore_border(bool): If false, kernels covering borders are also considered for the output.</span>
<span class="sd">            [default=``True``]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): Border padding values for each spatial axis. Padding will be added both sides of the dimension.</span>
<span class="sd">            [default=``(0,) * len(kernel)``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Summed values variable</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">kernel</span>
    <span class="k">if</span> <span class="n">pad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">SumPooling</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">ignore_border</span><span class="p">,</span> <span class="n">pad</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="unpooling"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.unpooling">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">unpooling</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Inverse operation of pooling. It spreads the input values:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_{k_1 i_1 + j_1, k_2 i_2 + j_2} = x_{i_1, i_2}</span>
<span class="sd">    </span>
<span class="sd">    where :math:`_{i_1, i_2}` is the input and :math:`y_{k_1 i_1 + j_1, k_2 i_2 + j_2}` is the output.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable.</span>
<span class="sd">        kernel(:obj:`tuple` of :obj:`int`): Kernel sizes for each spatial axis.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Spread values variable</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Unpooling</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="embed"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.embed">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">embed</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Embed slices of a matrix/tensor with indexing array/tensor.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Indices with shape :math:`(I_0, ..., I_N)`</span>
<span class="sd">        w(~nnabla.Variable): Weights with shape :math:`(W_0, ..., W_M)`</span>
<span class="sd">            [parameter]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output with shape :math:`(I_0, ..., I_N, W_1, ..., W_M)`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Embed</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="sigmoid"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.sigmoid">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise sigmoid function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        f(x) = \frac{1}{1 + \exp(-x)},</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="swish"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.swish">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">swish</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise swish function, by Ramachandran et al. (2017).</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        y_i = \frac{x_i}{1 + \exp(-x_i)},</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">        * `Prajit Ramachandran, Barret Zoph, and Quoc V. Le, Swish: a Self-Gated Activation Function, arXiv:1710.05941 [cs.NE]</span>
<span class="sd">          &lt;https://arxiv.org/abs/1710.05941&gt;`_</span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Swish</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="tanh"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.tanh">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise hyperbolic tangent (tanh) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \tanh (x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="relu"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.relu">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise Rectified Linear Unit (ReLU) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \max (0, x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        inplace(bool): The output array is shared with the input array if True.</span>
<span class="sd">            [default=``False``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="leaky_relu"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.leaky_relu">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">leaky_relu</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise Leaky Rectified Linear Unit (ReLU) function.</span>
<span class="sd">    </span>
<span class="sd">    It is defined as:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \alpha * \min(0, x_i) + \max (0, x_i)</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        alpha(float): The slope value multiplied to negative numbers. :math:`\alpha` in the definition.</span>
<span class="sd">            [default=``0.1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="softmax"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.softmax">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax normalization. Calculates</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)}</span>
<span class="sd">    </span>
<span class="sd">    along the dimension specified by `axis`, where :math:`y_i` is the input and :math:`x_i` is the output.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array. Typically indicates a score.</span>
<span class="sd">        axis(int): Axis normalization is taken.</span>
<span class="sd">            [default=``len(x.shape) - 1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="elu"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.elu">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">elu</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise Exponential Linear Unit (ELU) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i= \left\{</span>
<span class="sd">        \begin{array}{ll}</span>
<span class="sd">        x_i &amp; (x &gt; 0)\\</span>
<span class="sd">        \alpha (\exp(x_i) - 1) &amp; (x \leq 0)</span>
<span class="sd">        \end{array} \right..</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">        * `Clevart et al., Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs).</span>
<span class="sd">          &lt;http://arxiv.org/abs/1511.07289&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        alpha(float): Coefficient for negative outputs. :math:`\alpha` in definition</span>
<span class="sd">            [default=``1.0``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ELU</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="selu"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.selu">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">selu</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.05070098735548</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.673263242354377</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise Scaled Exponential Linear Unit (SELU) function by Klambauer et al. (2017).</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i= \lambda \left\{</span>
<span class="sd">        \begin{array}{ll}</span>
<span class="sd">        x_i &amp; (x &gt; 0)\\</span>
<span class="sd">        \alpha (\exp(x_i) - 1) &amp; (x \leq 0)</span>
<span class="sd">        \end{array} \right..</span>
<span class="sd">    </span>
<span class="sd">    The coefficients :math:`\lambda` and :math:`\alpha` default to the following values :math:`\lambda_{01}` and :math:`\alpha_{01}`, respectively, provided by Klambauer et al. (2017):</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{lll}</span>
<span class="sd">          \lambda_{01} &amp;=&amp;  \left(  1 - \operatorname{erfc}\left( \frac{1}{\sqrt{2}} \right) \sqrt{e}  \right)</span>
<span class="sd">                      \sqrt{2 \pi} \\</span>
<span class="sd">                     &amp;&amp; \left(</span>
<span class="sd">                          2 \operatorname{erfc} \left( \sqrt{2} \right) e^2</span>
<span class="sd">                          + \pi \operatorname{erfc}\left( \frac{1}{\sqrt{2}} \right)^2 e</span>
<span class="sd">                          \right. \\</span>
<span class="sd">                     &amp;&amp; \left.</span>
<span class="sd">                          - 2(2 + \pi) \operatorname{erfc} \left( \frac{1}{\sqrt{2}} \right) \sqrt{e}</span>
<span class="sd">                          + \pi + 2</span>
<span class="sd">                     \right)^{-1/2}  \\</span>
<span class="sd">                  &amp;\approx&amp; 1.0507 \\</span>
<span class="sd">          \alpha_{01} &amp;=&amp;  - \frac</span>
<span class="sd">                        {\sqrt {\frac {2}{\pi}}}</span>
<span class="sd">                        {\operatorname{erfc} \left( \frac{1}{\sqrt{2}} \right) \exp \left(\frac {1} {2} \right) - 1} \\</span>
<span class="sd">                  &amp;\approx&amp; 1.67326</span>
<span class="sd">        \end{array}</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">        * `Klambauer, G., Unterthiner, T., Mayr, A., &amp; Hochreiter, S. (2017).</span>
<span class="sd">          Self-Normalizing Neural Networks. In Advances in Neural Information</span>
<span class="sd">          Processing Systems (NIPS). &lt;https://arxiv.org/abs/1706.02515&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        scale(float): The coefficient :math:`\lambda` in the definition.</span>
<span class="sd">            [default=``1.05070098735548``]</span>
<span class="sd">        alpha(float): The coefficient :math:`\alpha` in the definition.</span>
<span class="sd">            [default=``1.673263242354377``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">SELU</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="crelu"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.crelu">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">crelu</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise Concatenated Rectified Linear Unit (CReLU) function.</span>
<span class="sd">    This function calculates the ReLU of :math:`x` and :math:`-x` , then concatenates the results together at a specified axis,</span>
<span class="sd">    and returns the resulting array.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">        * `Wenling Shang, Kihyuk Sohn, Diogo Almeida, Honglak Lee.</span>
<span class="sd">          Understanding and Improving Convolutional Neural Networks</span>
<span class="sd">          via Concatenated Rectified Linear Units.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1603.05201&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        axis(int): The ReLU activations of positive inputs and negative inputs are concatenated at axis.</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array where axis dimension is doubled by concatenating.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">CReLU</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="celu"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.celu">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">celu</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise Concatenated Exponential Linear Unit (CELU) function.</span>
<span class="sd">    Concatenates ELU outputs of positive and negative inputs together at specified axis.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        alpha(float): Coefficient for negative outputs. :math:`\alpha` in definition.</span>
<span class="sd">            [default=``1.0``]</span>
<span class="sd">        axis(int): The ELU activations of positive inputs and negative inputs are concatenated at axis.</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array where axis dimension is doubled by concatenating.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">CELU</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">axis</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="prelu"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.prelu">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">prelu</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise Parametrized Rectified Linear Unit function. Calculates:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \max(0, x_i) + w_i \min(0, -x_i)</span>
<span class="sd">    </span>
<span class="sd">    where negative slope :math:`w` is learned and can vary across channels (an</span>
<span class="sd">    axis specified with `base_axis`).</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): (N-D array) Input</span>
<span class="sd">        x1(~nnabla.Variable): (N-D array) Weights</span>
<span class="sd">        base_axis(int): Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">PReLU</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">batch_normalization</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">batch_stat</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Batch normalization.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \begin{eqnarray}</span>
<span class="sd">          \mu &amp;=&amp; \frac{1}{M} \sum x_i \\</span>
<span class="sd">          \sigma^2 &amp;=&amp; \frac{1}{M} \left(\sum x_i - \mu\right)^2 \\</span>
<span class="sd">          \hat{x}_i &amp;=&amp; \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} \\</span>
<span class="sd">          y_i &amp;=&amp; \hat{x}_i \gamma + \beta.</span>
<span class="sd">        \end{eqnarray}</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    At testing time, the mean and variance values used are those that were computed during training by moving average.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">        * `Ioffe and Szegedy, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1502.03167&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array of input.</span>
<span class="sd">        beta(~nnabla.Variable): N-D array of beta which is learned.</span>
<span class="sd">        gamma(~nnabla.Variable): N-D array of gamma which is learned.</span>
<span class="sd">        mean(~nnabla.Variable): N-D array of running mean (modified during forward execution).</span>
<span class="sd">        variance(~nnabla.Variable): N-D array of running variance (modified during forward execution).</span>
<span class="sd">        axes(repeated int64): Axes mean and variance are taken.</span>
<span class="sd">            [default=``(1,)``]</span>
<span class="sd">        decay_rate(float): Decay rate of running mean and variance.</span>
<span class="sd">            [default=``0.9``]</span>
<span class="sd">        eps(float): Tiny value to avoid zero division by std.</span>
<span class="sd">            [default=``1e-05``]</span>
<span class="sd">        batch_stat(bool): Use mini-batch statistics rather than running ones.</span>
<span class="sd">            [default=``True``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">decay_rate</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">batch_stat</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">mean_subtraction</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">rmean</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">update_running_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    It subtracts the mean of the elements of the input array,</span>
<span class="sd">    and normalizes it to :math:`0`. Preprocessing arrays with this function has the effect of improving accuracy</span>
<span class="sd">    in various tasks such as image classification.</span>
<span class="sd">    </span>
<span class="sd">    At training time, this function is defined as</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \begin{eqnarray}</span>
<span class="sd">          \mu &amp;=&amp; \frac{1}{M} \sum x_i \\</span>
<span class="sd">          y_i &amp;=&amp; x_i - \mu</span>
<span class="sd">        \end{eqnarray}</span>
<span class="sd">    </span>
<span class="sd">    At testing time, the mean values used are those that were computed during training by moving average.</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        The backward performs an approximated differentiation that takes into account only the latest mini-batch.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array of input.</span>
<span class="sd">        rmean(~nnabla.Variable): N-D array of running mean (modified during forward execution).</span>
<span class="sd">        t(~nnabla.Variable): Scalar of num of iteration of running mean (modified during forward execution).</span>
<span class="sd">        base_axis(int): Base axis of Mean Subtraction operation. Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">            [default=``1``]</span>
<span class="sd">        update_running_mean(bool): Update running mean during forward execution.</span>
<span class="sd">            [default=``True``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">MeanSubtraction</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">update_running_mean</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">rmean</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<div class="viewcode-block" id="clip_grad_by_value"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.clip_grad_by_value">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">clip_grad_by_value</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="nb">min</span><span class="p">,</span> <span class="nb">max</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;In forward pass, the function behaves as the identity.</span>
<span class="sd">    </span>
<span class="sd">    In backward pass,</span>
<span class="sd">    </span>
<span class="sd">        .. math::</span>
<span class="sd">            g_x = \begin{cases}</span>
<span class="sd">                max &amp; (g_y &gt; max) \\</span>
<span class="sd">                g_y &amp; (otherwise) \\</span>
<span class="sd">                min &amp; (g_y &lt; min)</span>
<span class="sd">               \end{cases}.</span>
<span class="sd">               </span>
<span class="sd">    A typical case for use is to prevent the gradient explosion through a whole computational graph. </span>
<span class="sd">    For example, if you want to clip gradient values for each feature map, </span>
<span class="sd">    </span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">      </span>
<span class="sd">      x = nn.Variable([16, 3, 32, 32])</span>
<span class="sd">      min = F.broadcast(nn.Variable.from_numpy_array(np.asarray([-1.0]).reshape((1, 1, 1, 1))), (16, 3, 32, 32))</span>
<span class="sd">      max = F.broadcast(nn.Variable.from_numpy_array(np.asarray([1.0]).reshape((1, 1, 1, 1))), (16, 3, 32, 32))</span>
<span class="sd">      c = F.clip_grad_by_value(x, min=min, max=max)</span>
<span class="sd">      h = PF.convolution(c, 64, (3, 3), pad=(1, 1))</span>
<span class="sd">                             </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array of input.</span>
<span class="sd">        min(~nnabla.Variable): N-D array of minimum input value by which the gradients of the `y` are clipped. Note that the shape of `min` must be the same as `x`&#39;s and the backward to `min` is not performed.</span>
<span class="sd">        max(~nnabla.Variable): N-D array of maximum input value by which the gradients of the `y` are clipped. Note that the shape of `max` must be the same as `x`&#39;s and the backward to `max` is not performed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ClipGradByValue</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="nb">min</span><span class="p">,</span> <span class="nb">max</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="clip_grad_by_norm"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.clip_grad_by_norm">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">clip_grad_by_norm</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">clip_norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    In the forward pass, the function behaves like the identity.</span>
<span class="sd">    </span>
<span class="sd">    In the backward pass,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        g_x = N \times \frac{g_y}{\|g_y\|_2}.</span>
<span class="sd">    </span>
<span class="sd">    where :math:`g_x` is the gradient w.r.t the input, :math:`g_y` is the gradient w.r.t. the output,</span>
<span class="sd">    and :math:`N` is `clip_norm` where the norm of :math:`g_y` becomes. this is the case that `axes` is not set.</span>
<span class="sd">    When `axes` is set, the norm is computed over `axes`.</span>
<span class="sd">    </span>
<span class="sd">    A typical case for use is to prevent the gradient explosion through a whole computational graph.</span>
<span class="sd">    For example, if you want to normalize gradient values over feature axis,</span>
<span class="sd">    </span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">      x = nn.Variable([16, 3, 32, 32])</span>
<span class="sd">      c = F.clip_grad_by_norm(x, axes=(1, ))</span>
<span class="sd">      h = PF.convolution(c, 64, (3, 3), pad=(1, 1))</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array of input.</span>
<span class="sd">        clip_norm(float): Clip to the norm of input to `clip_norm` in the backward pass.</span>
<span class="sd">            [default=``1.0``]</span>
<span class="sd">        axes(repeated int64): Axes to be reduced. If empty list is given, all dimensions are reduced to scalar. This is used in the forward pass.</span>
<span class="sd">            [default=``range(x.ndim)``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">clip_norm</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">clip_norm</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="k">if</span> <span class="n">axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ClipGradByNorm</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">clip_norm</span><span class="p">,</span> <span class="n">axes</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">sum</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a matrix along a specified axis with the sum function.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        axes(repeated int64): Axes to be reduced. If empty list is given, all dimensions are reduced to scalar.</span>
<span class="sd">            [default=``range(x.ndim)``]</span>
<span class="sd">        keep_dims(bool): Flag whether the reduced axis is kept.</span>
<span class="sd">            [default=``False``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Sum</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a matrix along a specified axis with the mean function.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        axes(repeated int64): Axes to be reduced.</span>
<span class="sd">            [default=``range(x.ndim)``]</span>
<span class="sd">        keep_dims(bool): Flag whether the reduced axis is kept.</span>
<span class="sd">            [default=``False``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Mean</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">max</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduction along axis or axes with max operation.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        axes(repeated int64): Axes to be reduced.</span>
<span class="sd">            [default=``range(x.ndim)``]</span>
<span class="sd">        keep_dims(bool): Flag whether the reduced axis is kept.</span>
<span class="sd">            [default=``False``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Max</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">min</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduction along axis or axes with min operation.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        axes(repeated int64): Axes to be reduced.</span>
<span class="sd">            [default=``range(x.ndim)``]</span>
<span class="sd">        keep_dims(bool): Flag whether the reduced axis is kept.</span>
<span class="sd">            [default=``False``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Min</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">prod</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduction along axis or axes with product operation.</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        Backward computation is not accurate in a zero value input.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        axes(repeated int64): Axes to be reduced.</span>
<span class="sd">            [default=``range(x.ndim)``]</span>
<span class="sd">        keep_dims(bool): Flag whether the reduced axis is kept.</span>
<span class="sd">            [default=``False``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Prod</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<div class="viewcode-block" id="reduce_sum"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.reduce_sum">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">reduce_sum</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduction along an axis with sum operation.</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        This is deprecated. Use ``sum`` instead.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="reduce_mean"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.reduce_mean">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">reduce_mean</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduction by mean along an axis.</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        This is deprecated. Use ``mean`` instead.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="add2"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.add2">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">add2</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise addition.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = x^{(0)}_i + x^{(1)}_i</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>
<span class="sd">        inplace(bool): The output array is shared with the 1st input array if True.</span>
<span class="sd">            [default=``False``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Add2</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">bc_add2</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Note: This shouldn&#39;t be called by users.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BcAdd2</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<div class="viewcode-block" id="sub2"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.sub2">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">sub2</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise subtraction.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = x^{(0)}_i - x^{(1)}_i</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Sub2</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="mul2"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.mul2">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">mul2</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise multiplication.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = x^{(0)}_i x^{(1)}_i</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Mul2</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="div2"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.div2">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">div2</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise division.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = \frac{x^{(0)}_i} {x^{(1)}_i}</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Div2</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="pow2"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.pow2">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">pow2</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise power function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = {(x^{(0)}_i)} ^ {x^{(1)}_i}</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Pow2</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="add_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.add_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">add_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise scalar addition.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = x_i + v</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">AddScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="mul_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.mul_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">mul_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise scalar multiplication.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = v x_i</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">MulScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="pow_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.pow_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">pow_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise scalar power function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = (x_i) ^ v</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">PowScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="r_sub_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.r_sub_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">r_sub_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise scalar subtraction.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = v - x_i</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">RSubScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="r_div_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.r_div_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">r_div_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise scalar division.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \frac{v}{x_i}</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">RDivScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="r_pow_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.r_pow_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">r_pow_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise scalar power function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = v ^ {x_i}</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">RPowScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="sign"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.sign">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">sign</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise sign function.</span>
<span class="sd">    </span>
<span class="sd">    In the forward pass, it is defined as</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        f(x) = \begin{cases}</span>
<span class="sd">            1  &amp; (x &gt; 0) \\</span>
<span class="sd">            -1 &amp; (x &lt; 0) \\</span>
<span class="sd">            \alpha &amp; (x = 0)</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    In the backward pass, it is defined as</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \frac{\partial f(x)}{\partial x} = 1,</span>
<span class="sd">    </span>
<span class="sd">    or in other words, it behaves as the identity function for the gradient in the backward pass.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input</span>
<span class="sd">        alpha(float): Value in case of :math:`x = 0`.</span>
<span class="sd">            [default=``0.0``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Sign</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="minimum2"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.minimum2">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">minimum2</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise minimum.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = \min(x^{(0)}_i, x^{(1)}_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array of min value</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Minimum2</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="maximum2"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.maximum2">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">maximum2</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise maximum.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = \max(x^{(0)}_i, x^{(1)}_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array of max value</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Maximum2</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="minimum_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.minimum_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">minimum_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise scalar minimum.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \min(x_i, v)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default=``1.0``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">MinimumScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="maximum_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.maximum_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">maximum_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise scalar maximum.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \max (x_i, v)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default=``1.0``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">MaximumScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="logical_and"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.logical_and">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">logical_and</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Elementwise logical AND.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x^{(0)}_i,x^{(1)}_i) = \begin{cases}</span>
<span class="sd">            1 &amp; (x^{(0)}_i \neq 0 \;\&amp;\; x^{(1)}_i \neq 0) \\</span>
<span class="sd">            0 &amp; otherwise</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: No Description</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LogicalAnd</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="logical_or"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.logical_or">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">logical_or</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Elementwise logical OR.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x^{(0)}_i,x^{(1)}_i) = \begin{cases}</span>
<span class="sd">            0 &amp; (x^{(0)}_i = 0 \;\&amp;\; x^{(1)}_i = 0) \\</span>
<span class="sd">            1 &amp; otherwise</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: No Description</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LogicalOr</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="logical_xor"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.logical_xor">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">logical_xor</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Elementwise logical XOR.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x^{(0)}_i,x^{(1)}_i) = \begin{cases}</span>
<span class="sd">            1 &amp; (x^{(0)}_i = 0 \;\&amp;\; x^{(1)}_i = 0) \\</span>
<span class="sd">            1 &amp; (x^{(0)}_i \neq 0 \;\&amp;\; x^{(1)}_i \neq 0) \\</span>
<span class="sd">            0 &amp; otherwise</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: No Description</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LogicalXor</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="equal"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.equal">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">equal</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element wise &#39;equal&#39;</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x^{(0)}_i,x^{(1)}_i) = \begin{cases}</span>
<span class="sd">            1 &amp; (x^{(0)}_i = x^{(1)}_i) \\</span>
<span class="sd">            0 &amp; otherwise</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: No Description</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Equal</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="not_equal"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.not_equal">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">not_equal</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    </span>
<span class="sd">    Element wise &#39;not equal&#39;</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x^{(0)}_i,x^{(1)}_i) = \begin{cases}</span>
<span class="sd">            0 &amp; (x^{(0)}_i = x^{(1)}_i) \\</span>
<span class="sd">            1 &amp; otherwise</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: No Description</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">NotEqual</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="greater_equal"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.greater_equal">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">greater_equal</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element wise comparison. The :math:`i^{th}` element of the output is:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        f(x^{(0)}_i,x^{(1)}_i) = \begin{cases}</span>
<span class="sd">            1  &amp; (x^{(0)}_i \geq x^{(1)}_i) \\</span>
<span class="sd">            0 &amp; (x^{(0)}_i &lt; x^{(1)}_i)</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: No Description</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">GreaterEqual</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="greater"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.greater">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">greater</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element wise comparison. The :math:`i^{th}` element of the output is:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        f(x^{(0)}_i,x^{(1)}_i) = \begin{cases}</span>
<span class="sd">            1  &amp; (x^{(0)}_i &gt; x^{(1)}_i) \\</span>
<span class="sd">            0 &amp; (x^{(0)}_i \leq x^{(1)}_i)</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: No Description</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Greater</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="less_equal"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.less_equal">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">less_equal</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element wise comparison. The :math:`i^{th}` element of the output is:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        f(x^{(0)}_i,x^{(1)}_i) = \begin{cases}</span>
<span class="sd">            1  &amp; (x^{(0)}_i \leq x^{(1)}_i) \\</span>
<span class="sd">            0 &amp; (x^{(0)}_i &gt; x^{(1)}_i)</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: No Description</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LessEqual</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="less"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.less">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">less</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element wise comparison. The :math:`i^{th}` element of the output is:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        f(x^{(0)}_i,x^{(1)}_i) = \begin{cases}</span>
<span class="sd">            1  &amp; (x^{(0)}_i &lt; x^{(1)}_i) \\</span>
<span class="sd">            0 &amp; (x^{(0)}_i \geq x^{(1)}_i)</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array</span>
<span class="sd">        x1(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: No Description</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Less</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="logical_and_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.logical_and_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">logical_and_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Elementwise logical AND with scalar.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x_i,v) = \begin{cases}</span>
<span class="sd">            1 &amp; (x_i \neq 0 \;\&amp;\; v \neq 0) \\</span>
<span class="sd">            0 &amp; otherwise</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Input variable</span>
<span class="sd">        val(bool): No Description</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LogicalAndScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="logical_or_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.logical_or_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">logical_or_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Elementwise logical OR with scalar.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x_i,v) = \begin{cases}</span>
<span class="sd">            0 &amp; (x_i = 0 \;\&amp;\; v = 0) \\</span>
<span class="sd">            1 &amp; otherwise</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Input variable</span>
<span class="sd">        val(bool): No Description</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LogicalOrScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="logical_xor_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.logical_xor_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">logical_xor_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Elementwise logical XOR with scalar.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x_i,v) = \begin{cases}</span>
<span class="sd">            1 &amp; (x_i = 0 \;\&amp;\; v = 0) \\</span>
<span class="sd">            1 &amp; (x_i \neq 0 \;\&amp;\; v \neq 0) \\</span>
<span class="sd">            0 &amp; otherwise</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Input variable</span>
<span class="sd">        val(bool): No Description</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LogicalXorScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="equal_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.equal_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">equal_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element wise &#39;equal&#39; with a scalar</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x_i,v) = \begin{cases}</span>
<span class="sd">            1 &amp; (x_i = v) \\</span>
<span class="sd">            0 &amp; otherwise</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">EqualScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="not_equal_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.not_equal_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">not_equal_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element wise &#39;not equal&#39; with a scalar</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x_i,v) = \begin{cases}</span>
<span class="sd">            0 &amp; (x_i = v) \\</span>
<span class="sd">            1 &amp; otherwise</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">NotEqualScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="greater_equal_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.greater_equal_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">greater_equal_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element wise comparison with a scalar. The :math:`i^{th}` element of the output is:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        f(x^{(0)}_i,v) = \begin{cases}</span>
<span class="sd">            1  &amp; (x^{(0)}_i \geq v \\</span>
<span class="sd">            0 &amp; (x^{(0)}_i &lt; v</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">GreaterEqualScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="greater_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.greater_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">greater_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element wise comparison with a scalar. The :math:`i^{th}` element of the output is:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        f(x^{(0)}_i,v) = \begin{cases}</span>
<span class="sd">            1  &amp; (x^{(0)}_i &gt; v \\</span>
<span class="sd">            0 &amp; (x^{(0)}_i \leq v</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">GreaterScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="less_equal_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.less_equal_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">less_equal_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element wise comparison with a scalar. The :math:`i^{th}` element of the output is:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        f(x^{(0)}_i,v) = \begin{cases}</span>
<span class="sd">            1  &amp; (x^{(0)}_i \leq v) \\</span>
<span class="sd">            0 &amp; (x^{(0)}_i &gt; v)</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LessEqualScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="less_scalar"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.less_scalar">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">less_scalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element wise comparison with a scalar. The :math:`i^{th}` element of the output is:</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        f(x^{(0)}_i,v) = \begin{cases}</span>
<span class="sd">            1  &amp; (x^{(0)}_i &lt; v) \\</span>
<span class="sd">            0 &amp; (x^{(0)}_i \geq v)</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Input variable</span>
<span class="sd">        val(float): Value of the scalar</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LessScalar</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="logical_not"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.logical_not">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">logical_not</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise logical NOT operation</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x_i) = \begin{cases}</span>
<span class="sd">            1 &amp; (x_i = 0) \\</span>
<span class="sd">            0 &amp; otherwise</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): Input variable</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">LogicalNot</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="constant"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.constant">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">constant</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate a constant-valued array.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        val(float): Constant value.</span>
<span class="sd">            [default=``0``]</span>
<span class="sd">        shape(:obj:`tuple` of :obj:`int`): Shape of the output array.</span>
<span class="sd">            [default=``[]``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array where all values are the specified constant.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">shape</span><span class="p">)(</span><span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="abs"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.abs">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">abs</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise absolute value function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = |x_i|</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Element-wise absolute variable</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Abs</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="exp"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.exp">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">exp</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise natural exponential function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = \exp(x_i).</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Element-wise exp variable</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Exp</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="log"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.log">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">log</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise natural logarithm function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">       y_i = \ln(x_i).</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Element-wise log variable</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Log</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="identity"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.identity">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">identity</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y = x</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Identity</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="batch_matmul"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.batch_matmul">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">batch_matmul</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">transpose_a</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Batch matrix multiplication.</span>
<span class="sd">    </span>
<span class="sd">    Two of batchs of matrices are multiplied for each sample in a batch. A batch of matrices is composed as [..., P, Q] where the last two dimensions compose matrix dimensions, and the first dimensions up to the third last dimension are considered as batch samples.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        a(~nnabla.Variable): N-D array with &gt;= 2-dim. The last two dimensions will be treated as a matrix.</span>
<span class="sd">        b(~nnabla.Variable): N-D array with &gt;= 2-dim. The last two dimensions will be treated as a matrix. The product of the size of 0-th dimension through the size of the third last dimension must be same as that of the input ``a``.</span>
<span class="sd">        transpose_a(bool): Transpose the last two axes of ``a`` in matrix multiplication.</span>
<span class="sd">            [default=``False``]</span>
<span class="sd">        transpose_b(bool): Transpose the last two axes of ``b`` in matrix multiplication.</span>
<span class="sd">            [default=``False``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output of sample-wise matrix multiplication in a batch. When ``a`` is of a shape of [N, P, Q], ``b`` is of a shape of [N, Q, R], and transpose options are all False, the output will be a shape of [N, P, R].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BatchMatmul</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">transpose_a</span><span class="p">,</span> <span class="n">transpose_b</span><span class="p">)(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="round"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.round">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">round</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise round function.</span>
<span class="sd">    </span>
<span class="sd">    In the forward pass, this function simply computes `round` to the nearest integer value.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = round(x_i).</span>
<span class="sd">    </span>
<span class="sd">    In the backward pass, the simple Straight-Through Estimator (STE) is applied,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \frac{\partial y_i}{\partial x_i} = 1.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Round</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="ceil"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.ceil">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">ceil</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise ceil function.</span>
<span class="sd">    </span>
<span class="sd">    In the forward pass, this function simply returns the smallest integer which is not less than the input.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = ceil(x_i).</span>
<span class="sd">    </span>
<span class="sd">    In the backward pass, the simple Straight-Through Estimator (STE) is applied,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \frac{\partial y_i}{\partial x_i} = 1.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Ceil</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="floor"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.floor">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">floor</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise floor function.</span>
<span class="sd">    </span>
<span class="sd">    In the forward pass, this function simply returns the largest integer which is not greater than the input.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = floor(x_i).</span>
<span class="sd">    </span>
<span class="sd">    In the backward pass, the simple Straight-Through Estimator (STE) is applied,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \frac{\partial y_i}{\partial x_i} = 1.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input variable</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Floor</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="sin"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.sin">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">sin</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise sine (sin) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \sin (x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Sin</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="cos"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.cos">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">cos</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise cosine (cos) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \cos (x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Cos</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="tan"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.tan">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">tan</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise tangent (tan) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \tan (x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Tan</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="sinh"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.sinh">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">sinh</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise hyperbolic sine (sinh) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \sinh (x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Sinh</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="cosh"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.cosh">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">cosh</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise hyperbolic cosine (cosh) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \cosh (x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Cosh</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="asin"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.asin">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">asin</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise arcsine (asin) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \arcsin (x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ASin</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="acos"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.acos">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">acos</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise arccosine (acos) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \arccos (x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ACos</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="atan"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.atan">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">atan</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise arctangent (atan) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \arctan (x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ATan</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="asinh"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.asinh">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">asinh</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise hyperbolic arcsine (asinh) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \text{arcsinh} (x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ASinh</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="acosh"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.acosh">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">acosh</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise hyperbolic arccosine (acosh) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \text{arccosh} (x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ACosh</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="atanh"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.atanh">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">atanh</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise hyperbolic arctangent (atanh) function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \text{arctanh} (x_i)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ATanh</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="concatenate"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.concatenate">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">concatenate</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Concatenate a variable number of input arrays along the specified axis.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        *x(~nnabla.Variable): N-D arrays.</span>
<span class="sd">            [variadic][parameter]</span>
<span class="sd">        axis(int): Axis</span>
<span class="sd">            [default=``len(x[0].shape) - 1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Concatenate variable</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;concatenate must take more than 1 inputs&quot;</span>
    <span class="n">n_outputs</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;n_outputs&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Concatenate</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">)(</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Split arrays at the specified axis.</span>
<span class="sd">    </span>
<span class="sd">    note:</span>
<span class="sd">        This function should not be called directly when constructing models.</span>
<span class="sd">        Instead, use :meth:`nnabla.functions.split` which</span>
<span class="sd">        automatically sets `n_output` from the input&#39;s shape and axis.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        axis(int): Axis</span>
<span class="sd">            [default=``0``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: list of N-D arrays</span>
<span class="sd">            [variadic][parameter]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<div class="viewcode-block" id="stack"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.stack">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">stack</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Joins two or more arrays on a new axis.</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        Unlike :meth:`nnabla.functions.concatenate` , which joins arrays on an existing axis,</span>
<span class="sd">        Stack joins arrays on a new axis.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        *x(~nnabla.Variable): N-D arrays. The sizes of all the arrays to be stacked must be the same.</span>
<span class="sd">            [variadic][parameter]</span>
<span class="sd">        axis(int): The axis on which to concatenate arrays. Axis indices take on values 0, 1, 2, and so on from the left. For example, to stack four (3,28,28) inputs on the second axis, specify 1. In this case, the output size will be (3,4,28,28).</span>
<span class="sd">            [default=``0``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;stack must take more than 1 inputs&quot;</span>
    <span class="n">n_outputs</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;n_outputs&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Stack</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">)(</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="slice"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.slice">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">slice</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Slice arrays along specified axis.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        start(repeated int64): Start indices for each axis</span>
<span class="sd">            [default=``(0,) * len(x.shape)``]</span>
<span class="sd">        stop(repeated int64): Stop indices for each axis</span>
<span class="sd">            [default=``tuple(x.shape)``]</span>
<span class="sd">        step(repeated int64): Step indices for each axis</span>
<span class="sd">            [default=``(1,) * len(x.shape)``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Sliced N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">start</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">start</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">stop</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stop</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">step</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">step</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Slice</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">,</span> <span class="n">step</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="pad"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.pad">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">pad</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">pad_width</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">constant_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pads given N-D array with specified sizes of dimensions.</span>
<span class="sd">    The dimensions that get padded begins with the last dimension and moves forward.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        pad_width(repeated int64): </span>
<span class="sd">            n-elem tuple, where n/2 &lt;= input dimensions and n is even.</span>
<span class="sd">            len(pad_width)/2 represents the padding dimension(e.g. 1D, 2D, 3D etc.).</span>
<span class="sd">            (Currently padding upto 3D is supported)</span>
<span class="sd">            </span>
<span class="sd">            [default=``(0,) * len(x.shape)``]</span>
<span class="sd">        mode(string): </span>
<span class="sd">            Padding mode is one of the following.</span>
<span class="sd">            </span>
<span class="sd">            1) constant : Elements in pad region are filled with constant_value.</span>
<span class="sd">            2) replicate : Padded elements are filled with the values in nearest edges.</span>
<span class="sd">            3) reflect : Padded with the reflection of the vector mirrored on the first and last values of the vector along each axis.</span>
<span class="sd">            </span>
<span class="sd">            (Currently only `constant` mode is supported)</span>
<span class="sd">            </span>
<span class="sd">            [default=``&#39;constant&#39;``]</span>
<span class="sd">        constant_value(float): </span>
<span class="sd">            Constant values filled in padded regions if mode is `constant`.</span>
<span class="sd">            </span>
<span class="sd">            [default=``0``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: </span>
<span class="sd">            Padded N-D array (e.g. (B, C, H, W) shape) where dimension depends on pad_width.</span>
<span class="sd">            ndim() of output N-D array will be same as ndim() of input N-D array.</span>
<span class="sd">            </span>
<span class="sd">            -for 1D padding :</span>
<span class="sd">                     N-D input array with padding of the form (padLeft, padRight).</span>
<span class="sd">                     The output N-D array dimension (B, C, H, padLeft + W + padRight).</span>
<span class="sd">            </span>
<span class="sd">            -for 2D padding :</span>
<span class="sd">                     N-D input array with padding of the form (padTop, padBottom, padLeft, padRight).</span>
<span class="sd">                     The output N-D array dimension (B, C, padTop + H + padBottom, padLeft + W + padRight).</span>
<span class="sd">            </span>
<span class="sd">            -for 3D padding :</span>
<span class="sd">                     N-D input array with padding of the form (pasFront, padBack, padTop, padBottom, padLeft, padRight).</span>
<span class="sd">                     The output N-D array dimension (B, padFront + C + padBack, padTop + H + padBottom, padLeft + W + padRight).</span>
<span class="sd">            </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">pad_width</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pad_width</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">constant_value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">constant_value</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Pad</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">pad_width</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">constant_value</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="transpose"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.transpose">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transposes tensor dimensions.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        axes(repeated int64): Source axis indices for each axis.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Transposed N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Transpose</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axes</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="broadcast"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.broadcast">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">broadcast</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Broadcasting ND-array to the specified shape.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        shape(:obj:`tuple` of :obj:`int`): Shape broadcasted to. The size must be the same in axis where ``x``&#39;s shape is not 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Broadcasted N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Broadcast</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">shape</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="broadcast_to"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.broadcast_to">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">broadcast_to</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;.. WARNING::</span>
<span class="sd">      This function is experimental suppport, so please do not actively use it.</span>
<span class="sd">    </span>
<span class="sd">    Broadcasting ND-array to the specified buffer.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        y(~nnabla.Variable): N-D array</span>
<span class="sd">        axis(int): Target axis to start broadcasting. If this is not set, broadcast will try to fit y to x starting from the last dimension</span>
<span class="sd">            [default=``-1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Broadcasted N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BroadcastTo</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="one_hot"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.one_hot">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">one_hot</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    OneHot creates one-hot vector based on input indices.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        shape(:obj:`tuple` of :obj:`int`): No Description</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">OneHot</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">shape</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="flip"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.flip">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">flip</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the order of elements of the specified dimension of an array.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        axes(repeated int64): The index of the dimension to reverse the order of the elements. Axis indices take on values 0, 1, 2, and so on from the left. For example, to flip a 32 (W) by 24 (H) 100 RGB image (100,3,24,32) vertically and horizontally, specify (2,3).</span>
<span class="sd">            [default=``[len(x.shape) - 1]``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Flip</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axes</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="shift"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.shift">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">shift</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">shifts</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">border_mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Shifts the array elements by the specified amount.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        shifts(repeated int64): The amount to shift elements. For example, to shift image data to the right by 2 pixels and up 3 pixels, specify (-3,2).</span>
<span class="sd">            [default=``(0,) * len(x.shape)``]</span>
<span class="sd">        border_mode(string): Specify how to process the ends of arrays whose values will be undetermined as a result of shifting. nearest: The data at the ends of the original      array is copied and used. reflect: Original data reflected      at the ends of the original array is used.</span>
<span class="sd">            [default=``&#39;nearest&#39;``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">shifts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">shifts</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Shift</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">shifts</span><span class="p">,</span> <span class="n">border_mode</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="reshape"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.reshape">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">reshape</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reshapes the input variable in-place. It does not create a copy of the variable.</span>
<span class="sd">    The output variable (y) has a new shape but points to the same data as the input variable (x).</span>
<span class="sd">    This means that if the data in the output variable (y) is modified, the data in the input</span>
<span class="sd">    variable (x) also gets modified since the reshape was done in-place.</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        This function has the same behavior as the :meth:`nnabla.Variable.reshape` method.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        shape(:obj:`tuple` of :obj:`int`): Dimensions for each axis. ``-1`` can be specified only in one shape dimension. The value is calculated from the size of the array and remaining dimensions.</span>
<span class="sd">        inplace(bool): The output array is shared with the input array if True.</span>
<span class="sd">            [default=``True``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Reshaped N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Reshape</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="matrix_diag"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.matrix_diag">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">matrix_diag</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns an array where the last two dimensions consist of the diagonal matrix.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array with shape (:math:`M_0 \times \ldots \times M_N`).</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with shape (:math:`M_0 \times \ldots \times M_N \times M_N`).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">MatrixDiag</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="matrix_diag_part"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.matrix_diag_part">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">matrix_diag_part</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns an array in which the values of the last dimension consist of the diagonal</span>
<span class="sd">    elements of the last two dimensions of an input array.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array with shape (:math:`M_0 \times \ldots \times M_N \times M_N`).</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with shape (:math:`M_0 \times \ldots \times M_N`).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">MatrixDiagPart</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="dropout"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.dropout">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">dropout</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Dropout.</span>
<span class="sd">    Samples a number :math:`u` from a uniform distribution in :math:`[0, 1]` ,</span>
<span class="sd">    and ignores the input if :math:`u \leq p`.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y = \left\{</span>
<span class="sd">        \begin{array}{ll}</span>
<span class="sd">          \frac{x}{1 - p} &amp; (u &gt; p) \\</span>
<span class="sd">          0 &amp; ({\rm otherwise})</span>
<span class="sd">        \end{array} \right.</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        Usually dropout only applied during training as below</span>
<span class="sd">        (except `Bayesian dropout`_).</span>
<span class="sd">    </span>
<span class="sd">        .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">            h = PF.affine(x, num_hidden)</span>
<span class="sd">            if train:</span>
<span class="sd">                h = F.dropout(h, 0.5)</span>
<span class="sd">    </span>
<span class="sd">    .. _Bayesian dropout: https://arxiv.org/abs/1506.02142</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        p(float): :math:`p` in definition.</span>
<span class="sd">            [default=``0.5``]</span>
<span class="sd">        seed(int): Random seed. When -1, seed is sampled from global random number generator.</span>
<span class="sd">            [default=``-1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with the same shape as x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">seed</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="top_k_data"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.top_k_data">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">top_k_data</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="nb">abs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Select the `k` largest values from each sample in `x` to</span>
<span class="sd">    propagate unmodified and set all other values to 0. If `abs` is</span>
<span class="sd">    True, the `k` largest values are selected by magnitude. If</span>
<span class="sd">    `reduce` is True (the default), all feature dimensions are</span>
<span class="sd">    reduced to a single dimension of size `k` that propagates only</span>
<span class="sd">    the `k` largest values. Otherwise, if `reduce` is False, input</span>
<span class="sd">    and output dimensions are identical. Dimensions before</span>
<span class="sd">    `base_axis` are treated as number of sample dimensions and `k`</span>
<span class="sd">    values get selected from all elements of a sample (dimensions</span>
<span class="sd">    from `base_axis`) regardless of shape.</span>
<span class="sd">    </span>
<span class="sd">    &gt;&gt;&gt; import nnabla as nn, nnabla.functions as F</span>
<span class="sd">    &gt;&gt;&gt; x = nn.Variable((4, 5, 6))</span>
<span class="sd">    &gt;&gt;&gt; F.top_k_data(x, 3, reduce=False).shape</span>
<span class="sd">    (4, 5, 6)</span>
<span class="sd">    &gt;&gt;&gt; F.top_k_data(x, 3, reduce=True).shape</span>
<span class="sd">    (4, 3)</span>
<span class="sd">    &gt;&gt;&gt; F.top_k_data(x, 3, reduce=True, base_axis=2).shape</span>
<span class="sd">    (4, 5, 3)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        k(int): Number of largest data values to propagate.</span>
<span class="sd">        abs(bool): Determine largest data values by magnitude.</span>
<span class="sd">            [default=``False``]</span>
<span class="sd">        reduce(bool): Reduce feature size to one dimension of size `k`.</span>
<span class="sd">            [default=``True``]</span>
<span class="sd">        base_axis(int): First dimension of the sample shape.</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">TopKData</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="nb">abs</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="top_k_grad"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.top_k_grad">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">top_k_grad</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="nb">abs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Select the `k` largest gradients for each sample in `x` to</span>
<span class="sd">    back-propagate unmodified and set all other gradients to 0. If</span>
<span class="sd">    `abs` is True, the `k` largest gradients are selected by</span>
<span class="sd">    magnitude. Dimensions before `base_axis` are treated as number</span>
<span class="sd">    of sample dimensions and `k` gradients get selected from all</span>
<span class="sd">    gradients of a sample (dimensions from `base_axis`) regardless</span>
<span class="sd">    of shape.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        k(int): Number of largest gradients to propagate.</span>
<span class="sd">        abs(bool): Determine largest gradients by magnitude.</span>
<span class="sd">            [default=``False``]</span>
<span class="sd">        base_axis(int): First dimension of the sample shape.</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array with same shape and data as `x`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">TopKGrad</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="nb">abs</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="rand"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.rand">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">rand</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Samples numbers from a uniform distribution :math:`x \sim U(low, high)`</span>
<span class="sd">    given lowest value :math:`low`, upper bound :math:`high`,</span>
<span class="sd">    and shape of the returned Variable.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        low(float): :math:`low` in definition.</span>
<span class="sd">            [default=``0``]</span>
<span class="sd">        high(float): :math:`high` in definition.</span>
<span class="sd">            [default=``1``]</span>
<span class="sd">        shape(:obj:`tuple` of :obj:`int`): Shape of returned variable.</span>
<span class="sd">            [default=``[]``]</span>
<span class="sd">        seed(int): Random seed. When -1, seed is sampled from global random number generator.</span>
<span class="sd">            [default=``-1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Variable with the shape specified in the argument.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Rand</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">seed</span><span class="p">)(</span><span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="randint"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.randint">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">randint</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Samples integer numbers from a uniform distribution :math:`x \sim U(low, high)`</span>
<span class="sd">    given lowest value :math:`low`, upper bound :math:`high`,</span>
<span class="sd">    and shape of the returned Variable.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        low(int): :math:`low` in definition.</span>
<span class="sd">            [default=``0``]</span>
<span class="sd">        high(int): :math:`high` in definition.</span>
<span class="sd">            [default=``1``]</span>
<span class="sd">        shape(:obj:`tuple` of :obj:`int`): Shape of returned variable.</span>
<span class="sd">            [default=``[]``]</span>
<span class="sd">        seed(int): Random seed. When -1, seed is sampled from global random number generator.</span>
<span class="sd">            [default=``-1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Variable with the shape specified in the argument. The dtype is int32.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Randint</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">seed</span><span class="p">)(</span><span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="randn"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.randn">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">randn</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[],</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Samples numbers from a normal distribution :math:`x \sim N(\mu, \sigma)`</span>
<span class="sd">    given mean :math:`\mu`, standard deviation :math:`\sigma`,</span>
<span class="sd">    and shape of the returned Variable.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        mu(float): :math:`\mu` in definition.</span>
<span class="sd">            [default=``0``]</span>
<span class="sd">        sigma(float): :math:`\sigma` in definition.</span>
<span class="sd">            [default=``1``]</span>
<span class="sd">        shape(:obj:`tuple` of :obj:`int`): Shape of returned variable.</span>
<span class="sd">            [default=``[]``]</span>
<span class="sd">        seed(int): Random seed. When -1, seed is sampled from global random number generator.</span>
<span class="sd">            [default=``-1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Variable with the shape specified in the argument.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Randn</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">seed</span><span class="p">)(</span><span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="random_crop"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.random_crop">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">random_crop</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    RandomCrop randomly extracts a portion of an array.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        shape(:obj:`tuple` of :obj:`int`): The data size to extract. For example, to randomly extract a portion of the image (3,48,48) from a 3,64,64 image, specify (3,48,48).</span>
<span class="sd">            [default=``x.shape``]</span>
<span class="sd">        base_axis(int): No Description</span>
<span class="sd">            [default=``1``]</span>
<span class="sd">        seed(int): Random seed. When -1, seed is sampled from global random number generator.</span>
<span class="sd">            [default=``-1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">RandomCrop</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">seed</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="random_flip"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.random_flip">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">random_flip</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the order of elements of the specified dimension of an array at 50% probability.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        axes(repeated int64): The index of the axis to reverse the order of the elements. Axis indices take on values 0, 1, 2, and so on from the left. For example, to flip a 32 (W) by 24 (H) 100 RGB images (100, 3,24,32) vertically and horizontally at random, specify (2,3).</span>
<span class="sd">            [default=``[len(x.shape) - 1]``]</span>
<span class="sd">        base_axis(int): No Description</span>
<span class="sd">            [default=``1``]</span>
<span class="sd">        seed(int): Random seed. When -1, seed is sampled from global random number generator.</span>
<span class="sd">            [default=``-1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">RandomFlip</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">seed</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="random_shift"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.random_shift">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">random_shift</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">shifts</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">border_mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Randomly shifts the array elements within the specified range.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        shifts(repeated int64): Max absolute amount to shift elements. For example, to shift image data horizontally by :math:`\pm 2` pixels and vertically by :math:`\pm 3` pixels, specify (3,2).</span>
<span class="sd">            [default=``(0,) * len(x.shape)``]</span>
<span class="sd">        border_mode(string): Specify how to process the ends of arrays whose values will be undetermined as a result of shifting. nearest: The data at the ends of the   original array is copied and used. reflect: Original data reflected at   the ends of the original array is used.</span>
<span class="sd">            [default=``&#39;nearest&#39;``]</span>
<span class="sd">        base_axis(int): No Description</span>
<span class="sd">            [default=``1``]</span>
<span class="sd">        seed(int): Random seed. When -1, seed is sampled from global random number generator.</span>
<span class="sd">            [default=``-1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">shifts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">shifts</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">RandomShift</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">shifts</span><span class="p">,</span> <span class="n">border_mode</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">seed</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="image_augmentation"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.image_augmentation">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">image_augmentation</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">min_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">max_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">angle</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">aspect_ratio</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">distortion</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">flip_lr</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">flip_ud</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">brightness</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">brightness_each</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">contrast</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">contrast_center</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">contrast_each</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ImageAugmentation randomly alters the input image.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>
<span class="sd">        shape(:obj:`tuple` of :obj:`int`): The output image data size.</span>
<span class="sd">            [default=``x.shape``]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): Border padding values for each spatial axis. Padding will be added both sides of the dimension.</span>
<span class="sd">            [default=``(0, 0)``]</span>
<span class="sd">        min_scale(float): The minimum scale ratio when randomly scaling the image. For example, to scale down to 0.8 times the size of the original image, specify &quot;0.8&quot;. To not apply random scaling, set both min_scale and max_scale to &quot;1.0&quot;.</span>
<span class="sd">            [default=``1.0``]</span>
<span class="sd">        max_scale(float): The maximum scale ratio when randomly scaling the image. For example, to scale down to 2 times the size of the original image, specify &quot;2.0&quot;.</span>
<span class="sd">            [default=``1.0``]</span>
<span class="sd">        angle(float): The rotation angle range in radians when randomly rotating the image. The image is randomly rotated in the -Angle to +Angle range. For example, to rotate in a +-15 degree range, specify &quot;0.26&quot; (15 degrees/360 degrees * 2PI). To not apply random rotation, specify &quot;0.0&quot;.</span>
<span class="sd">            [default=``0.0``]</span>
<span class="sd">        aspect_ratio(float): The aspect ratio range when randomly deforming the image. For example, to deform aspect ratio of image from 1:1.3 to 1.3:1, specify &quot;1.3&quot;. To not apply random deforming, specify &quot;1.0&quot;.</span>
<span class="sd">            [default=``1.0``]</span>
<span class="sd">        distortion(float): The distortion range when randomly distorting the image. To not apply distortion, specify &quot;0.0&quot;.</span>
<span class="sd">            [default=``0.0``]</span>
<span class="sd">        flip_lr(bool): Whether to randomly flip the image horizontally at 50% probability.</span>
<span class="sd">            [default=``False``]</span>
<span class="sd">        flip_ud(bool): Whether to randomly flip the image vertically at 50% probability.</span>
<span class="sd">            [default=``False``]</span>
<span class="sd">        brightness(float): The absolute range of values to randomly add to the brightness. A random value in the -Brightness to +Brightness range is added to the brightness. For example, to vary the brightness in the -0.05 to +0.05 range, specify &quot;0.05&quot;. To not apply random addition to brightness, specify &quot;0.0&quot;.</span>
<span class="sd">            [default=``0.0``]</span>
<span class="sd">        brightness_each(bool): Whether to apply the random addition to brightness (as specified by brightness) to each color channel. True: brightness is added based on a different random number for each channel. False: brightness is added based on a random number common to all channels.</span>
<span class="sd">            [default=``False``]</span>
<span class="sd">        contrast(float): The range in which to randomly vary the image contrast. The contrast is varied in the 1/Contrast times to Contrast times range. The output brightness is equal to (input - contrast_center) * contrast + contrast_center. For example, to vary the contrast in the 0.91 times to 1.1 times range, specify &quot;1.1&quot;. To not apply random contrast variation, specify &quot;1.0&quot;.</span>
<span class="sd">            [default=``1.0``]</span>
<span class="sd">        contrast_center(float): Intensity center used for applying contrast.</span>
<span class="sd">            [default=``0.0``]</span>
<span class="sd">        contrast_each(bool): Whether to apply the random contrast variation (as specified by contrast) to each color channel. True: contrast is varied based on a different random number for each channel. False: contrast is varied based on a random number common to all channels.</span>
<span class="sd">            [default=``False``]</span>
<span class="sd">        noise(float): Sigma of normal random number to be added.</span>
<span class="sd">            [default=``0.0``]</span>
<span class="sd">        seed(int): Random seed. When -1, seed is sampled from global random number generator.</span>
<span class="sd">            [default=``-1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ImageAugmentation</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">min_scale</span><span class="p">,</span> <span class="n">max_scale</span><span class="p">,</span> <span class="n">angle</span><span class="p">,</span> <span class="n">aspect_ratio</span><span class="p">,</span> <span class="n">distortion</span><span class="p">,</span> <span class="n">flip_lr</span><span class="p">,</span> <span class="n">flip_ud</span><span class="p">,</span> <span class="n">brightness</span><span class="p">,</span> <span class="n">brightness_each</span><span class="p">,</span> <span class="n">contrast</span><span class="p">,</span> <span class="n">contrast_center</span><span class="p">,</span> <span class="n">contrast_each</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">seed</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="sigmoid_cross_entropy"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.sigmoid_cross_entropy">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">sigmoid_cross_entropy</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise cross entropy between `x` and the target variables, passed to a sigmoid function.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = - \left(x^{(1)}_i \ln \left(\sigma \left(x^{(0)}_i \right)\right) + \</span>
<span class="sd">        \left(1 - x^{(1)}_i\right) \ln \left(1 - \sigma \left(x^{(0)}_i \</span>
<span class="sd">        \right)\right)\right)</span>
<span class="sd">    </span>
<span class="sd">    where :math:`\sigma(s)=\frac{1}{1+\exp(-s)}`.</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        SigmoidCrossEntropy is equivalent to Sigmoid+BinaryCrossEntropy, but computing them at once has the effect of reducing computational error.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array. Typically indicates a score. The value lies in :math:`[-\infty, \infty]`</span>
<span class="sd">            [parameter]</span>
<span class="sd">        target(~nnabla.Variable): N-D array of labels. Only 0 or 1 value is allowed.</span>
<span class="sd">            [parameter]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array of element-wise losses.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">SigmoidCrossEntropy</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="binary_cross_entropy"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.binary_cross_entropy">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise cross entropy between `x` and the target variables.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = - \left(x^{(1)}_i * \ln \left(x^{(0)}_i\right) + \left(1 - \</span>
<span class="sd">        x^{(1)}_i\right) * \ln \left(1 - x^{(0)}_i\right)\right).</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Probabilities N-D array. :math:`-\infty` to :math:`\infty`.</span>
<span class="sd">        target(~nnabla.Variable): N-D array of labels. Usually set as 0 or 1, but, unlike SigmoidCrossEntropy, it allows probability (0 to 1) as inputs and backpropagation can be done.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array of element-wise losses.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BinaryCrossEntropy</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="softmax_cross_entropy"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.softmax_cross_entropy">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">softmax_cross_entropy</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise cross entropy between the variables and the variables of a label given by a category index with Softmax normalization.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_{j} = -\ln \left(\frac{\exp(x_{j,t_j})}{\sum_{i&#39;} \exp(x_{j,i&#39;})}\right)</span>
<span class="sd">    </span>
<span class="sd">    along dimension specified by axis (:math:`i` is the axis where normalization is performed on).</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        SoftmaxCrossEntropy is equivalent to Softmax+CategoricalCrossEntropy, but computing them at once has the effect of reducing computational error.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array. Typically indicates a score. :math:`(D_1 \times ... \times D_i \times ... \times D_N)`</span>
<span class="sd">            [parameter]</span>
<span class="sd">        target(~nnabla.Variable): N-D array of labels. :math:`(D_1 \times ... \times 1 \times ... \times D_N)`</span>
<span class="sd">            [parameter]</span>
<span class="sd">        axis(int): Axis normalization is taken.</span>
<span class="sd">            [default=``len(x.shape) - 1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array of element-wise losses. :math:`(D_1 \times ... \times 1 \times ... \times D_N)`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">SoftmaxCrossEntropy</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="categorical_cross_entropy"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.categorical_cross_entropy">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">categorical_cross_entropy</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise cross entropy between `x` and the target `t` where targets are given by a category index.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_{j} = -\ln \left( x_{j, t_j} \right)</span>
<span class="sd">    </span>
<span class="sd">    along dimension specified by axis (:math:`i` is the axis where normalization is performed on).</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array. Typically indicates a score. :math:`(D_1 \times ... \times D_i \times ... \times D_N)`</span>
<span class="sd">            [parameter]</span>
<span class="sd">        target(~nnabla.Variable): N-D array of labels. :math:`(D_1 \times ... \times 1 \times ... \times D_N)`</span>
<span class="sd">            [parameter]</span>
<span class="sd">        axis(int): Axis normalization is taken.</span>
<span class="sd">            [default=``len(x.shape) - 1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array of element-wise losses. :math:`(D_1 \times ... \times 1 \times ... \times D_N)`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">CategoricalCrossEntropy</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="squared_error"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.squared_error">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">squared_error</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise squared error</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \left(x^{(0)}_i - x^{(1)}_i\right)^2.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array.</span>
<span class="sd">        x1(~nnabla.Variable): N-D array.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">SquaredError</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="absolute_error"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.absolute_error">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">absolute_error</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise absolute error</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = | x^{(0)}_i - x^{(1)}_i |.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array.</span>
<span class="sd">        x1(~nnabla.Variable): N-D array.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">AbsoluteError</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="huber_loss"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.huber_loss">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">huber_loss</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise Huber loss</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i= \left\{</span>
<span class="sd">        \begin{array}{ll}</span>
<span class="sd">          d^2 &amp; (|d| &lt; \delta)\\</span>
<span class="sd">          \delta (2 |d| - \delta) &amp; ({\rm otherwise})</span>
<span class="sd">        \end{array} \right.</span>
<span class="sd">    </span>
<span class="sd">    where :math:`d = x^{(0)}_i - x^{(1)}_i`</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array.</span>
<span class="sd">        x1(~nnabla.Variable): N-D array.</span>
<span class="sd">        delta(float): Delta</span>
<span class="sd">            [default=``1.0``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array of element-wise losses.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">HuberLoss</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">delta</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="epsilon_insensitive_loss"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.epsilon_insensitive_loss">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">epsilon_insensitive_loss</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise Epsilon Insensitive Loss</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i= \left\{</span>
<span class="sd">        \begin{array}{ll}</span>
<span class="sd">          | x^{(0)}_i - x^{(1)}_i | - \epsilon &amp; if \ \ | x^{(0)}_i - x^{(1)}_i | &gt; \epsilon \\</span>
<span class="sd">    			0 &amp; otherwise</span>
<span class="sd">        \end{array} \right.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x0(~nnabla.Variable): N-D array.</span>
<span class="sd">        x1(~nnabla.Variable): N-D array.</span>
<span class="sd">        epsilon(float): Insensitive parameter.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array of element-wise losses.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">EpsilonInsensitiveLoss</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="kl_multinomial"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.kl_multinomial">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">kl_multinomial</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The Kullback Leibler Divergence for multinomial distributions.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        D = \sum_i p_i \log \left( \frac{p_i}{q_i} \right)</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        p(~nnabla.Variable): N-D array of the source categorical probabilities</span>
<span class="sd">        q(~nnabla.Variable): N-D array of the target categorical probabilities</span>
<span class="sd">        base_axis(int): Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Kullback Leibler divergence :math:`KL(p \parallel q)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">KLMultinomial</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="binary_sigmoid"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.binary_sigmoid">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">binary_sigmoid</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise binary sigmoid function. In the forward pass, it computes</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x) = \begin{cases}</span>
<span class="sd">            1 &amp; (x &gt; 0) \\</span>
<span class="sd">            0 &amp; ({\rm otherwise})\end{cases},</span>
<span class="sd">    </span>
<span class="sd">    but in the backward pass, a straight-through approximation of the gradient</span>
<span class="sd">    is used, i.e.,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \frac{\partial f(x)}{\partial x} =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            0 &amp; (|x| \geq 1) \\</span>
<span class="sd">            \frac{1}{2} &amp; ({\rm otherwise})</span>
<span class="sd">        \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">        * `Courbariaux, Matthieu, and Yoshua Bengio. Binarynet: Training deep</span>
<span class="sd">          neural networks with weights and activations constrained to+ 1 or-1.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1602.02830&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input .</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BinarySigmoid</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="binary_tanh"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.binary_tanh">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">binary_tanh</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Element-wise binary tanh function. In the forward pass, it computes</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        f(x) = \begin{cases}</span>
<span class="sd">            1 &amp; (x &gt; 0) \\</span>
<span class="sd">            -1 &amp; ({\rm otherwise})</span>
<span class="sd">        \end{cases},</span>
<span class="sd">    </span>
<span class="sd">    but in the backward pass, a straight-through approximation of the gradient</span>
<span class="sd">    is used, i.e.,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \frac{\partial f(x)}{\partial x} =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            0 &amp; (|x| \geq 1) \\</span>
<span class="sd">            1 &amp; ({\rm otherwise}) \end{cases}.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">        * `Courbariaux, Matthieu, and Yoshua Bengio. Binarynet: Training deep</span>
<span class="sd">          neural networks with weights and activations constrained to+ 1 or-1.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1602.02830&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input .</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BinaryTanh</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="binary_connect_affine"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.binary_connect_affine">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">binary_connect_affine</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">binary_weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function provides a BinaryConnect affine layer. It computes in</span>
<span class="sd">    the forward pass</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        y_j = \sum_{i} sign(w_{j,i}) x_i,</span>
<span class="sd">    </span>
<span class="sd">    i.e., the weights :math:`w_{j,i}` are binarized to :math:`sign(w_{j,i})` and,</span>
<span class="sd">    hence, each weight is in :math:`\{-1,\,1\}`. By this weight binarization, the</span>
<span class="sd">    inner product computations do not require any multiplications anymore as</span>
<span class="sd">    they turn into additions/subtractions.</span>
<span class="sd">    </span>
<span class="sd">    This function should be used together with</span>
<span class="sd">    :meth:`~nnabla.functions.batch_normalization`.</span>
<span class="sd">    </span>
<span class="sd">    .. note::</span>
<span class="sd">    </span>
<span class="sd">        1) If you would like to share the binary weights between other</span>
<span class="sd">        layers, please use the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the binary weights (`binary_weight`).</span>
<span class="sd">    </span>
<span class="sd">        2) The weights and the binary weights become in sync only after a call to</span>
<span class="sd">        :meth:`~nnabla.Variable.forward`, and not after a call to</span>
<span class="sd">        :meth:`~nnabla.Variable.backward`. If you wish to store the parameters of</span>
<span class="sd">        the network, remember to call :meth:`~nnabla.Variable.forward`, once before</span>
<span class="sd">        doing so, otherwise the weights and the binary weights will not be in sync.</span>
<span class="sd">    </span>
<span class="sd">        3) CPU and GPU implementations now use floating values for `binary_weight`,</span>
<span class="sd">        since this function is for simulation purposes.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">        * `M. Courbariaux, Y. Bengio, and J.-P. David. BinaryConnect:</span>
<span class="sd">          Training Deep Neural Networks with binary weights during propagations.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1511.00363&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input .</span>
<span class="sd">        weight(~nnabla.Variable): Weight .</span>
<span class="sd">            [parameter]</span>
<span class="sd">        binary_weight(~nnabla.Variable): Binarized weight .</span>
<span class="sd">            [parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Bias.</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        base_axis(int): Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">binary_weight</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BinaryConnectAffine</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="binary_connect_convolution"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.binary_connect_convolution">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">binary_connect_convolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">binary_weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function provides a BinaryConnect convolution layer. It computes in</span>
<span class="sd">    the forward pass</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        y_{n, a, b} = \sum_{m} \sum_{i} \sum_{j} sign(w_{n, m, i, j}) x_{m, a + i, b + j},</span>
<span class="sd">    </span>
<span class="sd">    i.e., the weights :math:`w_{n, m, i, j}` are binarized to</span>
<span class="sd">    :math:`sign(w_{n, m, i, j})` and, hence,</span>
<span class="sd">    each weight is in :math:`\{-1,\,1\}`. By this weight binarization, the</span>
<span class="sd">    inner product computations do not require any multiplications anymore as</span>
<span class="sd">    they turn into additions/subtractions.</span>
<span class="sd">    </span>
<span class="sd">    This function should be used together with :meth:`~nnabla.functions.batch_normalization`.</span>
<span class="sd">    </span>
<span class="sd">    Reference</span>
<span class="sd">    </span>
<span class="sd">        * `M. Courbariaux, Y. Bengio, and J.-P. David. BinaryConnect:</span>
<span class="sd">          Training Deep Neural Networks with binary weights during propagations.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1511.00363&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    .. note::</span>
<span class="sd">    </span>
<span class="sd">        1) If you would like to share the binary weights between other</span>
<span class="sd">        layers, please use the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the binary weights (`binary_weight`).</span>
<span class="sd">    </span>
<span class="sd">        2) The weights and the binary weights become in sync only after a call to</span>
<span class="sd">        :meth:`~nnabla.Variable.forward`, and not after a call to</span>
<span class="sd">        :meth:`~nnabla.Variable.backward`. If you wish to store the parameters of</span>
<span class="sd">        the network, remember to call :meth:`~nnabla.Variable.forward`, once before</span>
<span class="sd">        doing so, otherwise the weights and the binary weights will not be in sync.</span>
<span class="sd">    </span>
<span class="sd">        3) CPU and GPU implementations now use floating values for `binary_weight`,</span>
<span class="sd">        since this function is for simulation purposes.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input.</span>
<span class="sd">        weight(~nnabla.Variable): Weight.</span>
<span class="sd">            [parameter]</span>
<span class="sd">        binary_weight(~nnabla.Variable): Binarized weight.</span>
<span class="sd">            [parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Bias.</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        base_axis(int): Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">            [default=``1``]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">            [default=``(0,) * (len(x.shape) - (base_axis+1))``]</span>
<span class="sd">        stride(:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">            [default=``(1,) * (len(x.shape) - (base_axis+1))``]</span>
<span class="sd">        dilation(:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">            [default=``(1,) * (len(x.shape) - (base_axis+1))``]</span>
<span class="sd">        group(int): Number of groups of channels. This makes the connection across channels sparser, by grouping connections along the mapping direction.</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">pad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">dilation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">binary_weight</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BinaryConnectConvolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="binary_weight_affine"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.binary_weight_affine">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">binary_weight_affine</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">binary_weight</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function provides a Binary Weight Network affine layer. It computes in</span>
<span class="sd">    the forward pass</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        y_j = \frac{1}{\|\mathbf{w}_j\|_{\ell_1}} \sum_{i} sign(w_{j,i}) x_i</span>
<span class="sd">    </span>
<span class="sd">    i.e., the weights :math:`w_{j,i}` are binarized to :math:`sign(w_{j,i})` and,</span>
<span class="sd">    hence, each weight is in :math:`\{-1,\,1\}`. By this weight binarization, the</span>
<span class="sd">    inner product computations turn into additions/subtractions which are followed</span>
<span class="sd">    by multiplication with the scaling factor</span>
<span class="sd">    :math:`\alpha_j = \frac{1}{\|\mathbf{w}_j\|_{\ell_1}}`.</span>
<span class="sd">    </span>
<span class="sd">    Reference</span>
<span class="sd">    </span>
<span class="sd">        * `Rastegari, Mohammad, et al. XNOR-Net: ImageNet Classification Using</span>
<span class="sd">          Binary Convolutional Neural Networks.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1603.05279&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    .. note::</span>
<span class="sd">    </span>
<span class="sd">        1) If you would like to share the binary weights with other layers, please</span>
<span class="sd">        use the standard, floating value weights (`weight`) and not the binary</span>
<span class="sd">        weights (`binary_weight`).</span>
<span class="sd">    </span>
<span class="sd">        2) The weights and the binary weights become in sync only after a call to</span>
<span class="sd">        :meth:`~nnabla.Variable.forward`, and not after a call to</span>
<span class="sd">        :meth:`~nnabla.Variable.backward`. If you wish to store the parameters of</span>
<span class="sd">        the network, remember to call :meth:`~nnabla.Variable.forward`, once before</span>
<span class="sd">        doing so, otherwise the weights and the binary weights will not be in sync.</span>
<span class="sd">    </span>
<span class="sd">        3) CPU and GPU implementations now use floating values for `binary_weight`,</span>
<span class="sd">        since this function is for simulation purposes.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input .</span>
<span class="sd">        weight(~nnabla.Variable): Weight.</span>
<span class="sd">            [parameter]</span>
<span class="sd">        binary_weight(~nnabla.Variable): Binarized weight.</span>
<span class="sd">            [parameter]</span>
<span class="sd">        alpha(~nnabla.Variable): Alpha.</span>
<span class="sd">            [parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Bias.</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        base_axis(int): Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">binary_weight</span><span class="p">,</span> <span class="n">alpha</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BinaryWeightAffine</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="binary_weight_convolution"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.binary_weight_convolution">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">binary_weight_convolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">binary_weight</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function provides a Binary Weight Network convolution layer. It computes in</span>
<span class="sd">    the forward pass</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        y_{n, a, b} = \frac{1}{\|\mathbf{w}_n\|_{\ell_1}} \sum_{m} \sum_{i} \sum_{j} sign(w_{n, m, i, j}) x_{m, a + i, b + j}.</span>
<span class="sd">    </span>
<span class="sd">    i.e., the weights :math:`w_{n, m, i, j}` are binarized to</span>
<span class="sd">    :math:`sign(w_{n, m, i, j})` and, hence, each weight is in :math:`\{-1,\,1\}`.</span>
<span class="sd">    By this weight binarization, the inner product computations turn into</span>
<span class="sd">    additions/subtractions which are followed by multiplication with the scaling</span>
<span class="sd">    factor :math:`\alpha_n = \frac{1}{\|\mathbf{w}_n\|_{\ell_1}}`.</span>
<span class="sd">    </span>
<span class="sd">    Reference</span>
<span class="sd">    </span>
<span class="sd">        * `Rastegari, Mohammad, et al. XNOR-Net: ImageNet Classification Using</span>
<span class="sd">          Binary Convolutional Neural Networks.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1603.05279&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    .. note::</span>
<span class="sd">    </span>
<span class="sd">        1) If you would like to share the binary weights between other standard layers, please</span>
<span class="sd">        use the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the binary weights (`binary_weight`).</span>
<span class="sd">    </span>
<span class="sd">        2) The weights and the binary weights become in sync only after a call to</span>
<span class="sd">        :meth:`~nnabla.Variable.forward`, and not after a call to</span>
<span class="sd">        :meth:`~nnabla.Variable.backward`. If you wish to store the parameters of</span>
<span class="sd">        the network, remember to call :meth:`~nnabla.Variable.forward`, once</span>
<span class="sd">        before doing so, otherwise the weights and the binary weights will not be</span>
<span class="sd">        in sync.</span>
<span class="sd">    </span>
<span class="sd">        3) CPU and GPU implementations now use floating values for `binary_weight`,</span>
<span class="sd">        since this function is for simulation purposes.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input.</span>
<span class="sd">        weight(~nnabla.Variable): Weight.</span>
<span class="sd">            [parameter]</span>
<span class="sd">        binary_weight(~nnabla.Variable): Binarized weight.</span>
<span class="sd">            [parameter]</span>
<span class="sd">        alpha(~nnabla.Variable): Alpha.</span>
<span class="sd">            [parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Bias.</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        base_axis(int): Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">            [default=``1``]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">            [default=``(0,) * (len(x.shape) - (base_axis+1))``]</span>
<span class="sd">        stride(:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">            [default=``(1,) * (len(x.shape) - (base_axis+1))``]</span>
<span class="sd">        dilation(:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">            [default=``(1,) * (len(x.shape) - (base_axis+1))``]</span>
<span class="sd">        group(int): Number of groups of channels. This makes the connection across channels sparser, by grouping connections along the mapping direction.</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">pad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">dilation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">binary_weight</span><span class="p">,</span> <span class="n">alpha</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BinaryWeightConvolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">inq_affine</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">indicator_fixedweights</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">inq_iterations</span><span class="o">=</span><span class="p">(),</span> <span class="n">selection_algorithm</span><span class="o">=</span><span class="s1">&#39;largest_abs&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function provides a INQ affine layer. It computes in</span>
<span class="sd">    the forward pass</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        y_j = \sum_{i} w_{j,i} x_i,</span>
<span class="sd">    </span>
<span class="sd">    where the weights :math:`w_{j,i}` are quantized sequentially during</span>
<span class="sd">    training to power-of-two numbers. In the backward pass, only the non-fixed</span>
<span class="sd">    (i.e., learnable) weights are updated.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">        * `Zhou A, Yao A, Guo Y, Xu L, Chen Y. Incremental network quantization:</span>
<span class="sd">          Towards lossless CNNs with low-precision weights.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1702.03044&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input .</span>
<span class="sd">        weight(~nnabla.Variable): Weight .</span>
<span class="sd">            [parameter]</span>
<span class="sd">        indicator_fixedweights(~nnabla.Variable): Indicates which weights are already fixed (0 = not fixed, 1 = fixed) .</span>
<span class="sd">            [parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Bias.</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        base_axis(int): Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">            [default=``1``]</span>
<span class="sd">        num_bits(int): Number of bits per weight. Needs to be &gt;= 2 as two bits are used to code `zero` and sign of weight.</span>
<span class="sd">            [default=``4``]</span>
<span class="sd">        inq_iterations(repeated int64): List which specifies after how many forward passes we fix 50% of the learnable weights. If we have done as many iterations as specified in the last element of `inq_iterations`, then all weights are fixed.</span>
<span class="sd">            [default=``()``]</span>
<span class="sd">        selection_algorithm(string): Chooses algorithm that we use for selecting the weights to fix (&quot;largest_abs&quot; ... fix weights with largest absolute value, &quot;random&quot; ... fix weights randomly)</span>
<span class="sd">            [default=``&#39;largest_abs&#39;``]</span>
<span class="sd">        seed(int): Random seed. When -1, seed is sampled from global random number generator.</span>
<span class="sd">            [default=``-1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">indicator_fixedweights</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">INQAffine</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">num_bits</span><span class="p">,</span> <span class="n">inq_iterations</span><span class="p">,</span> <span class="n">selection_algorithm</span><span class="p">,</span> <span class="n">seed</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">inq_convolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">indicator_fixedweights</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">inq_iterations</span><span class="o">=</span><span class="p">(),</span> <span class="n">selection_algorithm</span><span class="o">=</span><span class="s1">&#39;largest_abs&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function provides a INQ convolution layer. It computes in</span>
<span class="sd">    the forward pass</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        y_{n, a, b} = \sum_{m} \sum_{i} \sum_{j} w_{n, m, i, j} x_{m, a + i, b + j},</span>
<span class="sd">    </span>
<span class="sd">    where the weights :math:`w_{j,i}` are quantized sequentially during</span>
<span class="sd">    training to power-of-two numbers. In the backward pass, only the non-fixed</span>
<span class="sd">    (i.e., learnable) weights are updated.</span>
<span class="sd">    </span>
<span class="sd">    Reference</span>
<span class="sd">    </span>
<span class="sd">        * `Zhou A, Yao A, Guo Y, Xu L, Chen Y. Incremental network quantization:</span>
<span class="sd">          Towards lossless CNNs with low-precision weights.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1702.03044&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input.</span>
<span class="sd">        weight(~nnabla.Variable): Weight.</span>
<span class="sd">            [parameter]</span>
<span class="sd">        indicator_fixedweights(~nnabla.Variable): Indicates which weights are already fixed (0 = not fixed, 1 = fixed) .</span>
<span class="sd">            [parameter]</span>
<span class="sd">        bias(~nnabla.Variable): Bias.</span>
<span class="sd">            [optional][parameter]</span>
<span class="sd">        base_axis(int): Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">            [default=``1``]</span>
<span class="sd">        pad(:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">            [default=``(0,) * (len(x.shape) - (base_axis+1))``]</span>
<span class="sd">        stride(:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">            [default=``(1,) * (len(x.shape) - (base_axis+1))``]</span>
<span class="sd">        dilation(:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">            [default=``(1,) * (len(x.shape) - (base_axis+1))``]</span>
<span class="sd">        group(int): Number of groups of channels. This makes the connection across channels sparser, by grouping connections along the mapping direction.</span>
<span class="sd">            [default=``1``]</span>
<span class="sd">        num_bits(int): Number of bits per weight. Needs to be &gt;= 2 as two bits are used to code `zero` and sign of weight.</span>
<span class="sd">            [default=``4``]</span>
<span class="sd">        inq_iterations(repeated int64): List which specifies after how many forward passes we fix 50% of the learnable weights. If we have done as many iterations as specified in the last element of `inq_iterations`, then all weights are fixed.</span>
<span class="sd">            [default=``()``]</span>
<span class="sd">        selection_algorithm(string): Chooses algorithm that we use for selecting the weights to fix (&quot;largest_abs&quot; ... fix weights with largest absolute value, &quot;random&quot; ... fix weights randomly)</span>
<span class="sd">            [default=``&#39;largest_abs&#39;``]</span>
<span class="sd">        seed(int): Random seed. When -1, seed is sampled from global random number generator.</span>
<span class="sd">            [default=``-1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">pad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">dilation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">base_axis</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">indicator_fixedweights</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">bias</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">INQConvolution</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">num_bits</span><span class="p">,</span> <span class="n">inq_iterations</span><span class="p">,</span> <span class="n">selection_algorithm</span><span class="p">,</span> <span class="n">seed</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">fixed_point_quantize</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sign</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="mf">0.0625</span><span class="p">,</span> <span class="n">ste_fine_grained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;This function uniformly quantizes values in fixed-point number representation.</span>
<span class="sd">    </span>
<span class="sd">    In the forward pass,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">       q_i= \left\{</span>
<span class="sd">    	   \begin{array}{ll}</span>
<span class="sd">    			max &amp; if \ \ \ x_i &gt; max \\</span>
<span class="sd">    		  sign(x_i) \times floor(|x_i| \delta^{-1} + 2^{-1}) \times \delta &amp; if \ \ min \le x_i \le max \\</span>
<span class="sd">    	  	min &amp; if \ \ x_i &lt; min \\</span>
<span class="sd">    	   \end{array} \right.,</span>
<span class="sd">    </span>
<span class="sd">    where :math:`\delta` is the step size,</span>
<span class="sd">    :math:`(min, max) :=(- (2^{n-1} - 1)\delta, (2^{n-1} - 1)\delta)` if :math:`sign` is true,</span>
<span class="sd">    :math:`(min, max) := (0, (2^n - 1) \delta)` otherwise, and</span>
<span class="sd">    :math:`n` is the total bit-width used.</span>
<span class="sd">    </span>
<span class="sd">    In the backward pass when using `ste_fine_grained` as false,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">       \frac{\partial q_i}{\partial x_i} = 1.</span>
<span class="sd">    </span>
<span class="sd">    In the backward pass when using `ste_fine_grained` as true,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">       \frac{\partial q_i}{\partial x_i}= \left\{</span>
<span class="sd">    	   \begin{array}{ll}</span>
<span class="sd">    			0 &amp; if \ \ \ x_i &gt; max \\</span>
<span class="sd">    		  1 &amp; if \ \ min \le x_i \le max \\</span>
<span class="sd">    	  	0 &amp; if \ \ x_i &lt; min \\</span>
<span class="sd">    	   \end{array} \right..</span>
<span class="sd">    </span>
<span class="sd">    .. note::</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    	Quantized values are stored as floating point number, since this function is for simulation purposes.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        sign(bool): Indicate the signed number or the unsigned number. Default is true.</span>
<span class="sd">            [default=``True``]</span>
<span class="sd">        n(int): Bit width used. Note that `sign` consumes one bit. :math:`n-1` is used for number representation in `signed` case.</span>
<span class="sd">            [default=``8``]</span>
<span class="sd">        delta(float): Step size.</span>
<span class="sd">            [default=``0.0625``]</span>
<span class="sd">        ste_fine_grained(bool): Straight Through Estimator is fine-grained or not.</span>
<span class="sd">            [default=``True``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">FixedPointQuantize</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">sign</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">ste_fine_grained</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">pow2_quantize</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sign</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">with_zero</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ste_fine_grained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function quantizes values in the power of 2 number representation,</span>
<span class="sd">    in other words, it is linear (uniform) quantization in :math:`log_2` domain.</span>
<span class="sd">    </span>
<span class="sd">    In the forward pass of `signed` case,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">       q_i= \left\{</span>
<span class="sd">    	   \begin{array}{ll}</span>
<span class="sd">    			max_{+} &amp; if \ \ \overline{q_i} &gt; max_{+} \\</span>
<span class="sd">    			\overline{q_i} &amp; if \ \ min_{+} \le \overline{q_i} \le max_{+} \\</span>
<span class="sd">    		  min_{+} &amp; if \ \ 0 \le \overline{q_i} &lt; min_{+} \\</span>
<span class="sd">    		  min_{-} &amp; if \ \ min_{-} &lt; \overline{q_i} &lt; 0 \\</span>
<span class="sd">    		  \overline{q_i} &amp; if \ \ max_{-} \le \overline{q_i} \le min_{-}\\</span>
<span class="sd">    	  	max_{-} &amp; if \ \ \overline{q_i} &lt; max_{-} \\</span>
<span class="sd">    	   \end{array} \right.,</span>
<span class="sd">    </span>
<span class="sd">    where</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">       &amp;&amp; max_{+} = 2^{m}, min_{+} = 2^{m - (2^{n-1} - 1)},\\</span>
<span class="sd">       &amp;&amp; max_{-} = -2^{m}, min_{-} = -2^{m - (2^{n-1} - 1)},\\</span>
<span class="sd">       &amp;&amp; \overline{q_i} = sign(x_i) \times 2^{round(\log_2 |x_i|)}.</span>
<span class="sd">    </span>
<span class="sd">    This quantization uses the geometric mean between two power-of-two numbers</span>
<span class="sd">    as quantization threshold.</span>
<span class="sd">    </span>
<span class="sd">    In the forward pass of `unsigned` case,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">       q_i= \left\{</span>
<span class="sd">    	   \begin{array}{ll}</span>
<span class="sd">    			max &amp; if \ \ \overline{q_i} &gt; max \\</span>
<span class="sd">    			\overline{q_i} &amp; if \ \ min \le \overline{q_i} \le max \\</span>
<span class="sd">    		  min &amp; if \ \ 0 &lt; \overline{q_i} &lt; min \\</span>
<span class="sd">    	   \end{array} \right.,</span>
<span class="sd">    </span>
<span class="sd">    where</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">       &amp;&amp; max = 2^{m}, min = 2^{m - (2^{n} - 1)},\\</span>
<span class="sd">       &amp;&amp; \overline{q_i} = 2^{int(\log_2 |x_i|)}.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    When using `with_zero` as true, a pruning threshold is used to round an input to</span>
<span class="sd">    0 or :math:`min`. The pruning threshold is defined in this function as the following,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">       pruning\ threshold = min \times 2^{-\frac{1}{2}}.</span>
<span class="sd">    </span>
<span class="sd">    If an absolute value of the input is lesser than this value, the input is rounded to 0, otherwise :math:`min`.</span>
<span class="sd">    </span>
<span class="sd">    In the backward pass when using ste_fine_grained as false,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">       \frac{\partial q_i}{\partial x_i} = 1.</span>
<span class="sd">    </span>
<span class="sd">    In the backward pass when using ste_fine_grained as true,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">       \frac{\partial q_i}{\partial x_i}= \left\{</span>
<span class="sd">    	   \begin{array}{ll}</span>
<span class="sd">    			0 &amp; if \ \ \overline{q_i} &gt; max_{+} \\</span>
<span class="sd">    			1 &amp; if \ \ otherwise \\</span>
<span class="sd">    	  	0 &amp; if \ \ \overline{q_i} &lt; max_{-} \\</span>
<span class="sd">    	   \end{array} \right..</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    There are some literatures using pow2 quantization in their proposed methods.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">    </span>
<span class="sd">      * `Miyashita Daisuke, Lee H. Edward, Murmann Boris.</span>
<span class="sd">        Convolutional Neural Networks using Logarithmic Data Representation.</span>
<span class="sd">        &lt;https://arxiv.org/abs/1603.01025&gt;`_</span>
<span class="sd">    </span>
<span class="sd">      * `Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, Yurong Chen.</span>
<span class="sd">        Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights.</span>
<span class="sd">        &lt;https://arxiv.org/abs/1702.03044&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    .. note::</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    	Quantized values are stored as floating point number, since this function is for simulation purposes.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array</span>
<span class="sd">        sign(bool): Indicate the signed number or the unsigned number. Default is true.</span>
<span class="sd">            [default=``True``]</span>
<span class="sd">        with_zero(bool): Indicate using zero as a quantized value. Default is true. Note that `zero` consumes one bit.</span>
<span class="sd">            [default=``True``]</span>
<span class="sd">        n(int): Bit width used, Note that `sign` consumes one bit. :math:`n-1` is used for number representation in `signed` case. Default is 8.</span>
<span class="sd">            [default=``8``]</span>
<span class="sd">        m(int): :math:`2^m` is the upper bound of the dynamic range and :math:`-2^m` is the lower bound, :math:`m \in \mathcal{Z}`. Default is 1.</span>
<span class="sd">            [default=``1``]</span>
<span class="sd">        ste_fine_grained(bool): Straight Through Estimator is fine-grained or not.</span>
<span class="sd">            [default=``True``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Pow2Quantize</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">sign</span><span class="p">,</span> <span class="n">with_zero</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">ste_fine_grained</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<div class="viewcode-block" id="fft"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.fft">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">fft</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">signal_ndim</span><span class="p">,</span> <span class="n">normalized</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Complex-to-complex Descrete Fourier Transform,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">      X_{k_1, \ldots, k_d} = \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x_{n_1, \ldots, n_d} \exp\left(-2 \pi j \left( \sum_{i=0}^{d} \frac{k_i n_i}{N_i} \right) \right), </span>
<span class="sd">    </span>
<span class="sd">    where</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">      k_i = 0, \ldots, N_i - 1.</span>
<span class="sd">    </span>
<span class="sd">    This function now supports 1-D, 2-D, and 3-D DFT with or without the leading batch dimentsion(s).</span>
<span class="sd">    </span>
<span class="sd">    The input is expected to be complex-valued with at least signal_ndim + 1 dimensions. </span>
<span class="sd">    The last dimension has a shape of two where x[..., 0] is the real part and x[..., 1] the imaginary part.</span>
<span class="sd">    </span>
<span class="sd">    Example:</span>
<span class="sd">      </span>
<span class="sd">    .. code-block:: python</span>
<span class="sd">    </span>
<span class="sd">      import numpy as np</span>
<span class="sd">      import nnabla as nn</span>
<span class="sd">      import nnabla.functions as F</span>
<span class="sd">      from nnabla.ext_utils import get_extension_context</span>
<span class="sd">      </span>
<span class="sd">      ctx = get_extension_context(&quot;cudnn&quot;)</span>
<span class="sd">      nn.set_default_context(ctx)</span>
<span class="sd">      </span>
<span class="sd">      # Example for a batched 2D-FFT and 2D-IFFT (batch-size: 2, data-size: 4x3)</span>
<span class="sd">      x_data = np.random.rand(2, 4, 3) + 1j * np.random.rand(2, 4, 3)</span>
<span class="sd">      x = nn.Variable.from_numpy_array(np.stack([np.real(x_data), np.imag(x_data)], axis=3))</span>
<span class="sd">      y = F.fft(x, signal_ndim=2, normalized=True)</span>
<span class="sd">      z = F.ifft(y, signal_ndim=2, normalized=True)</span>
<span class="sd">      z.forward()</span>
<span class="sd">      </span>
<span class="sd">      np.allclose(z.d[..., 0] + 1j*z.d[...,1], x_data)</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input.</span>
<span class="sd">        signal_ndim(int): The number of dimentsions for each signal. It must be 1, 2, or 3.</span>
<span class="sd">        normalized(bool): Use unitary normalization. If `True`, the normalization constant :math:`\sqrt{\frac{1}{\prod_{i=1}^{d} N_i}}` is multiplied.</span>
<span class="sd">            [default=``False``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: FFT transformed signal.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">FFT</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">signal_ndim</span><span class="p">,</span> <span class="n">normalized</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="ifft"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.ifft">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">ifft</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">signal_ndim</span><span class="p">,</span> <span class="n">normalized</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Complex-to-complex inverse Descrete Fourier Transform,</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">      X_{k_1, \ldots, k_d} = \frac{1}{\prod_{i=1}^{d} N_i} \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x_{n_1, \ldots, n_d} \exp\left(2 \pi j \left( \sum_{i=0}^{d} \frac{k_i n_i}{N_i} \right) \right), </span>
<span class="sd">    </span>
<span class="sd">    where</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">      k_i = 0, \ldots, N_i - 1.</span>
<span class="sd">    </span>
<span class="sd">    This function now supports 1-D, 2-D, and 3-D DFT with or without the leading batch dimentsion(s).</span>
<span class="sd">    </span>
<span class="sd">    The input is expected to be complex-valued with at least signal_ndim + 1 dimensions. </span>
<span class="sd">    The last dimension has a shape of two where x[..., 0] is the real part and x[..., 1] the imaginary part.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Input.</span>
<span class="sd">        signal_ndim(int): The number of dimentsions for each signal. It must be 1, 2, or 3.</span>
<span class="sd">        normalized(bool): Use unitary normalization. If `True`, the normalization constant :math:`\frac{1}{\prod_{i=1}^{d} N_i}` becomes :math:`\sqrt{\frac{1}{\prod_{i=1}^{d} N_i}}`.</span>
<span class="sd">            [default=``False``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: IFFT transformed signal.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">IFFT</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">signal_ndim</span><span class="p">,</span> <span class="n">normalized</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="top_n_error"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.top_n_error">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">top_n_error</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Top N error along the dimension specified by the axis, the element of outputs is</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">    </span>
<span class="sd">        y_i = \left \{</span>
<span class="sd">        \begin{array}{l}</span>
<span class="sd">        1 \ (x_i \ is \ not \ within \ N-th \ place) \\</span>
<span class="sd">        0 \ (x_i \ is \ within \ N-th \ place)</span>
<span class="sd">        \end{array}</span>
<span class="sd">        \right.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Probabilities N-D array. :math:`D_1 \times ... \times D_i \times ... \times D_N`</span>
<span class="sd">        target(~nnabla.Variable): N-D array of labels. :math:`D_1 \times ... \times 1 \times ... \times D_N`</span>
<span class="sd">        axis(int): Axis on which the top N error is calculated.</span>
<span class="sd">            [default=``len(x.shape) - 1``]</span>
<span class="sd">        n(int): top N</span>
<span class="sd">            [default=``1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Element-wise error N-D array. (:math:`D_1 \times ... \times 1 \times ... \times D_N`)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">TopNError</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">n</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">binary_error</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Elementwise binary error.</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \left \{</span>
<span class="sd">        \begin{array}{l}</span>
<span class="sd">        0 ((x^{(0)} \geq 0.5) = (x^{(1)} \geq 0.5)) \\</span>
<span class="sd">        1 ((x^{(0)} \geq 0.5) \neq (x^{(1)} \geq 0.5))</span>
<span class="sd">        \end{array}</span>
<span class="sd">        \right.</span>
<span class="sd">    </span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Probabilities N-D array. \f$-\infty\f$ to \f$\infty\f$.</span>
<span class="sd">        target(~nnabla.Variable): Labels N-D array. Usually set as 0 or 1, but, it allows probability (0 to 1) as inputs.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Element-wise errors N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">BinaryError</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">confusion_matrix</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Confusion matrix.</span>
<span class="sd">    The return value is already summed over samples.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): Probabilities N-D array. (\f$D_1 \times ... \times D_i \times ... \times D_N\f$)</span>
<span class="sd">        target(~nnabla.Variable): Labels N-D array. (\f$D_1 \times ... \times 1 \times ... \times D_N\f$)</span>
<span class="sd">        axis(int): Axis on which the confusion matrix is calculated.</span>
<span class="sd">            [default=``len(x.shape) - 1``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Confusion matrix 2-D array. Col index is estimated class. Row index is label class.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ConfusionMatrix</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">axis</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>



<div class="viewcode-block" id="vat_noise"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.vat_noise">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">vat_noise</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Noise for virtual adversarial training.</span>
<span class="sd">    </span>
<span class="sd">    This layer is a special layer for GUI network designing, specialized for getting</span>
<span class="sd">    the noise of virtual adversarial training.</span>
<span class="sd">    </span>
<span class="sd">    In the backward process, the weight parameter will be replaced with the gradient.</span>
<span class="sd">    </span>
<span class="sd">    Forward</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = \frac{\epsilon x_i}{\sqrt{\sum_k x_k^2 + c}}</span>
<span class="sd">    </span>
<span class="sd">    Backward</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \delta x_i = 0</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        w_i = \epsilon \delta y_i</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        This layer is a special layer for GUI network designing.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">        * `Miyato et.al, Distributional Smoothing with Virtual Adversarial Training.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1507.00677&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array of noise input. Noise is standard Gaussian noise initially, but the next step, fed back gradient variable.</span>
<span class="sd">        w(~nnabla.Variable): N-D array for keep gradient values.</span>
<span class="sd">        base_axis(int): Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">            [default=``1``]</span>
<span class="sd">        eps(float): Noise norm (l2) factor.</span>
<span class="sd">            [default=``1.0``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">VATNoise</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">eps</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="unlink"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.unlink">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">unlink</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function behaves as an identity function on the forward pass,</span>
<span class="sd">    and deletes the gradient for the background pass.</span>
<span class="sd">    </span>
<span class="sd">    This layer is a special layer for GUI network designing, used for getting</span>
<span class="sd">    zero backward operation by adding this layer.</span>
<span class="sd">    </span>
<span class="sd">    Forward</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        y_i = x_i</span>
<span class="sd">    </span>
<span class="sd">    Backward</span>
<span class="sd">    </span>
<span class="sd">    .. math::</span>
<span class="sd">        \delta x_i = 0</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        This layer is a special layer for GUI network designing.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Unlink</span><span class="p">(</span><span class="n">ctx</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="sink"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.sink">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">sink</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a dummy variable used to call forward or backward function</span>
<span class="sd">    of multiple variables at one place.</span>
<span class="sd">    </span>
<span class="sd">    This takes any numbers of input variables with any shape,</span>
<span class="sd">    and creates a single 0-shape outputs.</span>
<span class="sd">    The forward pass does nothing. The backward pass set ones</span>
<span class="sd">    to the input grads if one_input_grad is set as true.</span>
<span class="sd">    </span>
<span class="sd">    Note:</span>
<span class="sd">        ``sink`` can only be called at the very end of the graph, and</span>
<span class="sd">        ``grad`` of input variables are cleared</span>
<span class="sd">         when ``y.backward(clear_buffer=True)`` is called.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        *x(~nnabla.Variable): Any number of inputs with any shape.</span>
<span class="sd">            [variadic]</span>
<span class="sd">        one_input_grad(bool): Set grads of inputs as one during backward. It is useful to set false if you want to set external gradients to the input variables.</span>
<span class="sd">            [default=``True``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Dummy variable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;sink must take more than 1 inputs&quot;</span>
    <span class="n">n_outputs</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;n_outputs&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;outputs&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">one_input_grad</span> <span class="o">=</span> <span class="n">kw</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;one_input_grad&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">Sink</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">one_input_grad</span><span class="p">)(</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>



<div class="viewcode-block" id="nms_detection2d"><a class="viewcode-back" href="../../python/api/function.html#nnabla.functions.nms_detection2d">[docs]</a><span class="nd">@function_api</span>
<span class="k">def</span> <span class="nf">nms_detection2d</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">thresh</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nms</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nms_per_class</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Non-Maximum Suppression (NMS) to 2D Object detector output.</span>
<span class="sd">    The input is a 3-dimensional tensor with shape of ``(B, N, 5 + C)``</span>
<span class="sd">    where ``B`` denotes batch size, ``N`` denotes the number of detection box</span>
<span class="sd">    candidates, and ``C`` denotes the number of classes of object detection.</span>
<span class="sd">    ``5 + C`` consists of the box coordinates ``x, y, w, h`` in normalized</span>
<span class="sd">    coordinates (size of each x and y are 1.0), objectness</span>
<span class="sd">    (learned to predict IoU value to ground truth box), and the class</span>
<span class="sd">     probabilities of ``C`` classes.</span>
<span class="sd">    It outputs a tensor with the same dimensions as the input, where all</span>
<span class="sd">    values are copied from the input to the output, except the class</span>
<span class="sd">    probabilities are multiplied by objectness, and possibly suppressed to 0</span>
<span class="sd">    by NMS.</span>
<span class="sd">    During NMS, all of combination of pairs of bounding boxes is compared.</span>
<span class="sd">    For each pair, the bounding box with a lower detection score</span>
<span class="sd">    (described below) is suppressed if the overlap ratio (the IoU)</span>
<span class="sd">    is greater than the value of ``nms``.</span>
<span class="sd">    </span>
<span class="sd">    There are two suppression modes for NMS.</span>
<span class="sd">    </span>
<span class="sd">    1. Suppress by class probability (``nms_per_class`` is ``True``):</span>
<span class="sd">    For each bounding box, the detection score is calculated by</span>
<span class="sd">    ``objectness * probability[class_id]`` for each class.</span>
<span class="sd">    The suppression is done for each class independently.</span>
<span class="sd">    </span>
<span class="sd">    2. Suppress by objectness (``nms_per_class`` is ``False``):</span>
<span class="sd">    The suppression is done for each bounding box using ``objectness``</span>
<span class="sd">    as a detection score. All class probabilities becomes 0 for</span>
<span class="sd">    every suppressed boxes.</span>
<span class="sd">    </span>
<span class="sd">    References:</span>
<span class="sd">        * `Joseph Redmon, Ali Farhadi, YOLO9000: Better, Faster, Stronger.</span>
<span class="sd">          &lt;https://arxiv.org/abs/1612.08242&gt;`_</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): A 3-dimensional array.</span>
<span class="sd">        thresh(float): Detection score threshold.</span>
<span class="sd">            [default=``0.5``]</span>
<span class="sd">        nms(float): IoU threshold for Non-maximum suppression (NMS).</span>
<span class="sd">            [default=``0.45``]</span>
<span class="sd">        nms_per_class(bool): If true, NMS is applied for each class.</span>
<span class="sd">            [default=``True``]</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: A 3-dim array with the same dimensions with the input.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">thresh</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">thresh</span> <span class="o">=</span> <span class="mf">0.5</span>
    <span class="k">if</span> <span class="n">nms</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">nms</span> <span class="o">=</span> <span class="mf">0.45</span>
    <span class="k">if</span> <span class="n">nms_per_class</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">nms_per_class</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">NmsDetection2d</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">thresh</span><span class="p">,</span> <span class="n">nms</span><span class="p">,</span> <span class="n">nms_per_class</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_outputs</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span> <span class="n">auto_forward</span><span class="o">=</span><span class="n">get_auto_forward</span><span class="p">(),</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span></div>

</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Sony Corporation

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>