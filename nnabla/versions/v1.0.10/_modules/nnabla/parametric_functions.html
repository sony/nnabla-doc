

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>nnabla.parametric_functions &mdash; Neural Network Libraries 1.0.10.dev1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> Neural Network Libraries
          

          
          </a>

          
            
            
              <div class="version">
                1.0.10.dev1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../python.html">Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cpp.html">C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data_exchange_file_format.html">Data exchange file format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../format.html">Data Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python/file_format_converter/file_format_converter.html">File format converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">Contributing Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../license.html">License</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Neural Network Libraries</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>nnabla.parametric_functions</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for nnabla.parametric_functions</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) 2017 Sony Corporation. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>

<span class="kn">from</span> <span class="nn">six</span> <span class="k">import</span> <span class="n">exec_</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">nnabla</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">nnabla.functions</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">nnabla.parameter</span> <span class="k">import</span> <span class="n">get_parameter_or_create</span><span class="p">,</span> <span class="n">get_parameter</span>
<span class="kn">from</span> <span class="nn">nnabla.initializer</span> <span class="k">import</span> <span class="p">(</span>
    <span class="n">calc_uniform_lim_glorot</span><span class="p">,</span>
    <span class="n">ConstantInitializer</span><span class="p">,</span> <span class="n">NormalInitializer</span><span class="p">,</span> <span class="n">UniformInitializer</span><span class="p">)</span>


<div class="viewcode-block" id="parametric_function_api"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.parametric_function_api">[docs]</a><span class="k">def</span> <span class="nf">parametric_function_api</span><span class="p">(</span><span class="n">scope_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">param_desc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Decorator for parametric functions.</span>

<span class="sd">    The decorated function is always called under</span>
<span class="sd">    a parameter scope ``scope_name``.</span>
<span class="sd">    Also, the decorator adds an additional argument ``name`` (:obj:`str`,</span>
<span class="sd">    default is ``None``) at the end. If ``name`` is specified, the</span>
<span class="sd">    scope ``scope_name`` comes under a scope ``name``. This feature</span>
<span class="sd">    could reduce vertical space usage of the source code.</span>
<span class="sd">    Any parametric function should be decorated by this.</span>

<span class="sd">    Args:</span>
<span class="sd">        scope_name (str, optional): The original function will be called</span>
<span class="sd">            under a parameter scope named by ``scope_name``.</span>
<span class="sd">        param_desc (list, optional):</span>
<span class="sd">            Descriptions of parameters will be automatically included into docstring.</span>
<span class="sd">            This must be a list of tuples with 4 elements composed of</span>
<span class="sd">            (name (str), description (str), shape info (str), need_grad (bool)).</span>

<span class="sd">    Returns:</span>
<span class="sd">        function: A decorated parametric function.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">scope_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scope_name</span> <span class="o">=</span> <span class="n">name</span>

    <span class="k">def</span> <span class="nf">parametric_function_api_inside</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">nnabla.utils.py23_compatible</span> <span class="k">import</span> <span class="n">getargspec</span>
        <span class="kn">import</span> <span class="nn">inspect</span>

        <span class="n">name</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="n">doc</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="vm">__doc__</span>

        <span class="k">if</span> <span class="n">param_desc</span><span class="p">:</span>
            <span class="n">indent</span> <span class="o">=</span> <span class="mi">8</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">desc</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">d</span><span class="p">:</span> <span class="s1">&#39; &#39;</span> <span class="o">*</span> <span class="n">indent</span> <span class="o">+</span>
                           <span class="s1">&#39;* </span><span class="si">{}</span><span class="s1"> (``need_grad=</span><span class="si">{}</span><span class="s1">``) : </span><span class="si">{}</span><span class="s1">. (shape: ``</span><span class="si">{}</span><span class="s1">``)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">d</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">d</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">d</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span> <span class="n">param_desc</span><span class="p">)</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;param_desc argument of parametric_function_api must be &#39;</span>
                    <span class="s1">&#39;None or a list of tuple with three elements composed of &#39;</span>
                    <span class="s1">&#39;(name(str), description(str), need_grad(bool)).&#39;</span><span class="p">)</span>
            <span class="n">doc</span> <span class="o">+=</span> <span class="s1">&#39;&#39;&#39;</span>
<span class="s1">    Parameters to be registered</span>
<span class="s1">        The following variables are registered in a parameter scope ``&quot;</span><span class="si">{}</span><span class="s1">&quot;``;</span>

<span class="si">{}</span><span class="s1"></span>

<span class="s1">            &#39;&#39;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">scope_name</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">desc</span><span class="p">))</span>

        <span class="n">doc</span> <span class="o">+=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Note:</span>

<span class="s2">        If the ``name`` option is passed, the parameters become wrapped inside the parameter scope</span>
<span class="s2">        with the specified name, yielding the same results as the following code.</span>
<span class="s2">        This can be used to simplify the code.</span>

<span class="s2">        .. code-block:: python</span>

<span class="s2">            with parametric_scope(name):</span>
<span class="s2">                output = </span><span class="si">{name}</span><span class="s2">(&lt;args&gt;)</span>

<span class="s2">        &quot;&quot;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

        <span class="n">spec</span> <span class="o">=</span> <span class="n">getargspec</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
        <span class="n">defaults</span> <span class="o">=</span> <span class="n">spec</span><span class="o">.</span><span class="n">defaults</span>
        <span class="k">if</span> <span class="n">defaults</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">defaults</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">()</span>  <span class="c1"># None will be appended later</span>
        <span class="n">signature</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">formatargspec</span><span class="p">(</span>
            <span class="n">spec</span><span class="o">.</span><span class="n">args</span> <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">],</span>
            <span class="n">spec</span><span class="o">.</span><span class="n">varargs</span><span class="p">,</span> <span class="n">spec</span><span class="o">.</span><span class="n">keywords</span><span class="p">,</span>
            <span class="n">defaults</span> <span class="o">+</span> <span class="p">(</span><span class="kc">None</span><span class="p">,))</span>
        <span class="n">shortsignature</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">formatargspec</span><span class="p">(</span>
            <span class="n">spec</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="n">spec</span><span class="o">.</span><span class="n">varargs</span><span class="p">,</span> <span class="n">spec</span><span class="o">.</span><span class="n">keywords</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="c1"># Check required argument</span>
        <span class="k">assert</span> <span class="s1">&#39;fix_parameters&#39;</span> <span class="ow">in</span> <span class="n">spec</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> \
            <span class="s2">&quot;A parametric function must take `fix_parameters` as an argument.&quot;</span> \
            <span class="s2">&quot; `</span><span class="si">{}{}</span><span class="s2">` doesn&#39;t have it.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">signature</span><span class="p">)</span>

        <span class="n">code</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">def </span><span class="si">{name}{signature}</span><span class="s2">:</span>
<span class="s2">    if name is None:</span>
<span class="s2">        with parameter_scope(scope_name):</span>
<span class="s2">            return func</span><span class="si">{shortsignature}</span><span class="s2"></span>
<span class="s2">    with parameter_scope(name):</span>
<span class="s2">        with parameter_scope(scope_name):</span>
<span class="s2">            return func</span><span class="si">{shortsignature}</span><span class="s2"></span>
<span class="s2">        &quot;&quot;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="nb">locals</span><span class="p">())</span>
        <span class="n">execdict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="n">func</span><span class="o">=</span><span class="n">func</span><span class="p">,</span> <span class="n">parameter_scope</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">parameter_scope</span><span class="p">,</span> <span class="n">scope_name</span><span class="o">=</span><span class="n">scope_name</span><span class="p">)</span>
        <span class="n">exec_</span><span class="p">(</span><span class="n">code</span><span class="p">,</span> <span class="n">execdict</span><span class="p">)</span>
        <span class="n">newfunc</span> <span class="o">=</span> <span class="n">execdict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
        <span class="n">newfunc</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">doc</span>
        <span class="n">newfunc</span><span class="o">.</span><span class="n">__parametric_function_api_base__</span> <span class="o">=</span> <span class="n">func</span>
        <span class="n">newfunc</span><span class="o">.</span><span class="n">__scope_name__</span> <span class="o">=</span> <span class="n">scope_name</span>
        <span class="n">newfunc</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">=</span> <span class="vm">__name__</span>
        <span class="k">return</span> <span class="n">newfunc</span>
    <span class="k">return</span> <span class="n">parametric_function_api_inside</span></div>


<div class="viewcode-block" id="affine"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.affine">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;affine&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Weight matrix&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outputs,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span>
           <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
           <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
           <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The affine layer, also known as the fully connected layer. Computes</span>

<span class="sd">    .. math::</span>
<span class="sd">        {\\mathbf y} = {\\mathbf A} {\\mathbf x} + {\\mathbf b}.</span>

<span class="sd">    where :math:`{\\mathbf x}, {\\mathbf y}` are the inputs and outputs respectively,</span>
<span class="sd">    and :math:`{\\mathbf A}, {\\mathbf b}` are constants.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): Input N-D array with shape (:math:`M_0 \\times \ldots \\times M_{B-1} \\times D_B \\times \ldots \\times D_N`). Dimensions before and after base_axis are flattened as if it is a matrix.</span>
<span class="sd">        n_outmaps (:obj:`int` or :obj:`tuple` of :obj:`int`): Number of output neurons per data.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.</span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: :math:`(B + 1)`-D array. (:math:`M_0 \\times \ldots \\times M_{B-1} \\times L`)f</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">,</span> <span class="s1">&#39;__iter__&#39;</span><span class="p">):</span>
        <span class="n">n_outmaps</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_outmaps</span><span class="p">]</span>
    <span class="n">n_outmaps</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">)</span>
    <span class="n">n_outmap</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inmaps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inmaps</span><span class="p">,</span> <span class="n">n_outmap</span><span class="p">),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="svd_affine"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.svd_affine">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;svd_affine&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;U&#39;</span><span class="p">,</span> <span class="s1">&#39;:math:`{</span><span class="se">\\</span><span class="s1">mathbf U}`&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, r)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;V&#39;</span><span class="p">,</span> <span class="s1">&#39;:math:`{</span><span class="se">\\</span><span class="s1">mathbf V}`&#39;</span><span class="p">,</span> <span class="s1">&#39;(r, outmaps)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">svd_affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">uv_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;SVD affine is a low rank approximation of the affine layer. It can</span>
<span class="sd">    be seen as two consecutive affine layers with a bottleneck. It</span>
<span class="sd">    computes:</span>

<span class="sd">    .. math::</span>
<span class="sd">        {\\mathbf y} = {\\mathbf U} {\\mathbf V} {\\mathbf x} + {\\mathbf b}.</span>

<span class="sd">    where :math:`{\\mathbf x}, {\\mathbf y}` are the inputs and</span>
<span class="sd">    outputs respectively, and :math:`{\\mathbf U}, {\\mathbf V},</span>
<span class="sd">    {\\mathbf b}` are constants.</span>

<span class="sd">    The weights :math:`{\\mathbf U}` and :math:`{\\mathbf V}` are</span>
<span class="sd">    approximated with singular value decomposition (SVD) of the</span>
<span class="sd">    original weight matrix :math:`{\\mathbf W}` and by selecting the</span>
<span class="sd">    :math:`{R}` dominant singular values and the corresponding</span>
<span class="sd">    singular vectors. Therefore the low rank :math:`{R}` is the size</span>
<span class="sd">    of the bottleneck.</span>

<span class="sd">    If `uv_init` is a numpy array, :math:`{\\mathbf U}` and</span>
<span class="sd">    :math:`{\\mathbf V}` are computed such that `uv_init` is</span>
<span class="sd">    approximated by :math:`{\\mathbf{UV}}`. If `uv_init` is `None` or</span>
<span class="sd">    an initializer, the product of :math:`{\\mathbf U}` and</span>
<span class="sd">    :math:`{\\mathbf V}` approximates the random initialization.</span>

<span class="sd">    If :math:`{\\mathbf U}` and :math:`{\\mathbf V}` exist in the context,</span>
<span class="sd">    they take precedence over `uv_init`.</span>

<span class="sd">    Suppose the weight of the affine is of :math:`{I \\times O}` and</span>
<span class="sd">    the compression rate you want to specify is :math:`{CR}`, then you</span>
<span class="sd">    set :math:`{R}` as</span>

<span class="sd">    .. math::</span>

<span class="sd">        R = \\left\\lfloor \\frac{(1 - CR)OI}{O + I} \\right\\rfloor.</span>

<span class="sd">    Args:</span>

<span class="sd">        inp (~nnabla.Variable): Input N-D array with shape (:math:`M_0</span>
<span class="sd">          \\times \ldots \\times M_{B-1} \\times D_B \\times \ldots</span>
<span class="sd">          \\times D_N`). Dimensions before and after base_axis are</span>
<span class="sd">          flattened as if it is a matrix.</span>

<span class="sd">        n_outmaps (int or tuple): Number of output neurons per data.</span>

<span class="sd">        r (int): rank of the factorized layer (size of the bottleneck)</span>

<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as</span>
<span class="sd">          the sample dimensions.</span>

<span class="sd">        uv_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`):</span>
<span class="sd">          Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`. </span>

<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights</span>
<span class="sd">          and biases will not be updated.</span>

<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>

<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: :math:`(B + 1)`-D array.</span>
<span class="sd">        (:math:`M_0 \\times \ldots \\times M_{B-1} \\times L`)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">,</span> <span class="s1">&#39;__iter__&#39;</span><span class="p">):</span>
        <span class="n">n_outmaps</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_outmaps</span><span class="p">]</span>

    <span class="n">n_outmaps</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">)</span>
    <span class="n">n_outmap</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">))</span>
    <span class="n">inmaps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])</span>

    <span class="k">if</span> <span class="n">uv_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">uv_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inmaps</span><span class="p">,</span> <span class="n">n_outmap</span><span class="p">),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">uv_init</span><span class="p">)</span> <span class="ow">is</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="c1"># TODO: Assert that size of uv_init is correct</span>
        <span class="c1"># uv is initialize with numpy array</span>
        <span class="n">uv</span> <span class="o">=</span> <span class="n">uv_init</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># uv is initialize from initializer</span>
        <span class="n">uv</span> <span class="o">=</span> <span class="n">uv_init</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])),</span> <span class="p">]</span> <span class="o">+</span>
                     <span class="nb">list</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">))</span>

    <span class="n">u</span> <span class="o">=</span> <span class="n">get_parameter</span><span class="p">(</span><span class="s1">&#39;U&#39;</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">get_parameter</span><span class="p">(</span><span class="s1">&#39;V&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">u</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">r</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;svd_ffine: The rank must larger than zero&quot;</span>
        <span class="n">u_</span><span class="p">,</span> <span class="n">s_</span><span class="p">,</span> <span class="n">v_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">uv</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">inmaps</span><span class="p">,</span> <span class="n">n_outmap</span><span class="p">),</span>
                                   <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">u_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u_</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s_</span><span class="p">))</span>  <span class="c1"># fold s into u</span>
        <span class="n">u_</span> <span class="o">=</span> <span class="n">u_</span><span class="p">[:,</span> <span class="p">:</span><span class="n">r</span><span class="p">]</span>
        <span class="n">v_</span> <span class="o">=</span> <span class="n">v_</span><span class="p">[:</span><span class="n">r</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">v_</span> <span class="o">=</span> <span class="n">v_</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="n">r</span><span class="p">]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">)</span>

        <span class="n">u</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])),</span> <span class="n">r</span><span class="p">],</span>
                        <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">u</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">u_</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">set_parameter</span><span class="p">(</span><span class="s2">&quot;U&quot;</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span>

        <span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="n">r</span><span class="p">]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">v</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">v_</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">set_parameter</span><span class="p">(</span><span class="s2">&quot;V&quot;</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fix_parameters</span> <span class="o">==</span> <span class="n">u</span><span class="o">.</span><span class="n">need_grad</span><span class="p">:</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">u</span><span class="o">.</span><span class="n">get_unlinked_variable</span><span class="p">(</span><span class="n">need_grad</span><span class="o">=</span><span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fix_parameters</span> <span class="o">==</span> <span class="n">v</span><span class="o">.</span><span class="n">need_grad</span><span class="p">:</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">get_unlinked_variable</span><span class="p">(</span><span class="n">need_grad</span><span class="o">=</span><span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
        <span class="n">v</span><span class="o">.</span><span class="n">need_grad</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">fix_parameters</span>

    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>

    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="n">base_axis</span><span class="p">),</span>
                    <span class="n">v</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="n">base_axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="binary_connect_affine"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.binary_connect_affine">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;bicon_affine&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Weight matrix in floating type&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;Wb&#39;</span><span class="p">,</span> <span class="s1">&#39;Binarized weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">binary_connect_affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span>
                          <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">wb_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Binary Connect Affine, multiplier-less inner-product.</span>

<span class="sd">    Binary Connect Affine is an affine function,</span>
<span class="sd">    except the definition of the inner product is modified.</span>
<span class="sd">    The input-output relation of this function is as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_i = \sum_{i} sign(w_i) x_i.</span>

<span class="sd">    Therefore :math:`sign(w_i)` is either :math:`1` or :math:`-1` and the inner product</span>
<span class="sd">    simplifies to addition.</span>

<span class="sd">    This function should be used together with Batch Normalization.</span>

<span class="sd">    References:</span>

<span class="sd">        M. Courbariaux, Y. Bengio, and J.-P. David. &quot;BinaryConnect:</span>
<span class="sd">        Training Deep Neural Networks with binary weights during propagations.&quot;</span>
<span class="sd">        Advances in Neural Information Processing Systems. 2015.</span>

<span class="sd">    .. note::</span>

<span class="sd">        1) if you would like to share weights between some layers, please</span>
<span class="sd">        make sure to share the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the binarized weights (`binary_weight`)</span>

<span class="sd">        2) The weights and the binary weights become synced only after :func:`~nnabla._variable.Variable.forward` is called,</span>
<span class="sd">        and not after a call to :func:`~nnabla._variable.Variable.backward`.</span>
<span class="sd">        To access the parameters of the network, remember to call :func:`~nnabla._variable.Variable.forward` once before doing so, otherwise the</span>
<span class="sd">        float weights and the binary weights will not be in sync.</span>

<span class="sd">        3) Quantized values are stored as floating point number for `binary_weight`,</span>
<span class="sd">        since this function is only for simulation purposes.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): Input N-D array with shape (:math:`M_0 \\times \ldots \\times M_{B-1} \\times D_B \\times \ldots \\times D_N`). Dimensions before and after base_axis are flattened as if it is a matrix.</span>
<span class="sd">        n_outmaps (int or :obj:`tuple` of :obj:`int`): Number of output neurons per data.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.  </span>
<span class="sd">        wb_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for binary weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.   </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">,</span> <span class="s1">&#39;__iter__&#39;</span><span class="p">):</span>
        <span class="n">n_outmaps</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_outmaps</span><span class="p">]</span>
    <span class="n">n_outmaps</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">)</span>
    <span class="n">n_outmap</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fan_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">fan_in</span><span class="p">,</span> <span class="n">n_outmap</span><span class="p">),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">wb_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fan_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])</span>
        <span class="n">wb_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">fan_in</span><span class="p">,</span> <span class="n">n_outmap</span><span class="p">),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">wb</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;Wb&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
        <span class="n">wb_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_connect_affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">wb</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="binary_weight_affine"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.binary_weight_affine">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;bwn_affine&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Weight matrix in floating type&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;Wb&#39;</span><span class="p">,</span> <span class="s1">&#39;Binarized weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="s1">&#39;Scaling factor :math:`</span><span class="se">\\</span><span class="s1">alpha`&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">binary_weight_affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span>
                         <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                         <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">wb_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Binary Weight Affine, multiplier-less inner-product with a scale factor.</span>

<span class="sd">    Binary Weight Affine is the affine function, but the inner product</span>
<span class="sd">    in this function is the following,</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_j = \\frac{1}{\\|\\mathbf{w}_j\\|_{\\ell_1}} \sum_{i} sign(w_{ji}) x_i</span>

<span class="sd">    Therefore :math:`sign(w_{ji})` is either :math:`1` or :math:`-1` and the inner product</span>
<span class="sd">    simplifies to addition followed by scaling factor :math:`\\alpha = \\frac{1}{\\|\\mathbf{w}_j\\|_{\\ell_1}}`.</span>
<span class="sd">    The number of ::math:`\\alpha` is the outmaps of the affine function.</span>

<span class="sd">    References:</span>

<span class="sd">        Rastegari, Mohammad, et al. &quot;XNOR-Net: ImageNet Classification Using</span>
<span class="sd">        Binary Convolutional Neural Networks.&quot; arXiv preprint</span>
<span class="sd">        arXiv:1603.05279 (2016).</span>

<span class="sd">    .. note::</span>

<span class="sd">        1) if you would like to share weights between some layers, please</span>
<span class="sd">        make sure to share the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the binarized weights (`binary_weight`)</span>

<span class="sd">        2) The weights and the binary weights become synced only after :func:`~nnabla._variable.Variable.forward` is called,</span>
<span class="sd">        and not after a call to :func:`~nnabla._variable.Variable.backward`.</span>
<span class="sd">        To access the parameters of the network, remember to call :func:`~nnabla._variable.Variable.forward` once before doing so, otherwise the</span>
<span class="sd">        float weights and the binary weights will not be in sync.</span>

<span class="sd">        3) Quantized values are stored as floating point number for `binary_weight`,</span>
<span class="sd">        since this function is only for simulation purposes.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): Input N-D array with shape (:math:`M_0 \\times \ldots \\times M_{B-1} \\times D_B \\times \ldots \\times D_N`). Dimensions before and after base_axis are flattened as if it was a matrix.</span>
<span class="sd">        n_outmaps (int or :obj:`tuple` of :obj:`int`): Number of output neurons per data.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for the weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`. </span>
<span class="sd">        wb_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for the binary weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.</span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for the bias. By defalut, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weight and bias will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">,</span> <span class="s1">&#39;__iter__&#39;</span><span class="p">):</span>
        <span class="n">n_outmaps</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_outmaps</span><span class="p">]</span>
    <span class="n">n_outmaps</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">)</span>
    <span class="n">n_outmap</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fan_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">fan_in</span><span class="p">,</span> <span class="n">n_outmap</span><span class="p">),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">wb_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fan_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])</span>
        <span class="n">wb_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">fan_in</span><span class="p">,</span> <span class="n">n_outmap</span><span class="p">),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">wb</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;Wb&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
        <span class="n">wb_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_weight_affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">wb</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="inq_affine"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.inq_affine">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;inq_affine&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Weight matrix in floating type&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;I&#39;</span><span class="p">,</span> <span class="s1">&#39;Binary indicator matrix of fixed weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">inq_affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
               <span class="n">inq_iterations</span><span class="o">=</span><span class="p">(),</span> <span class="n">selection_algorithm</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">,</span>
               <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">i_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Incremental Network Quantization Affine Layer</span>

<span class="sd">    During training, the weights are sequentially quantized to power-of-two</span>
<span class="sd">    values, which allows the training of a multiplierless network.</span>

<span class="sd">    Using `inq_iterations`, one can specify after how many forward passes</span>
<span class="sd">    half of the learnable weights are fixed and quantized to powers-of-two.</span>
<span class="sd">    After reaching the last value in `inq_iterations`, all weights are fixed.</span>

<span class="sd">    For more details, please refer to the reference.</span>

<span class="sd">    Reference:</span>
<span class="sd">    Zhou A, Yao A, Guo Y, Xu L, Chen Y. Incremental network quantization:</span>
<span class="sd">    Towards lossless CNNs with low-precision weights.</span>
<span class="sd">    &lt;https://arxiv.org/abs/1702.03044&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): Input N-D array with shape (:math:`M_0 \\times \ldots \\times M_{B-1} \\times D_B \\times \ldots \\times D_N`). Dimensions before and after base_axis are flattened as if it was a matrix.</span>
<span class="sd">        n_outmaps (int or :obj:`tuple` of :obj:`int`): Number of output neurons per data.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        num_bits (int): Number of bits per weight. Value has to be larger than 1 as one bit is already used to code the value &quot;0&quot;</span>
<span class="sd">        inq_iterations (tuple of int): Tuple of iteration numbers at which we fix half of the weights.</span>
<span class="sd">        selection_algorithm (str): Chooses algorithm that is used to decide which weights are fixed. (&quot;largest_abs&quot; ... fix weights with largest absolute value, &quot;random&quot; ... fix weights randomly)</span>
<span class="sd">        seed (int): Random seed for INQ algorithm</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.  </span>
<span class="sd">        i_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for indicators (0 ... learnable, 1 ... fixed). By default, it is initialized with zeros.</span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weight and bias will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">,</span> <span class="s1">&#39;__iter__&#39;</span><span class="p">):</span>
        <span class="n">n_outmaps</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_outmaps</span><span class="p">]</span>
    <span class="n">n_outmaps</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">)</span>
    <span class="n">n_outmap</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fan_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">fan_in</span><span class="p">,</span> <span class="n">n_outmap</span><span class="p">),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fan_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])</span>
        <span class="n">i_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;I&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
        <span class="n">i_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">inq_affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">num_bits</span><span class="p">,</span> <span class="n">inq_iterations</span><span class="p">,</span> <span class="n">selection_algorithm</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>


<div class="viewcode-block" id="convolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.convolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;conv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps // group, *kernel)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span>
                <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;N-D Convolution with a bias term.</span>

<span class="sd">    For Dilated Convolution (a.k.a. Atrous Convolution), refer to:</span>

<span class="sd">    - Chen et al., DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. https://arxiv.org/abs/1606.00915</span>

<span class="sd">    - Yu et al., Multi-Scale Context Aggregation by Dilated Convolutions. https://arxiv.org/abs/1511.07122</span>

<span class="sd">    Note:</span>

<span class="sd">        Convolution is a computationally intensive operation that</span>
<span class="sd">        should preferrably be run with the `cudnn` backend. NNabla</span>
<span class="sd">        then uses CuDNN library functions to determine and cache the</span>
<span class="sd">        fastest algorithm for the given set of convolution parameters,</span>
<span class="sd">        which results in additional memory consumption which may pose</span>
<span class="sd">        a problem for GPUs with insufficient memory size. In that</span>
<span class="sd">        case, the `NNABLA_CUDNN_WORKSPACE_LIMIT` environment variable</span>
<span class="sd">        can be used to restrict the choice of algorithms to those that</span>
<span class="sd">        fit the given workspace memory limit, expressed in bytes. In</span>
<span class="sd">        some cases it may also be desired to restrict the automatic</span>
<span class="sd">        search to algorithms that produce deterministic (reproducable)</span>
<span class="sd">        results. This can be requested by setting the the environment</span>
<span class="sd">        variable `NNABLA_CUDNN_DETERMINISTIC` to a non-zero value.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array.</span>
<span class="sd">        outmaps (int): Number of convolution kernels (which is equal to the number of output channels). For example, to apply convolution on an input with 16 types of filters, specify 16.</span>
<span class="sd">        kernel (:obj:`tuple` of :obj:`int`): Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</span>
<span class="sd">        pad (:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">        stride (:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">        dilation (:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">        group (int): Number of groups of channels. This makes connections across channels more sparse by grouping connections along map direction.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.  </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: N-D array. See :obj:`~nnabla.functions.convolution` for the output shape.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="n">outmaps</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span> <span class="o">//</span> <span class="n">group</span><span class="p">)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="svd_convolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.svd_convolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;svd_conv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;U&#39;</span><span class="p">,</span>
     <span class="s1">&#39;Decomposed filter weights :math:`{</span><span class="se">\\</span><span class="s1">mathbf U}`&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps * r, *kernel)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;V&#39;</span><span class="p">,</span> <span class="s1">&#39;Decomposed filter weights :math:`{</span><span class="se">\\</span><span class="s1">mathbf V}`&#39;</span><span class="p">,</span>
     <span class="s1">&#39;(outmaps, inmaps * r, 1, ...)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">svd_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">uv_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;SVD convolution is a low rank approximation of the convolution</span>
<span class="sd">    layer. It can be seen as a depth wise convolution followed by a</span>
<span class="sd">    1x1 convolution.</span>

<span class="sd">    The flattened kernels for the i-th input map are expressed by</span>
<span class="sd">    their low rank approximation. The kernels for the i-th input</span>
<span class="sd">    :math:`{\\mathbf W_i}` are approximated with the singular value</span>
<span class="sd">    decomposition (SVD) and by selecting the :math:`{R}` dominant</span>
<span class="sd">    singular values and the corresponding singular vectors.</span>

<span class="sd">    .. math::</span>
<span class="sd">        {\\mathbf W_{:,i,:}} ~ {\\mathbf U_i} {\\mathbf V_i}.</span>

<span class="sd">    :math:`{\\mathbf U}` contains the weights of the depthwise</span>
<span class="sd">    convolution with multiplier :math:`{R}` and :math:`{\\mathbf V}`</span>
<span class="sd">    contains the weights of the 1x1 convolution.</span>

<span class="sd">    If `uv_init` is a numpy array, :math:`{\\mathbf U}` and</span>
<span class="sd">    :math:`{\\mathbf V}` are computed such that `uv_init` is</span>
<span class="sd">    approximated by :math:`{\\mathbf{UV}}`. If `uv_init` is `None` or</span>
<span class="sd">    an initializer, the product of :math:`{\\mathbf U}` and</span>
<span class="sd">    :math:`{\\mathbf V}` approximates the random initialization.</span>

<span class="sd">    If :math:`{\\mathbf U}` and :math:`{\\mathbf V}` exist in the</span>
<span class="sd">    context, they take precedence over `uv_init`.</span>

<span class="sd">    Suppose the kernel tensor of the convolution is of :math:`{O \\times I \\times K \\times K}` and</span>
<span class="sd">    the compression rate you want to specify is :math:`{CR}`, then you</span>
<span class="sd">    set :math:`{R}` as</span>

<span class="sd">    .. math::</span>

<span class="sd">        R = \\left\\lfloor \\frac{(1 - CR)OIK^2}{I(O + K^2)} \\right\\rfloor.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array.</span>

<span class="sd">        outmaps (int): Number of convolution kernels (which is equal</span>
<span class="sd">          to the number of output channels). For example, to apply</span>
<span class="sd">          convolution on an input with 16 types of filters, specify</span>
<span class="sd">          16.</span>

<span class="sd">        kernel (tuple): Convolution kernel size. For example,</span>
<span class="sd">          to apply convolution on an image with a 3 (height) by 5</span>
<span class="sd">          (width) two-dimensional kernel, specify (3, 5).</span>

<span class="sd">        r (int): Rank of the factorized layer.</span>

<span class="sd">        pad (tuple): Padding sizes (`int`) for dimensions.</span>

<span class="sd">        stride (tuple): Stride sizes (`int`) for dimensions.</span>

<span class="sd">        dilation (tuple): Dilation sizes (`int`) for dimensions.</span>

<span class="sd">        uv_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`):</span>
<span class="sd">          Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`. </span>

<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>

<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the</span>
<span class="sd">          sample dimensions.</span>

<span class="sd">        fix_parameters (bool): When set to `True`, the weights and</span>
<span class="sd">          biases will not be updated.</span>

<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>

<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: :math:`(B + 1)`-D array.</span>
<span class="sd">        (:math:`M_0 \\times \ldots \\times M_{B-1} \\times L`)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">r</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;svd_convolution: The rank must larger than zero&quot;</span>

    <span class="k">if</span> <span class="n">uv_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">uv_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="n">outmaps</span><span class="p">,</span>
                                    <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">uv_init</span><span class="p">)</span> <span class="ow">is</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="c1"># TODO: Assert that size of uv_init is correct</span>
        <span class="c1"># uv is initialize with numpy array</span>
        <span class="n">uv</span> <span class="o">=</span> <span class="n">uv_init</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># uv is initialize from initializer</span>
        <span class="n">uv</span> <span class="o">=</span> <span class="n">uv_init</span><span class="p">((</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">])</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">))</span>

    <span class="c1"># flatten kernels</span>
    <span class="n">uv</span> <span class="o">=</span> <span class="n">uv</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">kernel</span><span class="p">)))</span>

    <span class="n">u</span> <span class="o">=</span> <span class="n">get_parameter</span><span class="p">(</span><span class="s1">&#39;U&#39;</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">get_parameter</span><span class="p">(</span><span class="s1">&#39;V&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">u</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>

        <span class="n">inmaps</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span>
        <span class="n">u_low_rank</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">inmaps</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span> <span class="n">r</span><span class="p">))</span>
        <span class="n">v_low_rank</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">inmaps</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">inmaps</span><span class="p">):</span>
            <span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">uv</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:])</span>
            <span class="n">u_</span><span class="p">,</span> <span class="n">s_</span><span class="p">,</span> <span class="n">v_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">u_low_rank</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u_</span><span class="p">[:,</span> <span class="p">:</span><span class="n">r</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s_</span><span class="p">[:</span><span class="n">r</span><span class="p">]))</span>
            <span class="n">v_low_rank</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">v_</span><span class="p">[:</span><span class="n">r</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1"># reshape U : (I,K*K,r) -&gt; (I*r,K,K) for depthwise conv</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">inmaps</span> <span class="o">*</span> <span class="n">r</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
                        <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">u</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">u_low_rank</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
               <span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">inmaps</span> <span class="o">*</span> <span class="n">r</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)))</span>

        <span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">set_parameter</span><span class="p">(</span><span class="s2">&quot;U&quot;</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span>

        <span class="c1"># reshape V :  (I,r,O) -&gt; (O,I*r,1,1) for 1X1 conv</span>
        <span class="n">kernel_one</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>  <span class="c1"># 1x1 for 2D convolution</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inmaps</span> <span class="o">*</span> <span class="n">r</span><span class="p">)</span> <span class="o">+</span> <span class="n">kernel_one</span><span class="p">,</span>
                        <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">v</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">v_low_rank</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
               <span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inmaps</span> <span class="o">*</span> <span class="n">r</span><span class="p">)</span> <span class="o">+</span> <span class="n">kernel_one</span><span class="p">))</span>

        <span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">set_parameter</span><span class="p">(</span><span class="s2">&quot;V&quot;</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">fix_parameters</span> <span class="o">==</span> <span class="n">u</span><span class="o">.</span><span class="n">need_grad</span><span class="p">:</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">u</span><span class="o">.</span><span class="n">get_unlinked_variable</span><span class="p">(</span><span class="n">need_grad</span><span class="o">=</span><span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fix_parameters</span> <span class="o">==</span> <span class="n">v</span><span class="o">.</span><span class="n">need_grad</span><span class="p">:</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">get_unlinked_variable</span><span class="p">(</span><span class="n">need_grad</span><span class="o">=</span><span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depthwise_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="n">base_axis</span><span class="p">,</span>
                                <span class="n">pad</span><span class="o">=</span><span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
                                <span class="n">multiplier</span><span class="o">=</span><span class="n">r</span><span class="p">)</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span></div>


<div class="viewcode-block" id="cpd3_convolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.cpd3_convolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;cpd3_conv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;I&#39;</span><span class="p">,</span>
     <span class="s1">&#39;Decomposed filter weights :math:`{</span><span class="se">\\</span><span class="s1">mathbf I}`&#39;</span><span class="p">,</span> <span class="s1">&#39;(r, inmaps, 1, ...)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;K&#39;</span><span class="p">,</span>
     <span class="s1">&#39;Decomposed filter weights :math:`{</span><span class="se">\\</span><span class="s1">mathbf K}`&#39;</span><span class="p">,</span> <span class="s1">&#39;(r, *kernel)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;O&#39;</span><span class="p">,</span>
     <span class="s1">&#39;Decomposed filter weights :math:`{</span><span class="se">\\</span><span class="s1">mathbf O}`&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, r, 1, ...)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">cpd3_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span>
                     <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                     <span class="n">oik_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                     <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                     <span class="n">max_iter</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">stopping_criterion</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;CP convolution is a low rank approximation of a convolution layer. A 3D tensor containing the parameter is built by collapsing the N-D kernels into 1D, then the tensor is decomposed into three matrices. The decomposed layer can be seen as linear combinations of the input feature maps to :math:`{R}` feature maps followed by a depthwise convolution and followed by linear combinations of the feature maps to compute the output feature maps.</span>

<span class="sd">    The CP decomposition allows to approximate the kernel tensor by :math:`{R}` rank-1 tensors of the form:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \\sum_{r=1}^{R} \\lambda_r {\\mathbf{o}^{(r)} \\otimes \\mathbf{i}^{(r)} \\otimes \\mathbf{k}^{(r)}},</span>

<span class="sd">    where :math:`{\\lambda}_r` is the normalization coefficient and :math:`{\\otimes}` is the outer product.</span>


<span class="sd">    If `oik_init` is a numpy array, U and V are computed so that uv_init can be approximates from UV</span>
<span class="sd">    If `oik_init` is None or an initializer, the product of U and V approximate the randomly initialized array</span>

<span class="sd">    If `O`, `I` and `K` exist in context, they are used to initialize the layer and oik_init is not used.</span>

<span class="sd">    Suppose the kernel tensor of the affine is of :math:`{I \\times O}` and</span>
<span class="sd">    the compression rate you want to specify is :math:`{CR}`, then you</span>
<span class="sd">    set :math:`{R}` as</span>

<span class="sd">    .. math::</span>

<span class="sd">        R = \\left\\lfloor \\frac{(1 - CR)OIK^2}{O + I + K^2} \\right\\rfloor.</span>

<span class="sd">    References:</span>
<span class="sd">        - Lebedev, Vadim, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and Victor Lempitsky,  &quot;Speeding-up convolutional neural networks using fine-tuned cp-decomposition.&quot;, arXiv preprint arXiv:1412.6553 (2014).</span>

<span class="sd">        - Marcella Astrid, Seung-Ik Lee, &quot;CP-decomposition with Tensor Power Method for Convolutional Neural Networks Compression&quot;, BigComp 2017.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array.</span>
<span class="sd">        outmaps (int): Number of convolution kernels (which is equal to the number of output channels). For example, to apply convolution on an input with 16 types of filters, specify 16.</span>
<span class="sd">        kernel (:obj:`tuple` of :obj:`int`): Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</span>
<span class="sd">        r (int): rank of the factorized layer</span>
<span class="sd">        pad (:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">        stride (:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">        dilation (:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">        oik_init (numpy array or :obj:`nnabla.initializer.BaseInitializer`): Initializer for weight. Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`. </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. It is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>
<span class="sd">        max_iter (int): Max iteration of the ALS.</span>
<span class="sd">        stopping_criterion (float): Threshold for stopping the ALS.</span>
<span class="sd">                If the value is negative, the convergence check is ignored;</span>
<span class="sd">                in other words, it may reduce the computation time.</span>
<span class="sd">        lambda_reg (float): regularization parameter for the ALS. Larger</span>
<span class="sd">                lambda_reg means larger regularization.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: :math:`(B + 1)`-D array. (:math:`M_0 \\times \ldots \\times M_{B-1} \\times L`)</span>


<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">oik_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">oik_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="n">outmaps</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">oik_init</span><span class="p">)</span> <span class="ow">is</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="c1"># TODO: Assert that size of uv_init is correct</span>
        <span class="c1"># uv is initialize with numpy array</span>
        <span class="n">oik</span> <span class="o">=</span> <span class="n">oik_init</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># uv is initialize from initializer</span>
        <span class="n">oik</span> <span class="o">=</span> <span class="n">oik_init</span><span class="p">((</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">])</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">))</span>

    <span class="c1"># flatten kernels</span>
    <span class="n">oik</span> <span class="o">=</span> <span class="n">oik</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">kernel</span><span class="p">)))</span>

    <span class="n">o</span> <span class="o">=</span> <span class="n">get_parameter</span><span class="p">(</span><span class="s1">&#39;O&#39;</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">get_parameter</span><span class="p">(</span><span class="s1">&#39;I&#39;</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">get_parameter</span><span class="p">(</span><span class="s1">&#39;K&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">o</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">i</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">k</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">r</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;cpd3_convolution: The rank must larger than zero&quot;</span>
        <span class="kn">from</span> <span class="nn">nnabla.utils.factorization</span> <span class="k">import</span> <span class="n">cpd</span>
        <span class="n">als</span> <span class="o">=</span> <span class="n">cpd</span><span class="o">.</span><span class="n">ALS</span><span class="p">()</span>
        <span class="n">U</span><span class="p">,</span> <span class="n">lmbda</span> <span class="o">=</span> <span class="n">als</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">oik</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">r</span><span class="p">,</span>
                             <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
                             <span class="n">stopping_criterion</span><span class="o">=</span><span class="n">stopping_criterion</span><span class="p">,</span>
                             <span class="n">lambda_reg</span><span class="o">=</span><span class="n">lambda_reg</span><span class="p">,</span>
                             <span class="n">dtype</span><span class="o">=</span><span class="n">oik</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                             <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

        <span class="n">o_</span> <span class="o">=</span> <span class="n">U</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">lmbda</span>
        <span class="n">i_</span> <span class="o">=</span> <span class="n">U</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">k_</span> <span class="o">=</span> <span class="n">U</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

        <span class="n">kernel_one</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>  <span class="c1"># 1x1 for 2D convolution</span>
        <span class="n">inmaps</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span>

        <span class="c1"># reshape I :  (I,r) -&gt; (r,I,1,1)</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">r</span><span class="p">,</span> <span class="n">inmaps</span><span class="p">)</span> <span class="o">+</span> <span class="n">kernel_one</span><span class="p">,</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">i</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">i_</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">r</span><span class="p">,</span> <span class="n">inmaps</span><span class="p">)</span> <span class="o">+</span> <span class="n">kernel_one</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">set_parameter</span><span class="p">(</span><span class="s2">&quot;I&quot;</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>

        <span class="c1"># reshape O :  (O,r) -&gt; (O,r,1,1)</span>
        <span class="n">o</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span> <span class="o">+</span> <span class="n">kernel_one</span><span class="p">,</span>
                        <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">o</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">o_</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span> <span class="o">+</span> <span class="n">kernel_one</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">set_parameter</span><span class="p">(</span><span class="s2">&quot;O&quot;</span><span class="p">,</span> <span class="n">o</span><span class="p">)</span>

        <span class="c1"># reshape K :  (K*K,r) -&gt; (r,K,K)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="n">r</span><span class="p">,)</span> <span class="o">+</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">need_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">k</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">k_</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">r</span><span class="p">,)</span> <span class="o">+</span> <span class="n">kernel</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">set_parameter</span><span class="p">(</span><span class="s2">&quot;K&quot;</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">fix_parameters</span> <span class="o">==</span> <span class="n">o</span><span class="o">.</span><span class="n">need_grad</span><span class="p">:</span>
        <span class="n">o</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="n">get_unlinked_variable</span><span class="p">(</span><span class="n">need_grad</span><span class="o">=</span><span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fix_parameters</span> <span class="o">==</span> <span class="n">i</span><span class="o">.</span><span class="n">need_grad</span><span class="p">:</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">i</span><span class="o">.</span><span class="n">get_unlinked_variable</span><span class="p">(</span><span class="n">need_grad</span><span class="o">=</span><span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">fix_parameters</span> <span class="o">==</span> <span class="n">k</span><span class="o">.</span><span class="n">need_grad</span><span class="p">:</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">get_unlinked_variable</span><span class="p">(</span><span class="n">need_grad</span><span class="o">=</span><span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depthwise_convolution</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="n">base_axis</span><span class="p">,</span>
                                <span class="n">pad</span><span class="o">=</span><span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
                                <span class="n">multiplier</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span></div>


<div class="viewcode-block" id="binary_connect_convolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.binary_connect_convolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;bicon_conv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps, *kernel)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;Wb&#39;</span><span class="p">,</span> <span class="s1">&#39;Binarized filter weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps, *kernel)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">binary_connect_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span>
                               <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                               <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">wb_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                               <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                               <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Binary Connect Convolution, multiplier-less inner-product.</span>

<span class="sd">    Binary Connect Convolution is the convolution function,</span>
<span class="sd">    except the definition of the inner product is modified.</span>
<span class="sd">    The input-output relation of this function is as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_{n, a, b} = \sum_{m} \sum_{i} \sum_{j} sign(w_{n, m, i, j}) x_{m, a + i, b + j}.</span>

<span class="sd">    Therefore :math:`sign(w_i)` is either :math:`1` or :math:`-1` and the inner product</span>
<span class="sd">    simplifies to addition.</span>

<span class="sd">    This function should be used together with BatchNormalization.</span>

<span class="sd">    References:</span>

<span class="sd">        M. Courbariaux, Y. Bengio, and J.-P. David. &quot;BinaryConnect:</span>
<span class="sd">        Training Deep Neural Networks with binary weights during propagations.&quot;</span>
<span class="sd">        Advances in Neural Information Processing Systems. 2015.</span>

<span class="sd">    .. note::</span>

<span class="sd">        1) if you would like to share weights between some layers, please</span>
<span class="sd">        make sure to share the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the binarized weights (`binary_weight`)</span>

<span class="sd">        2) The weights and the binary weights become synced only after :func:`~nnabla._variable.Variable.forward` is called,</span>
<span class="sd">        and not after a call to :func:`~nnabla._variable.Variable.backward`.</span>
<span class="sd">        To access the parameters of the network, remember to call :func:`~nnabla._variable.Variable.forward` once before doing so, otherwise the</span>
<span class="sd">        float weights and the binary weights will not be in sync.</span>

<span class="sd">        3) Quantized values are stored as floating point number for `binary_weight`,</span>
<span class="sd">        since this function is only for simulation purposes.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array.</span>
<span class="sd">        outmaps (int): Number of convolution kernels (which is equal to the number of output channels). For example, to apply convolution on an input with 16 types of filters, specify 16.</span>
<span class="sd">        kernel (:obj:`tuple` of :obj:`int`): Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</span>
<span class="sd">        pad (:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">        stride (:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">        dilation (:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">        group (int): Number of groups of channels. This makes connections across channels sparser by grouping connections along map direction.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`. </span>
<span class="sd">        wb_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for binary weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.   </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="n">outmaps</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">wb_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">wb_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="n">outmaps</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">])</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">wb</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;Wb&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">])</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
        <span class="n">wb_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_connect_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">wb</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="binary_weight_convolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.binary_weight_convolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;bwn_conv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps, *kernel)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;Wb&#39;</span><span class="p">,</span> <span class="s1">&#39;Binarized filter weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps, *kernel)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="s1">&#39;Scaling factor :math:`</span><span class="se">\\</span><span class="s1">alpha`&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">binary_weight_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span>
                              <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                              <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">wb_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Binary Weight Convolution, multiplier-less inner-product with a scale factor.</span>

<span class="sd">    Binary Weight Convolution is the convolution function, but the</span>
<span class="sd">    inner product in this function is the following,</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_{n, a, b} = \\frac{1}{\\|\\mathbf{w}_n\\|_{\\ell_1}} \sum_{m} \sum_{i} \sum_{j} sign(w_{n, m, i, j}) x_{m, a + i, b + j}.</span>


<span class="sd">    Therefore :math:`sign(w_{n, m, i, j})`  is either :math:`1` or :math:`-1` and the inner product</span>
<span class="sd">    simplifies to addition followed by scaling factor :math:`\\alpha = \\frac{1}{\\|\\mathbf{w}_n\\|_{\\ell_1}}`.</span>
<span class="sd">    The number of :math:`n` is the number of outmaps of the convolution</span>
<span class="sd">    function.</span>

<span class="sd">    References:</span>

<span class="sd">        Rastegari, Mohammad, et al. &quot;XNOR-Net: ImageNet Classification Using</span>
<span class="sd">        Binary Convolutional Neural Networks.&quot; arXiv preprint</span>
<span class="sd">        arXiv:1603.05279 (2016).</span>

<span class="sd">    .. note::</span>

<span class="sd">        1) if you would like to share weights between some layers, please</span>
<span class="sd">        make sure to share the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the binarized weights (`binary_weight`)</span>

<span class="sd">        2) The weights and the binary weights become synced only after :func:`~nnabla._variable.Variable.forward` is called,</span>
<span class="sd">        and not after a call to :func:`~nnabla._variable.Variable.backward`.</span>
<span class="sd">        To access the parameters of the network, remember to call :func:`~nnabla._variable.Variable.forward` once before doing so, otherwise the</span>
<span class="sd">        float weights and the binary weights will not be in sync.</span>

<span class="sd">        3) Quantized values are stored as floating point number for `binary_weight`,</span>
<span class="sd">        since this function is only for simulation purposes.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array.</span>
<span class="sd">        outmaps (int): Number of convolution kernels (which is equal to the number of output channels). For example, to apply convolution on an input with 16 types of filters, specify 16.</span>
<span class="sd">        kernel (:obj:`tuple` of :obj:`int`): Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</span>
<span class="sd">        pad (:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">        stride (:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">        dilation (:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">        group (int): Number of groups of channels. This makes connections across channels sparser by grouping connections along map direction.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`. </span>
<span class="sd">        wb_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for binary weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.  </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="n">outmaps</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">wb_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">wb_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="n">outmaps</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">])</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">wb</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;Wb&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">])</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
        <span class="n">wb_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="p">),</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_weight_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">wb</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="inq_convolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.inq_convolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;inq_conv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps, *kernel)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;I&#39;</span><span class="p">,</span> <span class="s1">&#39;Binary indicator matrix of fixed weights&#39;</span><span class="p">,</span>
     <span class="s1">&#39;(outmaps, inmaps, *kernel)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">inq_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span>
                    <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">num_bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">inq_iterations</span><span class="o">=</span><span class="p">(),</span> <span class="n">selection_algorithm</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">,</span>
                    <span class="n">seed</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">i_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Incremental Network Quantization Convolution Layer</span>

<span class="sd">    During training, the weights are sequentially quantized to power-of-two</span>
<span class="sd">    values, which allows the training of a multiplierless network.</span>

<span class="sd">    Using `inq_iterations`, one can specify after how many forward passes</span>
<span class="sd">    half of the learnable weights are fixed and quantized to powers-of-two.</span>
<span class="sd">    After reaching the last value in `inq_iterations`, all weights are fixed.</span>

<span class="sd">    For more details, please refer to the reference.</span>

<span class="sd">    Reference:</span>
<span class="sd">    Zhou A, Yao A, Guo Y, Xu L, Chen Y. Incremental network quantization:</span>
<span class="sd">    Towards lossless CNNs with low-precision weights.</span>
<span class="sd">    &lt;https://arxiv.org/abs/1702.03044&gt;</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): Input N-D array with shape (:math:`M_0 \\times \ldots \\times M_{B-1} \\times D_B \\times \ldots \\times D_N`). Dimensions before and after base_axis are flattened as if it was a matrix.</span>
<span class="sd">        n_outmaps (int or :obj:`tuple` of :obj:`int`): Number of output neurons per data.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        num_bits (int): Number of bits per weight. Value has to be larger than 1 as one bit is already used to code the value &quot;0&quot;</span>
<span class="sd">        inq_iterations (tuple of int): Tuple of iteration numbers at which we fix half of the weights.</span>
<span class="sd">        selection_algorithm (str): Chooses algorithm that is used to decide which weights are fixed. (&quot;largest_abs&quot; ... fix weights with largest absolute value, &quot;random&quot; ... fix weights randomly)</span>
<span class="sd">        seed (int): Random seed for INQ algorithm</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for the weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`. </span>
<span class="sd">        i_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for the indicators (0 ... learnable, 1 ... fixed). By default, it is initialized with zeros.</span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for the bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weight and bias will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="n">outmaps</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">i_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">])</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;I&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">])</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
        <span class="n">i_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">inq_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">num_bits</span><span class="p">,</span> <span class="n">inq_iterations</span><span class="p">,</span> <span class="n">selection_algorithm</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></div>


<div class="viewcode-block" id="depthwise_convolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.depthwise_convolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;depthwise_conv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps * multiplier, *kernel)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps * multiplier,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">depthwise_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">multiplier</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    N-D Depthwise Convolution with a bias term.</span>

<span class="sd">    Reference:</span>

<span class="sd">    - F. Chollet: Chollet, Francois. &quot;Xception: Deep Learning with Depthwise Separable Convolutions. https://arxiv.org/abs/1610.02357</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array.</span>
<span class="sd">        kernel (:obj:`tuple` of :obj:`int`): Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</span>
<span class="sd">        pad (:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">        stride (:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">        dilation (:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">        multiplier (:obj:`int`): Number of output feature maps per input feature map.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight.  By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`. </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: N-D array. See :obj:`~nnabla.functions.depthwise_convolution` for the output shape.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span>
                <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">multiplier</span><span class="p">,</span>
                <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span>
            <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">multiplier</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span> <span class="o">*</span> <span class="n">multiplier</span><span class="p">,),</span>
            <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">depthwise_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
                                   <span class="n">multiplier</span><span class="p">)</span></div>


<div class="viewcode-block" id="deconvolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.deconvolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;deconv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps // group, *kernel)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">deconvolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span>
                  <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                  <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Deconvolution layer.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array.</span>
<span class="sd">        outmaps (int): Number of deconvolution kernels (which is equal to the number of output channels). For example, to apply deconvolution on an input with 16 types of filters, specify 16.</span>
<span class="sd">        kernel (:obj:`tuple` of :obj:`int`): Convolution kernel size. For example, to apply deconvolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</span>
<span class="sd">        pad (:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">        stride (:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">        dilation (:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">        group (int): Number of groups of channels. This makes connections across channels sparser by grouping connections along map direction.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.  </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: N-D array. See :obj:`~nnabla.functions.deconvolution` for the output shape.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="n">outmaps</span> <span class="o">//</span> <span class="n">group</span><span class="p">)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">deconvolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="depthwise_deconvolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.depthwise_deconvolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;depthwise_deconv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps,) + kernel&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps / divisor,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">depthwise_deconvolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                            <span class="n">divisor</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                            <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Depthwise deconvolution computes the transposed depthwise</span>
<span class="sd">    convolution for one-dimensional and two-dimensional input data.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array.</span>
<span class="sd">        kernel (:obj:`tuple` of :obj:`int`): Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</span>
<span class="sd">        pad (:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">        stride (:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">        dilation (:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">        divisor (:obj:`int`): Number of input feature maps per output feature map.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.  </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: N-D array. See :obj:`~nnabla.functions.depthwise_deconvolution` for the output shape.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span>
                <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span>
                <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span>
                <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span>
            <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span> <span class="o">//</span> <span class="n">divisor</span><span class="p">,),</span>
            <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">depthwise_deconvolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span>
                                     <span class="n">dilation</span><span class="p">,</span> <span class="n">divisor</span><span class="p">)</span></div>


<div class="viewcode-block" id="batch_normalization"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.batch_normalization">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;bn&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="s1">&#39;Trainable bias :math:`</span><span class="se">\\</span><span class="s1">beta`&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;see above&gt;&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;gamma&#39;</span><span class="p">,</span> <span class="s1">&#39;Trainable scaling factor :math:`</span><span class="se">\\</span><span class="s1">gamma`&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;see above&gt;&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;Moving average of batch mean&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;see above&gt;&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;Moving average of batch variance&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;see above&gt;&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">batch_normalization</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
                        <span class="n">batch_stat</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_stat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">param_init</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Batch normalization layer.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \\begin{array}{lcl}</span>
<span class="sd">        \\mu &amp;=&amp; \\frac{1}{M} \\sum x_i\\\\</span>
<span class="sd">        \\sigma^2 &amp;=&amp; \\frac{1}{M} \\left(\\sum x_i - \\mu\\right)^2\\\\</span>
<span class="sd">        \\hat{x}_i &amp;=&amp; \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon }}\\\\</span>
<span class="sd">        y_i &amp;= &amp; \\hat{x}_i \\gamma + \\beta.</span>
<span class="sd">        \\end{array}</span>

<span class="sd">    where :math:`x_i, y_i` are the inputs.</span>
<span class="sd">    In testing, the mean and variance computed by moving average calculated during training are used.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array of input.</span>
<span class="sd">        axes (:obj:`tuple` of :obj:`int`):</span>
<span class="sd">            Mean and variance for each element in ``axes`` are calculated using</span>
<span class="sd">            elements on the rest axes. For example, if an input is 4 dimensions,</span>
<span class="sd">            and ``axes`` is ``[1]``,  batch mean is calculated as</span>
<span class="sd">            ``np.mean(inp.d, axis=(0, 2, 3), keepdims=True)``</span>
<span class="sd">            (using numpy expression as an example).</span>
<span class="sd">        decay_rate (float): Decay rate of running mean and variance.</span>
<span class="sd">        eps (float): Tiny value to avoid zero division by std.</span>
<span class="sd">        batch_stat (bool): Use mini-batch statistics rather than running ones.</span>
<span class="sd">        output_stat (bool): Output batch mean and variance.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the beta and gamma will not be updated.</span>
<span class="sd">        param_init (dict):</span>
<span class="sd">            Parameter initializers can be set with a dict. A key of the dict must</span>
<span class="sd">            be ``&#39;beta&#39;``, ``&#39;gamma&#39;``, ``&#39;mean&#39;`` or ``&#39;var&#39;``.</span>
<span class="sd">            A value of the dict must be an :obj:`~nnabla.initializer.Initializer`</span>
<span class="sd">            or a :obj:`numpy.ndarray`.</span>
<span class="sd">            E.g. ``{&#39;beta&#39;: ConstantIntializer(0), &#39;gamma&#39;: np.ones(gamma_shape) * 2}``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: N-D array.</span>

<span class="sd">    References:</span>

<span class="sd">        - Ioffe and Szegedy, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. https://arxiv.org/abs/1502.03167</span>

<span class="sd">    The shape of parameters has the same number of dimensions with the input</span>
<span class="sd">    data, and the shapes in ``axes`` has the same dimensions with the input, while the rest has ``1``.</span>
<span class="sd">    If an input is 4-dim and ``axes=[1]``, the parameter shape will be</span>
<span class="sd">    ``param_shape  = np.mean(inp.d, axis=(0, 2, 3), keepdims=True).shape``</span>
<span class="sd">    (using numpy expression as an example).</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="n">shape_stat</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">]</span>
    <span class="n">shape_stat</span><span class="p">[</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>

    <span class="k">if</span> <span class="n">param_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">param_init</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">beta_init</span> <span class="o">=</span> <span class="n">param_init</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">gamma_init</span> <span class="o">=</span> <span class="n">param_init</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;gamma&#39;</span><span class="p">,</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">mean_init</span> <span class="o">=</span> <span class="n">param_init</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">var_init</span> <span class="o">=</span> <span class="n">param_init</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">shape_stat</span><span class="p">,</span> <span class="n">beta_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;gamma&quot;</span><span class="p">,</span> <span class="n">shape_stat</span><span class="p">,</span> <span class="n">gamma_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">shape_stat</span><span class="p">,</span> <span class="n">mean_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;var&quot;</span><span class="p">,</span> <span class="n">shape_stat</span><span class="p">,</span> <span class="n">var_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">batch_normalization</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span>
                                 <span class="n">decay_rate</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">batch_stat</span><span class="p">,</span> <span class="n">output_stat</span><span class="p">)</span></div>


<div class="viewcode-block" id="mean_subtraction"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.mean_subtraction">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;mean_subtraction&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;Moving average&#39;</span><span class="p">,</span> <span class="s1">&#39;inp.shape[base_axis:]&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;t&#39;</span><span class="p">,</span> <span class="s1">&#39;Minibatch counter used in forward pass&#39;</span><span class="p">,</span> <span class="s1">&#39;(1,)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">mean_subtraction</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">update_running_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Mean subtraction layer.</span>

<span class="sd">    It subtracts the mean of the elements of the input array,</span>
<span class="sd">    and normalizes it to :math:`0`. Preprocessing arrays with this function has the effect of improving accuracy</span>
<span class="sd">    in various tasks such as image classification.</span>

<span class="sd">    At training time, this function is defined as</span>

<span class="sd">    .. math::</span>

<span class="sd">        \\begin{array}{lcl}</span>
<span class="sd">        \\mu &amp;=&amp; \\frac{1}{M} \\sum x_i \\\\</span>
<span class="sd">        y_i &amp;=&amp; x_i - \\mu</span>
<span class="sd">        \\end{array}</span>

<span class="sd">    At testing time, the mean values used are those that were computed during training by moving average.</span>

<span class="sd">    Note:</span>
<span class="sd">        The backward performs an approximated differentiation that takes into account only the latest mini-batch.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array of input.</span>
<span class="sd">        base_axis (int): Base axis of Mean Subtraction operation. Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">        update_running_mean (bool): When set to `True`, the running mean will not be updated.</span>
<span class="sd">        fix_parameters (bool): dummy parameter. This argument dose not affect anything.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">base_axis</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;t&quot;</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">ConstantInitializer</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">mean_subtraction</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="n">base_axis</span><span class="p">,</span> <span class="n">update_running_mean</span><span class="o">=</span><span class="n">update_running_mean</span><span class="p">)</span></div>


<div class="viewcode-block" id="embed"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.embed">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;embed&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Embedding matrix&#39;</span><span class="p">,</span> <span class="s1">&#39;(n_inputs, n_features)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">embed</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Embed.</span>

<span class="sd">    Embed slices a matrix/tensor with indexing array/tensor. Weights are initialized with :obj:`nnabla.initializer.UniformInitializer` within the range of :math:`-\\sqrt{3}` and :math:`\\sqrt{3}`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): [Integer] Indices with shape :math:`(I_0, ..., I_N)`</span>
<span class="sd">        n_inputs : number of possible inputs, words or vocabraries</span>
<span class="sd">        n_features : number of embedding features</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the embedding weight matrix</span>
<span class="sd">            will not be updated.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: Output with shape :math:`(I_0, ..., I_N, W_1, ..., W_M)`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span><span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_features</span><span class="p">],</span>
                                <span class="n">UniformInitializer</span><span class="p">((</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">3.</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">3</span><span class="p">))),</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span></div>


<div class="viewcode-block" id="prelu"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.prelu">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;prelu&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;slope&#39;</span><span class="p">,</span> <span class="s1">&#39;Negative slope&#39;</span><span class="p">,</span>
     <span class="s1">&#39;tuple() if shared else (inp.shape[base_axis],)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">prelu</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shared</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parametrized Rectified Linear Unit function defined as</span>

<span class="sd">    .. math::</span>
<span class="sd">        y_i = \max(0, x_i) + w_i \min(0, -x_i)</span>

<span class="sd">    where negative slope :math:`w` is learned and can vary across channels (an</span>
<span class="sd">    axis specified with base_axis). Weights are initialized with :math:`-1`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x(~nnabla.Variable): N-D array as input</span>
<span class="sd">        base_axis(int): Dimensions up to base_axis is treated as sample dimension.</span>
<span class="sd">        shared(bool): Use shared weight value or not</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the negative slope values</span>
<span class="sd">            will not be updated.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ~nnabla.Variable: N-D array.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">()</span> <span class="k">if</span> <span class="n">shared</span> <span class="k">else</span> <span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span><span class="s2">&quot;slope&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span>
                                <span class="n">ConstantInitializer</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">prelu</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="fixed_point_quantized_affine"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.fixed_point_quantized_affine">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;fp_quantized_affine&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Weight matrix in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;W_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Quantized weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Quantized biases&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">fixed_point_quantized_affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span>
                                 <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                 <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                 <span class="n">quantize_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sign_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_w</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">delta_w</span><span class="o">=</span><span class="mi">2</span><span class="o">**-</span><span class="mi">4</span><span class="p">,</span> <span class="n">ste_fine_grained_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                 <span class="n">quantize_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sign_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_b</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">delta_b</span><span class="o">=</span><span class="mi">2</span><span class="o">**-</span><span class="mi">4</span><span class="p">,</span> <span class="n">ste_fine_grained_b</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fixed-Point Quantized Affine.</span>

<span class="sd">    Fixed-Point Quantized Affine is the affine function,</span>
<span class="sd">    except the definition of the inner product is modified.</span>
<span class="sd">    The input-output relation of this function is as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_j = \sum_{i} Q(w_{ji}) x_i,</span>

<span class="sd">    where :math:`Q(w_{ji})` is the fixed-point quantization function.</span>

<span class="sd">    .. note::</span>

<span class="sd">        1) if you would like to share weights between some layers, please</span>
<span class="sd">        make sure to share the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the quantized weights (`quantized weight`)</span>

<span class="sd">        2) The weights and the quantized weights become synced only after :func:`~nnabla._variable.Variable.forward` is called,</span>
<span class="sd">        and not after a call to :func:`~nnabla._variable.Variable.backward`.</span>
<span class="sd">        To access the parameters of the network, remember to call :func:`~nnabla._variable.Variable.forward` once before doing so, otherwise the</span>
<span class="sd">        float weights and the quantized weights will not be in sync.</span>

<span class="sd">        3) CPU and GPU implementations now use float value for `quantized weight`,</span>
<span class="sd">        since this function is only for simulation purposes.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): Input N-D array with shape (:math:`M_0 \\times \ldots \\times M_{B-1} \\times D_B \\times \ldots \\times D_N`). Dimensions before and after base_axis are flattened as if it is a matrix.</span>
<span class="sd">        n_outmaps (:obj:`int` or :obj:`tuple` of :obj:`int`): Number of output neurons per data.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`. </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>
<span class="sd">        quantize_w (bool): Quantize weights if `True`.</span>
<span class="sd">        sign_w (bool): Use signed quantization if `True`.</span>
<span class="sd">        n_w (int): Bit width used for weight.</span>
<span class="sd">        delta_w (float): Step size for weight.</span>
<span class="sd">        ste_fine_grained_w (bool): STE is fine-grained if `True`.</span>
<span class="sd">        quantize_b (bool): Quantize bias if `True`.</span>
<span class="sd">        n_b (int): Bit width used for bias.</span>
<span class="sd">        delta_w (float): Step size for bias.</span>
<span class="sd">        ste_fine_grained_b (bool): STE is fine-grained if `True`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: :math:`(B + 1)`-D array. (:math:`M_0 \\times \ldots \\times M_{B-1} \\times L`)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">,</span> <span class="s1">&#39;__iter__&#39;</span><span class="p">):</span>
        <span class="n">n_outmaps</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_outmaps</span><span class="p">]</span>
    <span class="n">n_outmaps</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">)</span>
    <span class="n">n_outmap</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inmaps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inmaps</span><span class="p">,</span> <span class="n">n_outmap</span><span class="p">),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>

    <span class="c1"># Floating Weight</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="c1"># Quantized Weight</span>
    <span class="k">if</span> <span class="n">quantize_w</span><span class="p">:</span>
        <span class="n">w_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;W_q&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
            <span class="n">w_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="c1"># Link computation graph</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fixed_point_quantize</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">quantize</span><span class="o">=</span><span class="n">quantize_w</span><span class="p">,</span>
                                          <span class="n">sign</span><span class="o">=</span><span class="n">sign_w</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n_w</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="n">delta_w</span><span class="p">,</span>
                                          <span class="n">ste_fine_grained</span><span class="o">=</span><span class="n">ste_fine_grained_w</span><span class="p">,</span>
                                          <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">w_q</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
        <span class="n">real_w_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">w</span>

    <span class="c1"># Bias</span>
    <span class="c1"># Floating</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">b_q</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">real_b_q</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">quantize_b</span><span class="p">:</span>
            <span class="n">b_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
                <span class="s2">&quot;b_q&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="c1"># Link computation graph</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fixed_point_quantize</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">quantize</span><span class="o">=</span><span class="n">quantize_b</span><span class="p">,</span>
                                              <span class="n">sign</span><span class="o">=</span><span class="n">sign_b</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n_b</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="n">delta_b</span><span class="p">,</span>
                                              <span class="n">ste_fine_grained</span><span class="o">=</span><span class="n">ste_fine_grained_b</span><span class="p">,</span>
                                              <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">b_q</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
            <span class="n">real_b_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">b</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">real_w_q</span><span class="p">,</span> <span class="n">real_b_q</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="fixed_point_quantized_convolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.fixed_point_quantized_convolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;fp_quantized_conv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps // group, *kernel)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;W_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Quantized weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps // group, *kernel)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Quantized biases&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">fixed_point_quantized_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span>
                                      <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                      <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                      <span class="n">quantize_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sign_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_w</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">delta_w</span><span class="o">=</span><span class="mi">2</span><span class="o">**-</span><span class="mi">4</span><span class="p">,</span> <span class="n">ste_fine_grained_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                      <span class="n">quantize_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sign_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_b</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">delta_b</span><span class="o">=</span><span class="mi">2</span><span class="o">**-</span><span class="mi">4</span><span class="p">,</span> <span class="n">ste_fine_grained_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,):</span>
    <span class="sd">&quot;&quot;&quot;Fixed-Point Quantized Convolution.</span>

<span class="sd">    Fixed-Point Quantized Convolution is the convolution function,</span>
<span class="sd">    except the definition of the inner product is modified.</span>
<span class="sd">    The input-output relation of this function is as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_{n, a, b} = \sum_{m} \sum_{i} \sum_{j} Q(w_{n, m, i, j}) x_{m, a + i, b + j},</span>

<span class="sd">    where :math:`Q(w_{n, m, i, j})` is the fixed-point quantization function.</span>

<span class="sd">    .. note::</span>

<span class="sd">        1) if you would like to share weights between some layers, please</span>
<span class="sd">        make sure to share the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the quantized weights (`quantized weight`)</span>

<span class="sd">        2) The weights and the quantized weights become synced only after :func:`~nnabla._variable.Variable.forward` is called,</span>
<span class="sd">        and not after a call to :func:`~nnabla._variable.Variable.backward`.</span>
<span class="sd">        To access the parameters of the network, remember to call :func:`~nnabla._variable.Variable.forward` once before doing so, otherwise the</span>
<span class="sd">        float weights and the quantized weights will not be in sync.</span>

<span class="sd">        3) CPU and GPU implementations now use float value for `quantized weight`,</span>
<span class="sd">        since this function is only for simulation purposes.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array.</span>
<span class="sd">        outmaps (int): Number of convolution kernels (which is equal to the number of output channels). For example, to apply convolution on an input with 16 types of filters, specify 16.</span>
<span class="sd">        kernel (:obj:`tuple` of :obj:`int`): Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</span>
<span class="sd">        pad (:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">        stride (:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">        dilation (:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">        group (int): Number of groups of channels. This makes connections across channels more sparse by grouping connections along map direction.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.  </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>
<span class="sd">        quantize_w (bool): Quantize weights if `True`.</span>
<span class="sd">        quantize_bias (bool): Quantize bias if `True`.</span>
<span class="sd">        sign_w (bool): Use signed quantization if `True`.</span>
<span class="sd">        n_w (int): Bit width used for weight.</span>
<span class="sd">        delta_w (float): Step size for weight.</span>
<span class="sd">        ste_fine_grained_w (bool): STE is fine-grained if `True`.</span>
<span class="sd">        quantize_b (bool): Quantize bias if `True`.</span>
<span class="sd">        n_b (int): Bit width used for bias.</span>
<span class="sd">        delta_w (float): Step size for bias.</span>
<span class="sd">        ste_fine_grained_b (bool): STE is fine-grained if `True`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: N-D array.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="n">outmaps</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>

    <span class="c1"># Floating Weight</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span> <span class="o">//</span> <span class="n">group</span><span class="p">)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="c1"># Quantized Weight</span>
    <span class="k">if</span> <span class="n">quantize_w</span><span class="p">:</span>
        <span class="n">w_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;W_q&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span> <span class="o">//</span> <span class="n">group</span><span class="p">)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
            <span class="n">w_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="c1"># Link computation graph</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fixed_point_quantize</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">quantize</span><span class="o">=</span><span class="n">quantize_w</span><span class="p">,</span>
                                          <span class="n">sign</span><span class="o">=</span><span class="n">sign_w</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n_w</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="n">delta_w</span><span class="p">,</span>
                                          <span class="n">ste_fine_grained</span><span class="o">=</span><span class="n">ste_fine_grained_w</span><span class="p">,</span>
                                          <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">w_q</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
        <span class="n">real_w_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">w</span>

    <span class="c1"># Bias</span>
    <span class="c1"># Floating</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">b_q</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">real_b_q</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">quantize_b</span><span class="p">:</span>
            <span class="n">b_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
                <span class="s2">&quot;b_q&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="c1"># Link computation graph</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fixed_point_quantize</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">quantize</span><span class="o">=</span><span class="n">quantize_b</span><span class="p">,</span>
                                              <span class="n">sign</span><span class="o">=</span><span class="n">sign_b</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n_b</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="n">delta_b</span><span class="p">,</span>
                                              <span class="n">ste_fine_grained</span><span class="o">=</span><span class="n">ste_fine_grained_b</span><span class="p">,</span>
                                              <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">b_q</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
            <span class="n">real_b_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">b</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">real_w_q</span><span class="p">,</span> <span class="n">real_b_q</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="pow2_quantized_affine"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.pow2_quantized_affine">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;pow2_quantized_affine&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Weight matrix in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;W_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Quantized weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Quantized biases&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">pow2_quantized_affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span>
                          <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                          <span class="n">quantize_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sign_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">with_zero_w</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_w</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">m_w</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ste_fine_grained_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                          <span class="n">quantize_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sign_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">with_zero_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_b</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">m_b</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ste_fine_grained_b</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Pow2 Quantized Affine.</span>

<span class="sd">    Pow2 Quantized Affine is the affine function,</span>
<span class="sd">    except the definition of the inner product is modified.</span>
<span class="sd">    The input-output relation of this function is as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_j = \sum_{i} Q(w_{ji}) x_i,</span>

<span class="sd">    where :math:`Q(w_{ji})` is the power-of-2 quantization function.</span>

<span class="sd">    .. note::</span>

<span class="sd">        1) if you would like to share weights between some layers, please</span>
<span class="sd">        make sure to share the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the quantized weights (`quantized weight`)</span>

<span class="sd">        2) The weights and the quantized weights become synced only after :func:`~nnabla._variable.Variable.forward` is called,</span>
<span class="sd">        and not after a call to :func:`~nnabla._variable.Variable.backward`.</span>
<span class="sd">        To access the parameters of the network, remember to call :func:`~nnabla._variable.Variable.forward` once before doing so, otherwise the</span>
<span class="sd">        float weights and the quantized weights will not be in sync.</span>

<span class="sd">        3) Quantized values are stored as floating point number for `quantized weight`,</span>
<span class="sd">        since this function is only for simulation purposes.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): Input N-D array with shape (:math:`M_0 \\times \ldots \\times M_{B-1} \\times D_B \\times \ldots \\times D_N`). Dimensions before and after base_axis are flattened as if it is a matrix.</span>
<span class="sd">        n_outmaps (:obj:`int` or :obj:`tuple` of :obj:`int`): Number of output neurons per data.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.  </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>
<span class="sd">        quantize_w (bool): Quantize weights if `True`.</span>
<span class="sd">        sign_w (bool): Use signed quantization if `True`.</span>
<span class="sd">        with_zero_w (bool): Indicate using zero as a quantized value. Default is false.</span>
<span class="sd">        n_w (int): Bit width used for weight.</span>
<span class="sd">        m_w (int): :math:`2^m` is upper bound and :math:`-2^m` is lower bound for weights. Default is 2.</span>
<span class="sd">        ste_fine_grained_w (bool): STE is fine-grained if `True`.</span>
<span class="sd">        quantize_b (bool): Quantize bias if `True`.</span>
<span class="sd">        with_zero_b (bool): Indicate using zero as a quantized value. Default is false.</span>
<span class="sd">        n_b (int): Bit width used for bias.</span>
<span class="sd">        m_b (int): :math:`2^m` is upper bound and :math:`-2^m` is lower bound for bias. Default is 2.</span>
<span class="sd">        ste_fine_grained_b (bool): STE is fine-grained if `True`.</span>
<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: :math:`(B + 1)`-D array. (:math:`M_0 \\times \ldots \\times M_{B-1} \\times L`)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">,</span> <span class="s1">&#39;__iter__&#39;</span><span class="p">):</span>
        <span class="n">n_outmaps</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_outmaps</span><span class="p">]</span>
    <span class="n">n_outmaps</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">)</span>
    <span class="n">n_outmap</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inmaps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inmaps</span><span class="p">,</span> <span class="n">n_outmap</span><span class="p">),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>

    <span class="c1"># Floating Weight</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="c1"># Quantized Weight</span>
    <span class="k">if</span> <span class="n">quantize_w</span><span class="p">:</span>
        <span class="n">w_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;W_q&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
            <span class="n">w_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="c1"># Link computation graph</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pow2_quantize</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">quantize</span><span class="o">=</span><span class="n">quantize_w</span><span class="p">,</span>
                                   <span class="n">sign</span><span class="o">=</span><span class="n">sign_w</span><span class="p">,</span> <span class="n">with_zero</span><span class="o">=</span><span class="n">with_zero_w</span><span class="p">,</span>
                                   <span class="n">n</span><span class="o">=</span><span class="n">n_w</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="n">m_w</span><span class="p">,</span> <span class="n">ste_fine_grained</span><span class="o">=</span><span class="n">ste_fine_grained_w</span><span class="p">,</span>
                                   <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">w_q</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
        <span class="n">real_w_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">w</span>

    <span class="c1"># Bias</span>
    <span class="c1"># Floating</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">b_q</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">real_b_q</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">quantize_b</span><span class="p">:</span>
            <span class="n">b_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
                <span class="s2">&quot;b_q&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pow2_quantize</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">quantize</span><span class="o">=</span><span class="n">quantize_b</span><span class="p">,</span>
                                       <span class="n">sign</span><span class="o">=</span><span class="n">sign_b</span><span class="p">,</span> <span class="n">with_zero</span><span class="o">=</span><span class="n">with_zero_b</span><span class="p">,</span>
                                       <span class="n">n</span><span class="o">=</span><span class="n">n_b</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="n">m_b</span><span class="p">,</span> <span class="n">ste_fine_grained</span><span class="o">=</span><span class="n">ste_fine_grained_b</span><span class="p">,</span>
                                       <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">b_q</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
            <span class="n">real_b_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">b</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">real_w_q</span><span class="p">,</span> <span class="n">real_b_q</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="pow2_quantized_convolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.pow2_quantized_convolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;pow2_quantized_conv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps // group, *kernel)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;W_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Quantized weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps // group, *kernel)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Quantized biases&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">pow2_quantized_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span>
                               <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                               <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                               <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                               <span class="n">quantize_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">with_zero_w</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sign_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_w</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">m_w</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ste_fine_grained_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                               <span class="n">quantize_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">with_zero_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sign_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_b</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">m_b</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ste_fine_grained_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,):</span>
    <span class="sd">&quot;&quot;&quot;Pow2 Quantized Convolution.</span>

<span class="sd">    Pow2 Quantized Convolution is the convolution function,</span>
<span class="sd">    except the definition of the inner product is modified.</span>
<span class="sd">    The input-output relation of this function is as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_{n, a, b} = \sum_{m} \sum_{i} \sum_{j} Q(w_{n, m, i, j}) x_{m, a + i, b + j},</span>

<span class="sd">    where :math:`Q(w_{n, m, i, j})` is the power-of-2 quantization function.</span>

<span class="sd">    .. note::</span>

<span class="sd">        1) if you would like to share weights between some layers, please</span>
<span class="sd">        make sure to share the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the quantized weights (`quantized weight`)</span>

<span class="sd">        2) The weights and the quantized weights become synced only after :func:`~nnabla._variable.Variable.forward` is called,</span>
<span class="sd">        and not after a call to :func:`~nnabla._variable.Variable.backward`.</span>
<span class="sd">        To access the parameters of the network, remember to call :func:`~nnabla._variable.Variable.forward` once before doing so, otherwise the</span>
<span class="sd">        float weights and the quantized weights will not be in sync.</span>

<span class="sd">        3) Quantized values are stored as floating point number for `quantized weight`,</span>
<span class="sd">        since this function is only for simulation purposes.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array.</span>
<span class="sd">        outmaps (int): Number of convolution kernels (which is equal to the number of output channels). For example, to apply convolution on an input with 16 types of filters, specify 16.</span>
<span class="sd">        kernel (:obj:`tuple` of :obj:`int`): Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</span>
<span class="sd">        pad (:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">        stride (:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">        dilation (:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">        group (int): Number of groups of channels. This makes connections across channels more sparse by grouping connections along map direction.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.  </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>
<span class="sd">        quantize_w (bool): Quantize weights if `True`.</span>
<span class="sd">        sign_w (bool): Use signed quantization if `True`.</span>
<span class="sd">        n_w (int): Bit width used for weight.</span>
<span class="sd">        m_w (int): :math:`2^m` is upper bound and :math:`-2^m` is lower bound for weights. Default is 2.</span>
<span class="sd">        ste_fine_grained_w (bool): STE is fine-grained if `True`.</span>
<span class="sd">        quantize_b (bool): Quantize bias if `True`.</span>
<span class="sd">        sign_b (bool): Use signed quantization if `True`.</span>
<span class="sd">        n_b (int): Bit width used for bias.</span>
<span class="sd">        m_b (int): :math:`2^m` is upper bound and :math:`-2^m` is lower bound for bias. Default is 2.</span>
<span class="sd">        ste_fine_grained_b (bool): STE is fine-grained if `True`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: N-D array.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="n">outmaps</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>

    <span class="c1"># Floating Weight</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span> <span class="o">//</span> <span class="n">group</span><span class="p">)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="c1"># Quantized Weight</span>
    <span class="k">if</span> <span class="n">quantize_w</span><span class="p">:</span>
        <span class="n">w_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;W_q&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span> <span class="o">//</span> <span class="n">group</span><span class="p">)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
            <span class="n">w_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Link computation graph</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pow2_quantize</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">quantize</span><span class="o">=</span><span class="n">quantize_w</span><span class="p">,</span>
                                   <span class="n">sign</span><span class="o">=</span><span class="n">sign_w</span><span class="p">,</span> <span class="n">with_zero</span><span class="o">=</span><span class="n">with_zero_w</span><span class="p">,</span>
                                   <span class="n">n</span><span class="o">=</span><span class="n">n_w</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="n">m_w</span><span class="p">,</span> <span class="n">ste_fine_grained</span><span class="o">=</span><span class="n">ste_fine_grained_w</span><span class="p">,</span>
                                   <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">w_q</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
        <span class="n">real_w_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">w</span>

    <span class="c1"># Bias</span>
    <span class="c1"># Floating</span>
    <span class="n">b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">b_q</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">real_b_q</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">quantize_b</span><span class="p">:</span>
            <span class="n">b_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
                <span class="s2">&quot;b_q&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="c1"># Link computation graph</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pow2_quantize</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">quantize</span><span class="o">=</span><span class="n">quantize_b</span><span class="p">,</span>
                                       <span class="n">sign</span><span class="o">=</span><span class="n">sign_b</span><span class="p">,</span> <span class="n">with_zero</span><span class="o">=</span><span class="n">with_zero_b</span><span class="p">,</span>
                                       <span class="n">n</span><span class="o">=</span><span class="n">n_b</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="n">m_b</span><span class="p">,</span> <span class="n">ste_fine_grained</span><span class="o">=</span><span class="n">ste_fine_grained_b</span><span class="p">,</span>
                                       <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">b_q</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
            <span class="n">real_b_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">b</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">real_w_q</span><span class="p">,</span> <span class="n">real_b_q</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="pruned_affine"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.pruned_affine">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;pruned_affine&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Weight matrix in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;W_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Qunatized weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(inmaps, outmaps)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Quantized biases&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">pruned_affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span>
                  <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                  <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">prune_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rate_w</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">prune_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rate_b</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Pruned Affine.</span>

<span class="sd">    Pruned Affine is the affine function, </span>
<span class="sd">    except the definition of the inner product is modified.</span>
<span class="sd">    The input-output relation of this function is as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_j = \sum_{i} Q(w_{ji}) x_i, </span>

<span class="sd">    where :math:`Q(w_{ji})` is the pruning function, i.e., `F.prune`.</span>

<span class="sd">    .. note::</span>

<span class="sd">        1) if you would like to share weights between some layers, please</span>
<span class="sd">        make sure to share the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the quantized weights (`quantized weight`)</span>

<span class="sd">        2) The weights and the quantized weights become synced only after :func:`~nnabla._variable.Variable.forward` is called,</span>
<span class="sd">        and not after a call to :func:`~nnabla._variable.Variable.backward`.</span>
<span class="sd">        To access the parameters of the network, remember to call :func:`~nnabla._variable.Variable.forward` once before doing so, otherwise the</span>
<span class="sd">        float weights and the quantized weights will not be in sync.</span>

<span class="sd">        3) CPU and GPU implementations now use float value for `quantized weight`,</span>
<span class="sd">        since this function is only for simulation purposes.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): Input N-D array with shape (:math:`M_0 \\times \ldots \\times M_{B-1} \\times D_B \\times \ldots \\times D_N`). Dimensions before and after base_axis are flattened as if it is a matrix.</span>
<span class="sd">        n_outmaps (:obj:`int` or :obj:`tuple` of :obj:`int`): Number of output neurons per data.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight.</span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>
<span class="sd">        prune_w (bool): Quantize weights if `True`.</span>
<span class="sd">        rate_w (float): Pruning rate for weights.</span>
<span class="sd">        prune_b (bool): Quantize bias if `True`.</span>
<span class="sd">        rate_b (float): Pruning rate for bias.</span>


<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: :math:`(B + 1)`-D array. (:math:`M_0 \\times \ldots \\times M_{B-1} \\times L`)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">,</span> <span class="s1">&#39;__iter__&#39;</span><span class="p">):</span>
        <span class="n">n_outmaps</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_outmaps</span><span class="p">]</span>
    <span class="n">n_outmaps</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">)</span>
    <span class="n">n_outmap</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">n_outmaps</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">inmaps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:])</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inmaps</span><span class="p">,</span> <span class="n">n_outmap</span><span class="p">),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>

    <span class="c1"># Floating Weight</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="c1"># sparsed Weight</span>
    <span class="k">if</span> <span class="n">prune_w</span><span class="p">:</span>
        <span class="n">w_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;W_q&quot;</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">:]))]</span> <span class="o">+</span> <span class="n">n_outmaps</span><span class="p">,</span>
            <span class="n">w_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="c1"># Link computation graph</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">prune</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="n">rate_w</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">w_q</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
        <span class="n">real_w_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">w</span>

    <span class="c1"># Bias</span>
    <span class="c1"># Floating</span>
    <span class="n">real_b_q</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">prune_b</span><span class="p">:</span>
            <span class="n">b_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
                <span class="s2">&quot;b_q&quot;</span><span class="p">,</span> <span class="n">n_outmaps</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="c1"># Link computation graph</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">prune</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="n">rate_b</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">b_q</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
            <span class="n">real_b_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">b</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">affine</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">real_w_q</span><span class="p">,</span> <span class="n">real_b_q</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="pruned_convolution"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.pruned_convolution">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;pruned_conv&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="s1">&#39;Filter weights in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps // group, *kernel)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;Bias vector in float&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;W_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Qunatized weights&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps, inmaps // group, *kernel)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;b_q&#39;</span><span class="p">,</span> <span class="s1">&#39;Quantized biases&#39;</span><span class="p">,</span> <span class="s1">&#39;(outmaps,)&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">pruned_convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">outmaps</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span>
                       <span class="n">pad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                       <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                       <span class="n">base_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                       <span class="n">prune_w</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rate_w</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">prune_b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rate_b</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Pruned Convolution.</span>

<span class="sd">    Pruned Convolution is the convolution function, </span>
<span class="sd">    except the definition of the inner product is modified.</span>
<span class="sd">    The input-output relation of this function is as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_{n, a, b} = \sum_{m} \sum_{i} \sum_{j} Q(w_{n, m, i, j}) x_{m, a + i, b + j}, </span>

<span class="sd">    where :math:`Q(w_{ji})` is the pruning function, i.e., `F.prune`.</span>

<span class="sd">    .. note::</span>

<span class="sd">        1) if you would like to share weights between some layers, please</span>
<span class="sd">        make sure to share the standard, floating value weights (`weight`)</span>
<span class="sd">        and not the quantized weights (`quantized weight`)</span>

<span class="sd">        2) The weights and the quantized weights become synced only after :func:`~nnabla._variable.Variable.forward` is called,</span>
<span class="sd">        and not after a call to :func:`~nnabla._variable.Variable.backward`.</span>
<span class="sd">        To access the parameters of the network, remember to call :func:`~nnabla._variable.Variable.forward` once before doing so, otherwise the</span>
<span class="sd">        float weights and the quantized weights will not be in sync.</span>

<span class="sd">        3) CPU and GPU implementations now use float value for `quantized weight`,</span>
<span class="sd">        since this function is only for simulation purposes.</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (~nnabla.Variable): N-D array.</span>
<span class="sd">        outmaps (int): Number of convolution kernels (which is equal to the number of output channels). For example, to apply convolution on an input with 16 types of filters, specify 16.</span>
<span class="sd">        kernel (:obj:`tuple` of :obj:`int`): Convolution kernel size. For example, to apply convolution on an image with a 3 (height) by 5 (width) two-dimensional kernel, specify (3,5).</span>
<span class="sd">        pad (:obj:`tuple` of :obj:`int`): Padding sizes for dimensions.</span>
<span class="sd">        stride (:obj:`tuple` of :obj:`int`): Stride sizes for dimensions.</span>
<span class="sd">        dilation (:obj:`tuple` of :obj:`int`): Dilation sizes for dimensions.</span>
<span class="sd">        group (int): Number of groups of channels. This makes connections across channels more sparse by grouping connections along map direction.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for weight.</span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`): Initializer for bias.</span>
<span class="sd">        base_axis (int): Dimensions up to `base_axis` are treated as the sample dimensions.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>
<span class="sd">        rng (numpy.random.RandomState): Random generator for Initializer.</span>
<span class="sd">        with_bias (bool): Specify whether to include the bias term.</span>
<span class="sd">        prune_w (bool): Quantize weights if `True`.</span>
<span class="sd">        rate_w (float): Pruning rate for weights.</span>
<span class="sd">        prune_b (bool): Quantize bias if `True`.</span>
<span class="sd">        rate_b (float): Pruning rate for bias.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`: N-D array.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">w_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_init</span> <span class="o">=</span> <span class="n">UniformInitializer</span><span class="p">(</span>
            <span class="n">calc_uniform_lim_glorot</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">],</span> <span class="n">outmaps</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">)),</span> <span class="n">rng</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">with_bias</span> <span class="ow">and</span> <span class="n">b_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">b_init</span> <span class="o">=</span> <span class="n">ConstantInitializer</span><span class="p">()</span>

    <span class="c1"># Floating Weight</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
        <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span> <span class="o">//</span> <span class="n">group</span><span class="p">)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
        <span class="n">w_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>

    <span class="c1"># Quantized Weight</span>
    <span class="k">if</span> <span class="n">prune_w</span><span class="p">:</span>
        <span class="n">w_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;W_q&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">base_axis</span><span class="p">]</span> <span class="o">//</span> <span class="n">group</span><span class="p">)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel</span><span class="p">),</span>
            <span class="n">w_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="c1"># Link computation graph</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">prune</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="n">rate_w</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">w_q</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
        <span class="n">real_w_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">real_w_q</span> <span class="o">=</span> <span class="n">w</span>

    <span class="c1"># Bias</span>
    <span class="c1"># Floating</span>
    <span class="n">real_b_q</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
            <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="ow">not</span> <span class="n">fix_parameters</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">prune_b</span><span class="p">:</span>
            <span class="n">b_q</span> <span class="o">=</span> <span class="n">get_parameter_or_create</span><span class="p">(</span>
                <span class="s2">&quot;b_q&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">outmaps</span><span class="p">,),</span> <span class="n">b_init</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="c1"># Link computation graph</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">prune</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="n">rate_b</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">b_q</span><span class="o">.</span><span class="n">data</span><span class="p">])</span>
            <span class="n">real_b_q</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">real_b_q</span> <span class="o">=</span> <span class="n">b</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">real_w_q</span><span class="p">,</span> <span class="n">real_b_q</span><span class="p">,</span> <span class="n">base_axis</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span></div>


<div class="viewcode-block" id="lstm"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.lstm">[docs]</a><span class="nd">@parametric_function_api</span><span class="p">(</span><span class="s2">&quot;lstm&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;affine/W&#39;</span><span class="p">,</span> <span class="s1">&#39;Stacked weight matrixes of LSTM block&#39;</span><span class="p">,</span>
     <span class="s1">&#39;(inmaps, 4, state_size)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;affine/b&#39;</span><span class="p">,</span> <span class="s1">&#39;Stacked bias vectors of LSTM block&#39;</span><span class="p">,</span> <span class="s1">&#39;(4, state_size,)&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
<span class="p">])</span>
<span class="k">def</span> <span class="nf">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">state_size</span><span class="p">,</span> <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Long Short-Term Memory.</span>

<span class="sd">    Long Short-Term Memory, or LSTM, is a building block for recurrent neural networks (RNN) layers.</span>
<span class="sd">    LSTM unit consists of a cell and input, output, forget gates whose functions are defined as following:</span>

<span class="sd">    .. math::</span>
<span class="sd">        f_t&amp;&amp;=\\sigma(W_fx_t+U_fh_{t-1}+b_f) \\\\</span>
<span class="sd">        i_t&amp;&amp;=\\sigma(W_ix_t+U_ih_{t-1}+b_i) \\\\</span>
<span class="sd">        o_t&amp;&amp;=\\sigma(W_ox_t+U_oh_{t-1}+b_o) \\\\</span>
<span class="sd">        c_t&amp;&amp;=f_t\\odot c_{t-1}+i_t\\odot\\tanh(W_cx_t+U_ch_{t-1}+b_c) \\\\</span>
<span class="sd">        h_t&amp;&amp;=o_t\\odot\\tanh(c_t).</span>

<span class="sd">    References:</span>

<span class="sd">        S. Hochreiter, and J. Schmidhuber. &quot;Long Short-Term Memory.&quot;</span>
<span class="sd">        Neural Computation. 1997.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (~nnabla.Variable): Input N-D array with shape (batch_size, input_size).</span>
<span class="sd">        h (~nnabla.Variable): Input N-D array with shape (batch_size, state_size).</span>
<span class="sd">        c (~nnabla.Variable): Input N-D array with shape (batch_size, state_size).</span>
<span class="sd">        state_size (int): Internal state size is set to `state_size`.</span>
<span class="sd">        w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`, optional): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.  </span>
<span class="sd">        b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`, optional): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">        fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~nnabla.Variable`</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">xh</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">iofc</span> <span class="o">=</span> <span class="n">affine</span><span class="p">(</span><span class="n">xh</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">state_size</span><span class="p">),</span> <span class="n">w_init</span><span class="o">=</span><span class="n">w_init</span><span class="p">,</span>
                  <span class="n">b_init</span><span class="o">=</span><span class="n">b_init</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="n">fix_parameters</span><span class="p">)</span>
    <span class="n">i_t</span><span class="p">,</span> <span class="n">o_t</span><span class="p">,</span> <span class="n">f_t</span><span class="p">,</span> <span class="n">gate</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">iofc</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">c_t</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">f_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">c</span> <span class="o">+</span> <span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">i_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">F</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">gate</span><span class="p">)</span>
    <span class="n">h_t</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">o_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">F</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">c_t</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">h_t</span><span class="p">,</span> <span class="n">c_t</span></div>


<div class="viewcode-block" id="LSTMCell"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.LSTMCell">[docs]</a><span class="k">class</span> <span class="nc">LSTMCell</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">state_size</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes an LSTM cell.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_size (int): Internal batch size is set to `batch_size`.</span>
<span class="sd">            state_size (int): Internal state size is set to `state_size`.</span>
<span class="sd">            h (~nnabla.Variable): Input N-D array with shape (batch_size, state_size). If not specified, it is initialized to zero by default.</span>
<span class="sd">            c (~nnabla.Variable): Input N-D array with shape (batch_size, state_size). If not specified, it is initialized to zero by default.</span>
<span class="sd">            name (str): Name for this LSTM Cell.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state_size</span> <span class="o">=</span> <span class="n">state_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="k">if</span> <span class="n">h</span><span class="p">:</span>  <span class="c1"># when user defines h</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">h</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_size</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">c</span><span class="p">:</span>  <span class="c1"># when user defines c</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">c</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_size</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">c</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Resets states h and c to zero.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>

<div class="viewcode-block" id="LSTMCell.__call__"><a class="viewcode-back" href="../../python/api/parametric_function.html#nnabla.parametric_functions.LSTMCell.__call__">[docs]</a>    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">w_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fix_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Updates h and c by calling lstm function.</span>

<span class="sd">        Args:</span>
<span class="sd">            x (~nnabla.Variable): Input N-D array with shape (batch_size, input_size).</span>
<span class="sd">            w_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`, optional): Initializer for weight. By default, it is initialized with :obj:`nnabla.initializer.UniformInitializer` within the range determined by :obj:`nnabla.initializer.calc_uniform_lim_glorot`.  </span>
<span class="sd">            b_init (:obj:`nnabla.initializer.BaseInitializer` or :obj:`numpy.ndarray`, optional): Initializer for bias. By default, it is initialized with zeros if `with_bias` is `True`.</span>
<span class="sd">            fix_parameters (bool): When set to `True`, the weights and biases will not be updated.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">lstm</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">c</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_size</span><span class="p">,</span>
            <span class="n">w_init</span><span class="p">,</span> <span class="n">b_init</span><span class="p">,</span>
            <span class="n">fix_parameters</span><span class="o">=</span><span class="n">fix_parameters</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span></div></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Sony Corporation

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>